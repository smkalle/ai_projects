{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MicroGPT: Building a GPT from Scratch in Pure Python\n\n> *\"The most atomic way to train and inference a GPT in pure, dependency-free Python.*\n> *This file is the complete algorithm. Everything else is just efficiency.\"*\n> -- **Andrej Karpathy**\n\n## The Core Insight\n\nA GPT language model -- the same architecture behind ChatGPT, Claude, and every modern LLM --\ncan be reduced to **six atomic math operations** on individual scalar numbers:\n\n| Operation | Code | Derivative | Purpose |\n|-----------|------|-----------|---------|\n| Addition | `a + b` | `da=1, db=1` | Combine values |\n| Multiplication | `a * b` | `da=b, db=a` | Scale values |\n| Power | `a ** n` | `da = n * a^(n-1)` | Non-linear transform |\n| Logarithm | `log(a)` | `da = 1/a` | Compress range (loss) |\n| Exponential | `exp(a)` | `da = exp(a)` | Expand range (softmax) |\n| ReLU | `max(0, a)` | `da = 1 if a>0 else 0` | Activation gate |\n\n**That's it.** The entire architecture -- embeddings, attention, MLP, normalization,\nloss function -- is built from just these six operations on scalars. A tiny autograd\nengine (30 lines) tracks every operation and computes gradients via the chain rule.\nAdam optimizer updates the parameters. The model learns.\n\n**No PyTorch. No TensorFlow. No NumPy. Just Python + these 6 operations.**",
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport math\nimport random\nfrom datetime import datetime\n\nrandom.seed(42)\n\nprint(\"Imports: os, math, random, datetime\")\nprint(\"No PyTorch, TensorFlow, NumPy, or any other library needed.\")",
   "id": "cell-1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 1: The Autograd Engine\n\nThe `Value` class wraps a single number and tracks the computation graph.\nWhen you do `a + b`, it records that `a` and `b` were the inputs, and stores the\nderivative rule so gradients can be computed later by `backward()`.\n\nEach of the 6 operations stores its **local derivative** (from calculus):\n- `d(a+b)/da = 1` -- adding doesn't change the rate\n- `d(a*b)/da = b` -- scaling by b\n- `d(a^n)/da = n * a^(n-1)` -- power rule\n- `d(log a)/da = 1/a`\n- `d(exp a)/da = exp(a)` -- the exponential is its own derivative\n- `d(relu a)/da = 1 if a > 0 else 0` -- pass-through or block",
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class Value:\n    \"\"\"A scalar value with automatic differentiation.\n\n    This is the ENTIRE autograd engine. Every number in the neural network\n    is a Value. Math on Values builds a computation graph. Then .backward()\n    walks that graph in reverse to compute gradients via the chain rule.\n    \"\"\"\n    __slots__ = ('data', 'grad', '_children', '_local_grads')\n\n    def __init__(self, data, children=(), local_grads=()):\n        self.data = data\n        self.grad = 0\n        self._children = children\n        self._local_grads = local_grads\n\n    def __repr__(self):\n        return f\"Value({self.data:.6f})\"\n\n    # --- THE 6 ATOMIC OPERATIONS ---\n\n    def __add__(self, other):            # a + b    -> da=1, db=1\n        other = other if isinstance(other, Value) else Value(other)\n        return Value(self.data + other.data, (self, other), (1, 1))\n\n    def __mul__(self, other):            # a * b    -> da=b, db=a\n        other = other if isinstance(other, Value) else Value(other)\n        return Value(self.data * other.data, (self, other), (other.data, self.data))\n\n    def __pow__(self, other):            # a ** n   -> da = n * a^(n-1)\n        return Value(self.data**other, (self,), (other * self.data**(other-1),))\n\n    def log(self):                       # log(a)   -> da = 1/a\n        return Value(math.log(self.data), (self,), (1/self.data,))\n\n    def exp(self):                       # exp(a)   -> da = exp(a)\n        return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n\n    def relu(self):                      # max(0,a) -> da = 1 if a>0 else 0\n        return Value(max(0, self.data), (self,), (float(self.data > 0),))\n\n    # --- DERIVED OPERATIONS (built from the 6 above) ---\n    def __neg__(self): return self * -1\n    def __radd__(self, other): return self + other\n    def __sub__(self, other): return self + (-other)\n    def __rsub__(self, other): return other + (-self)\n    def __rmul__(self, other): return self * other\n    def __truediv__(self, other): return self * other**-1\n    def __rtruediv__(self, other): return other * self**-1\n\n    def backward(self):\n        \"\"\"Backpropagate gradients through the entire computation graph.\"\"\"\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._children:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for v in reversed(topo):\n            for child, local_grad in zip(v._children, v._local_grads):\n                child.grad += local_grad * v.grad\n        return len(topo)\n\nprint(\"Value class defined -- the complete autograd engine\")\nprint(\"6 atomic ops: +, *, **, log, exp, relu\")",
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Test: Forward Computation\nVerify all 6 operations compute correct values.",
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Testing the 6 Atomic Operations ===\\n\")\n\na, b = Value(3.0), Value(4.0)\ntests = [\n    (\"add\",    (a + b).data,             3.0 + 4.0),\n    (\"mul\",    (a * b).data,             3.0 * 4.0),\n    (\"pow\",    (a ** 2).data,            3.0 ** 2),\n    (\"log\",    Value(2.0).log().data,    math.log(2.0)),\n    (\"exp\",    Value(1.0).exp().data,    math.exp(1.0)),\n    (\"relu+\",  Value(5.0).relu().data,   5.0),\n    (\"relu-\",  Value(-3.0).relu().data,  0.0),\n]\nall_pass = True\nfor name, got, expected in tests:\n    ok = abs(got - expected) < 1e-10\n    all_pass = all_pass and ok\n    print(f\"  {name:6s}: got {got:10.6f}  expected {expected:10.6f}  [{'PASS' if ok else 'FAIL'}]\")\n\nprint(\"\\n=== Derived Operations ===\\n\")\nx = Value(6.0)\nfor name, got, expected in [(\"neg\", (-x).data, -6.0), (\"sub\", (Value(10.0)-x).data, 4.0), (\"div\", (x/Value(3.0)).data, 2.0)]:\n    ok = abs(got - expected) < 1e-10\n    all_pass = all_pass and ok\n    print(f\"  {name:6s}: got {got:10.6f}  expected {expected:10.6f}  [{'PASS' if ok else 'FAIL'}]\")\n\nassert all_pass\nprint(\"\\nAll forward tests passed.\")",
   "id": "cell-5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Test: Gradient Computation (Backward Pass)\n\n**Exercise**: For each test, try to compute the gradient BY HAND before reading the answer.\nRemember the chain rule: if `f = g(h(x))`, then `df/dx = dg/dh * dh/dx`",
   "id": "cell-6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Gradient Tests ===\")\nprint(\"Predict each gradient BEFORE reading the output!\\n\")\n\n# Test 1: f = x + y\nx, y = Value(3.0), Value(4.0); f = x + y; f.backward()\nprint(f\"Test 1: f = x + y       df/dx = {x.grad} (rule: 1)  df/dy = {y.grad} (rule: 1)\")\nassert abs(x.grad - 1.0) < 1e-10\n\n# Test 2: f = x * y\nx, y = Value(3.0), Value(4.0); f = x * y; f.backward()\nprint(f\"Test 2: f = x * y       df/dx = {x.grad} (rule: b=4)  df/dy = {y.grad} (rule: a=3)\")\nassert abs(x.grad - 4.0) < 1e-10\n\n# Test 3: f = x^3\nx = Value(2.0); f = x ** 3; f.backward()\nprint(f\"Test 3: f = x^3         df/dx = {x.grad} (power rule: 3*2^2=12)\")\nassert abs(x.grad - 12.0) < 1e-10\n\n# Test 4: CHAIN RULE\nx = Value(3.0); f = (x * 2 + 1) ** 2; f.backward()\nprint(f\"Test 4: f = (2x+1)^2    df/dx = {x.grad} (chain: 2*(2*3+1)*2 = 28)\")\nassert abs(x.grad - 28.0) < 1e-10\n\n# Test 5: DEEP CHAIN\nx = Value(1.0); f = (x.exp() + 1).log(); f.backward()\nexpected = math.exp(1) / (math.exp(1) + 1)\nprint(f\"Test 5: f = log(exp(x)+1)  df/dx = {x.grad:.6f} (sigmoid = {expected:.6f})\")\nassert abs(x.grad - expected) < 1e-6\n\nprint(\"\\nAll gradient tests passed!\")",
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Verifying Autograd: Numerical Gradient Checking\n\nWe verify autograd against **numerical differentiation** (brute-force ground truth):\n```\ndf/dx ~ (f(x+h) - f(x-h)) / (2h)    where h ~ 1e-5\n```\nSlow but reliable. If both match, our engine is correct.",
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def check_gradients(build_fn, inputs, names=None, h=1e-5):\n    \"\"\"Verify autograd vs numerical finite differences.\"\"\"\n    if names is None: names = [f\"x{i}\" for i in range(len(inputs))]\n    vals = [Value(x) for x in inputs]\n    output = build_fn(vals); output.backward()\n    auto_grads = [v.grad for v in vals]\n    num_grads = []\n    for i in range(len(inputs)):\n        vp = [Value(x) for x in inputs]; vp[i].data += h\n        vm = [Value(x) for x in inputs]; vm[i].data -= h\n        num_grads.append((build_fn(vp).data - build_fn(vm).data) / (2*h))\n    print(f\"  {'Input':>8s}  {'Autograd':>12s}  {'Numerical':>12s}  {'Match':>6s}\")\n    print(f\"  {'-'*44}\")\n    ok = True\n    for nm, ag, ng in zip(names, auto_grads, num_grads):\n        m = abs(ag - ng) < h * 100; ok = ok and m\n        print(f\"  {nm:>8s}  {ag:12.6f}  {ng:12.6f}  {'OK' if m else 'FAIL':>6s}\")\n    return ok\n\nprint(\"=== Numerical Gradient Verification ===\\n\")\nprint(\"Test 1: f(x,y) = x^2 * y + y^3\")\nok1 = check_gradients(lambda v: v[0]**2 * v[1] + v[1]**3, [3.0, 2.0], ['x','y'])\nprint(\"\\nTest 2: f(x) = relu(log(exp(x)+1) * x^2)  [all 6 ops]\")\nok2 = check_gradients(lambda v: ((v[0].exp()+1).log() * v[0]**2).relu(), [1.5], ['x'])\nprint(\"\\nTest 3: cross-entropy loss (the EXACT loss we train with)\")\ndef xent(v):\n    mx = max(vi.data for vi in v)\n    exps = [(vi - mx).exp() for vi in v]\n    return -(exps[0] / sum(exps)).log()\nok3 = check_gradients(xent, [1.0, 2.0, 3.0], ['a','b','c'])\nassert ok1 and ok2 and ok3\nprint(\"\\nAll numerical checks passed -- autograd is correct!\")",
   "id": "cell-9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Counting the Computation Graph\nEvery operation creates a node. Let's count them to understand scale.",
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def count_graph_nodes(v):\n    \"\"\"Count nodes in the computation graph.\"\"\"\n    visited = set(); ops = 0; leaves = 0\n    def walk(node):\n        nonlocal ops, leaves\n        if node not in visited:\n            visited.add(node)\n            if node._children:\n                ops += 1\n                for c in node._children: walk(c)\n            else: leaves += 1\n    walk(v)\n    return {'total': len(visited), 'ops': ops, 'leaves': leaves}\n\nx, y = Value(3.0), Value(4.0)\nf = (x + y) * (x - y)\ns = count_graph_nodes(f)\nprint(f\"f = (x+y)*(x-y) = {f.data}\")\nprint(f\"  Graph: {s['total']} nodes = {s['ops']} ops + {s['leaves']} leaves\")\n\nx = Value(2.0); f = (x**2 + x*3 + 1).log()\ns = count_graph_nodes(f)\nprint(f\"\\nf = log(x^2 + 3x + 1) = {f.data:.6f}\")\nprint(f\"  Graph: {s['total']} nodes = {s['ops']} ops + {s['leaves']} leaves\")\nprint(f\"\\nEvery node has a tracked derivative. backward() walks them all.\")",
   "id": "cell-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 2: Dataset & Tokenization\n\nWe train on ~32K human names. **Character-level tokenization**: each unique character\ngets an ID (`a`=0, `b`=1, ..., `z`=25), plus a **BOS** (Beginning of Sequence) token.",
   "id": "cell-12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not os.path.exists('input.txt'):\n    print(\"Downloading names dataset...\")\n    import urllib.request\n    url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'\n    urllib.request.urlretrieve(url, 'input.txt')\ndocs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()]\nrandom.shuffle(docs)\n\nuchars = sorted(set(''.join(docs)))\nBOS = len(uchars)\nvocab_size = len(uchars) + 1\n\ndef encode(text): return [uchars.index(ch) for ch in text]\ndef decode(tokens): return ''.join(uchars[t] if t < len(uchars) else '<BOS>' for t in tokens)\n\nprint(f\"Dataset: {len(docs):,} names\")\nprint(f\"Vocabulary: {vocab_size} tokens = {len(uchars)} chars + BOS(={BOS})\")\nprint(f\"Sample: {docs[:8]}\")\nfor name in [\"alice\", \"bob\", \"zara\"]:\n    assert decode(encode(name)) == name\nprint(\"\\nEncode/decode roundtrip verified.\")",
   "id": "cell-13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Training Sequence Format\nFor `emma`: `[BOS, e, m, m, a, BOS]`. Model predicts each next character:\n```\nBOS->'e'  'e'->'m'  'm'->'m'  'm'->'a'  'a'->BOS\n```",
   "id": "cell-14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Training Sequence Examples ===\\n\")\nfor name in [\"emma\", \"bob\", \"lily\"]:\n    tokens = [BOS] + encode(name) + [BOS]\n    print(f\"\\'{name}\\' -> {tokens}\")\n    for i in range(len(tokens)-1):\n        ic = uchars[tokens[i]] if tokens[i] < len(uchars) else 'BOS'\n        tc = uchars[tokens[i+1]] if tokens[i+1] < len(uchars) else 'BOS'\n        print(f\"    pos {i}: \\'{ic}\\' -> \\'{tc}\\'\")\n    print()",
   "id": "cell-15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 3: Building Blocks (All from Atomic Operations)\n\nEvery component is a composition of the 6 atomic ops. Autograd tracks every one.\n\n### 3.1: Linear Layer\n`output = W @ x` -- matrix-vector multiply. For 16x16: ~496 atomic ops.",
   "id": "cell-16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def linear(x, w):\n    \"\"\"Matrix-vector multiply. Built from multiply + add.\"\"\"\n    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n\n# Test\nx_t = [Value(1.0), Value(2.0), Value(3.0)]\nw_t = [[Value(1),Value(0),Value(0)], [Value(0),Value(1),Value(0)]]\nout = linear(x_t, w_t)\nassert out[0].data == 1.0 and out[1].data == 2.0\nprint(\"linear: identity test passed\")\n\nprint(\"\\nGradient check:\")\ndef lin_fn(v):\n    x = v[:3]; w = [[v[3],v[4],v[5]], [v[6],v[7],v[8]]]\n    return sum(linear(x, w))\ncheck_gradients(lin_fn, [1.,2.,3.,.5,.3,.1,.2,.4,.6],\n                ['x0','x1','x2','w00','w01','w02','w10','w11','w12'])\n\nx16 = [Value(0.)]*16; w16 = [[Value(0.)]*16 for _ in range(16)]\ns = sum(linear(x16, w16)); st = count_graph_nodes(s)\nprint(f\"\\nlinear(16->16): {st['ops']} atomic ops\")",
   "id": "cell-17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2: Softmax\nConverts logits to probabilities summing to 1. Uses: subtract, exp, add, divide.",
   "id": "cell-18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def softmax(logits):\n    \"\"\"Logits -> probabilities (sum to 1).\"\"\"\n    max_val = max(val.data for val in logits)\n    exps = [(val - max_val).exp() for val in logits]\n    total = sum(exps)\n    return [e / total for e in exps]\n\nprobs = softmax([Value(1.0), Value(2.0), Value(3.0)])\npsum = sum(p.data for p in probs)\nassert abs(psum - 1.0) < 1e-6\nprint(f\"softmax([1,2,3]) = [{', '.join(f'{p.data:.4f}' for p in probs)}]  sum={psum:.6f}\")\n\nprint(\"\\nGradient check (-log softmax = cross-entropy):\")\ncheck_gradients(lambda v: -(softmax(v)[0]).log(), [1.,2.,3.], ['a','b','c'])",
   "id": "cell-19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3: RMSNorm\nNormalize to consistent scale: `output = x / sqrt(mean(x^2) + eps)`",
   "id": "cell-20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rmsnorm(x):\n    \"\"\"Root Mean Square normalization.\"\"\"\n    ms = sum(xi * xi for xi in x) / len(x)\n    scale = (ms + 1e-5) ** -0.5\n    return [xi * scale for xi in x]\n\nx_t = [Value(1.), Value(2.), Value(3.), Value(4.)]\nx_n = rmsnorm(x_t)\nms = sum(v.data**2 for v in x_n) / len(x_n)\nprint(f\"rmsnorm([1,2,3,4]) = [{', '.join(f'{v.data:.4f}' for v in x_n)}]\")\nprint(f\"  Mean of squares: {ms:.6f} (should be ~1.0)\")\n\nprint(\"\\nGradient check:\")\ncheck_gradients(lambda v: sum(rmsnorm(v)), [1.,2.,3.,4.], ['x0','x1','x2','x3'])",
   "id": "cell-21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Operation Count Summary",
   "id": "cell-22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Atomic Operations per Component ===\\n\")\nfor label, fn in [\n    (\"linear(16->16)\", lambda: sum(linear([Value(0.)]*16, [[Value(0.)]*16 for _ in range(16)]))),\n    (\"linear(16->64)\", lambda: sum(linear([Value(0.)]*16, [[Value(0.)]*16 for _ in range(64)]))),\n    (\"softmax(27)\",    lambda: sum(softmax([Value(.1*i) for i in range(27)]))),\n    (\"rmsnorm(16)\",    lambda: sum(rmsnorm([Value(.1*i) for i in range(16)]))),\n]:\n    s = count_graph_nodes(fn())\n    print(f\"  {label:20s}: {s['ops']:5d} ops\")\nprint(f\"\\nA full forward pass chains THOUSANDS of these together.\")",
   "id": "cell-23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 4: Model Parameters\n\n~8,000 learnable Values. Config: 16-dim embedding, 4 heads, 1 layer, 16 context.",
   "id": "cell-24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "n_embd = 16; n_head = 4; n_layer = 1; block_size = 16\nhead_dim = n_embd // n_head\n\ndef matrix(nout, nin, std=0.08):\n    return [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n\nstate_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd),\n              'lm_head': matrix(vocab_size, n_embd)}\nfor i in range(n_layer):\n    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n    state_dict[f'layer{i}.mlp_fc1'] = matrix(4*n_embd, n_embd)\n    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4*n_embd)\n\nparams = [p for mat in state_dict.values() for row in mat for p in row]\n\nprint(\"=== Model Parameters ===\\n\")\ntotal = 0\nfor name, mat in state_dict.items():\n    r, c = len(mat), len(mat[0]); n = r*c; total += n\n    print(f\"  {name:25s}  {r:3d} x {c:3d} = {n:5d}\")\nprint(f\"  {'TOTAL':25s}           {total:5d}\")",
   "id": "cell-25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 5: The GPT Architecture\n```\nToken -> [Embed + Position] -> RMSNorm\n      -> [Multi-Head Attention] + Residual -> RMSNorm\n      -> [MLP: Linear->ReLU->Linear] + Residual\n      -> [Linear -> Logits]\n```\n**Attention**: Q*K^T/sqrt(d) -> softmax -> weighted V. 4 heads in parallel.\n**Residual**: out = layer(x) + x. **KV Cache**: past keys/values saved.",
   "id": "cell-26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def gpt(token_id, pos_id, keys, values):\n    \"\"\"GPT forward pass for one token. Every op is atomic, tracked by autograd.\"\"\"\n    tok_emb = state_dict['wte'][token_id]\n    pos_emb = state_dict['wpe'][pos_id]\n    x = [t + p for t, p in zip(tok_emb, pos_emb)]\n    x = rmsnorm(x)\n    for li in range(n_layer):\n        # Attention\n        x_res = x; x = rmsnorm(x)\n        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n        keys[li].append(k); values[li].append(v)\n        x_attn = []\n        for h in range(n_head):\n            hs = h * head_dim\n            q_h = q[hs:hs+head_dim]\n            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n            al = [sum(q_h[j]*k_h[t][j] for j in range(head_dim))/head_dim**0.5\n                  for t in range(len(k_h))]\n            aw = softmax(al)\n            x_attn.extend([sum(aw[t]*v_h[t][j] for t in range(len(v_h)))\n                           for j in range(head_dim)])\n        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n        x = [a+b for a, b in zip(x, x_res)]\n        # MLP\n        x_res = x; x = rmsnorm(x)\n        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n        x = [xi.relu() for xi in x]\n        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n        x = [a+b for a, b in zip(x, x_res)]\n    return linear(x, state_dict['lm_head'])\n\nprint(f\"GPT defined: {n_layer} layer, {n_head} heads, {n_embd}-dim\")",
   "id": "cell-27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Test: Forward Pass & Computation Graph",
   "id": "cell-28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Forward Pass Test ===\\n\")\nkeys_t, values_t = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\nlogits = gpt(BOS, 0, keys_t, values_t)\nprobs = softmax(logits)\nassert len(logits) == vocab_size\nassert abs(sum(p.data for p in probs) - 1.0) < 1e-6\n\ntop5 = sorted(range(len(probs)), key=lambda i: probs[i].data, reverse=True)[:5]\nprint(\"Top 5 predictions (untrained = random):\")\nfor r, idx in enumerate(top5, 1):\n    ch = uchars[idx] if idx < len(uchars) else 'BOS'\n    print(f\"  {r}. \\'{ch}\\' = {probs[idx].data:.4f}\")\n\nst = count_graph_nodes(-probs[0].log())\nprint(f\"\\nComputation graph (1 token): {st['total']:,} nodes ({st['ops']:,} ops)\")",
   "id": "cell-29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Full Sequence Graph Analysis",
   "id": "cell-30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Processing \\'emma\\' token by token ===\\n\")\ntokens = [BOS] + encode(\"emma\") + [BOS]; n = len(tokens)-1\nkeys_a, values_a = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\nlosses_a = []\nfor pos_id in range(n):\n    tid, tgt = tokens[pos_id], tokens[pos_id+1]\n    logits = gpt(tid, pos_id, keys_a, values_a)\n    probs = softmax(logits); lt = -probs[tgt].log(); losses_a.append(lt)\n    ic = uchars[tid] if tid<len(uchars) else 'BOS'\n    tc = uchars[tgt] if tgt<len(uchars) else 'BOS'\n    st = count_graph_nodes(lt)\n    print(f\"  pos {pos_id}: \\'{ic}\\'->\\'{tc}\\'  loss={lt.data:.4f}  graph={st['total']:,} nodes\")\ntotal_loss = (1/n) * sum(losses_a)\nst = count_graph_nodes(total_loss)\nprint(f\"\\nTotal loss: {total_loss.data:.4f}\")\nprint(f\"Total graph: {st['total']:,} nodes ({st['ops']:,} ops)\")\nnn = total_loss.backward()\nprint(f\"Backward traversed {nn:,} nodes\")\nprint(f\"Params with gradients: {sum(1 for p in params if p.grad!=0)}/{len(params)}\")\nfor p in params: p.grad = 0",
   "id": "cell-31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 6: Anatomy of a Training Step\n\nBefore training 1000 steps, let's dissect ONE step:\n1. **Forward**: name -> predictions -> loss\n2. **Backward**: gradients via chain rule through entire graph\n3. **Adam update**: adjust each parameter\n4. **Zero**: reset gradients",
   "id": "cell-32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== One Training Step (Detailed) ===\\n\")\ndoc = docs[0]\ntokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\nn = min(block_size, len(tokens)-1)\nprint(f\"Name: \\'{doc}\\'  ({n} predictions)\\n\")\n\n# Forward\nprint(\"FORWARD:\")\nks, vs = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\nlosses = []\nfor pos_id in range(n):\n    tid, tgt = tokens[pos_id], tokens[pos_id+1]\n    logits = gpt(tid, pos_id, ks, vs); probs = softmax(logits)\n    lt = -probs[tgt].log(); losses.append(lt)\n    ic = uchars[tid] if tid<len(uchars) else 'BOS'\n    tc = uchars[tgt] if tgt<len(uchars) else 'BOS'\n    top3 = sorted(range(len(probs)), key=lambda i: probs[i].data, reverse=True)[:3]\n    t3 = \" \".join(f\"\\'{uchars[i] if i<len(uchars) else 'BOS'}\\':{probs[i].data:.2f}\" for i in top3)\n    print(f\"  \\'{ic}\\'->\\'{tc}\\' loss={lt.data:.3f}  top3=[{t3}]  {'HIT' if tgt in top3 else ''}\")\nloss = (1/n)*sum(losses)\nprint(f\"  Avg loss: {loss.data:.4f}\\n\")\n\n# Backward\nprint(\"BACKWARD:\")\nnn = loss.backward()\ngrads = [abs(p.grad) for p in params if p.grad!=0]\nprint(f\"  {nn:,} nodes traversed\")\nprint(f\"  {len(grads)}/{len(params)} params got gradients\")\nif grads: print(f\"  |grad| range: [{min(grads):.6f}, {max(grads):.6f}]\\n\")\n\n# Adam demo\nprint(\"ADAM UPDATE (example: param[0]):\")\np0 = params[0]; old = p0.data\nmh = (0.15*p0.grad)/(1-0.85); vh = (0.01*p0.grad**2)/(1-0.99)\nupd = 0.01*mh/(vh**0.5+1e-8)\nprint(f\"  grad={p0.grad:.6f} -> update={upd:.6f}\")\nprint(f\"  {old:.6f} -> {old-upd:.6f}\")\nfor p in params: p.grad = 0",
   "id": "cell-33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 7: Training\n\nAdam optimizer with linear LR decay. 1000 steps. Watch the loss decrease.",
   "id": "cell-34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "learning_rate = 0.01; beta1, beta2, eps_adam = 0.85, 0.99, 1e-8\nm = [0.0]*len(params); v = [0.0]*len(params)\nnum_steps = 1000; loss_history = []\n\nprint(f\"Training {num_steps} steps on {len(docs):,} names...\\n\")\nstart_time = datetime.now()\n\nfor step in range(num_steps):\n    doc = docs[step % len(docs)]\n    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n    n = min(block_size, len(tokens)-1)\n    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n    losses = []\n    for pos_id in range(n):\n        token_id, target_id = tokens[pos_id], tokens[pos_id+1]\n        logits = gpt(token_id, pos_id, keys, values)\n        probs = softmax(logits)\n        losses.append(-probs[target_id].log())\n    loss = (1/n)*sum(losses); loss_history.append(loss.data)\n    loss.backward()\n    lr_t = learning_rate * (1 - step/num_steps)\n    for i, p in enumerate(params):\n        m[i] = beta1*m[i] + (1-beta1)*p.grad\n        v[i] = beta2*v[i] + (1-beta2)*p.grad**2\n        m_hat = m[i]/(1-beta1**(step+1)); v_hat = v[i]/(1-beta2**(step+1))\n        p.data -= lr_t * m_hat / (v_hat**0.5 + eps_adam)\n        p.grad = 0\n    if (step+1) % 100 == 0 or step == 0:\n        elapsed = (datetime.now()-start_time).total_seconds()\n        avg = sum(loss_history[max(0,step-99):step+1])/min(step+1,100)\n        print(f\"  step {step+1:4d}/{num_steps} | loss {loss.data:.4f} | avg {avg:.4f} | \"\n              f\"lr {lr_t:.4f} | {(step+1)/elapsed:.1f} s/s | \\'{doc}\\'\")\n\nelapsed = (datetime.now()-start_time).total_seconds()\nprint(f\"\\nDone in {elapsed:.0f}s. Final loss: {loss_history[-1]:.4f}\")",
   "id": "cell-35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Training Analysis",
   "id": "cell-36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Training Analysis ===\\n\")\nfor s, e in [(0,100),(100,300),(300,500),(500,700),(700,900),(900,1000)]:\n    avg = sum(loss_history[s:e])/(e-s)\n    print(f\"  Steps {s+1:4d}-{e:4d}: {avg:.4f}  {'#'*int(avg*10)}\")\nimp = (loss_history[0]-loss_history[-1])/loss_history[0]*100\nppl = math.exp(sum(loss_history[-100:])/100)\nprint(f\"\\nImprovement: {loss_history[0]:.4f} -> {loss_history[-1]:.4f} ({imp:.1f}%)\")\nprint(f\"Final perplexity: {ppl:.1f} (random={vocab_size}, lower=better)\")\nprint(f\"Model is {vocab_size/ppl:.1f}x better than random guessing\")",
   "id": "cell-37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 8: Generation\n\nSample from trained model: BOS -> predict -> sample -> feed back -> repeat.\n**Temperature**: low=conservative, high=creative.",
   "id": "cell-38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate(temperature=0.5):\n    \"\"\"Generate a name by sampling from the trained model.\"\"\"\n    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n    token_id = BOS; chars = []\n    for pos_id in range(block_size):\n        logits = gpt(token_id, pos_id, keys, values)\n        probs = softmax([l/temperature for l in logits])\n        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n        if token_id == BOS: break\n        chars.append(uchars[token_id])\n    return ''.join(chars)\n\nprint(\"generate() defined\")",
   "id": "cell-39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Step-by-step Generation ===\\n\")\nfor si in range(3):\n    kg, vg = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n    tid = BOS; chars = []\n    print(f\"Sample {si+1}:\")\n    for pos_id in range(block_size):\n        logits = gpt(tid, pos_id, kg, vg)\n        probs = softmax([l/0.5 for l in logits])\n        top3 = sorted(range(len(probs)), key=lambda i: probs[i].data, reverse=True)[:3]\n        t3 = \" \".join(f\"\\'{uchars[i] if i<len(uchars) else 'BOS'}\\':{probs[i].data:.2f}\" for i in top3)\n        tid = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n        if tid == BOS:\n            print(f\"  pos {pos_id}: [{t3}] -> BOS (done)\"); break\n        chars.append(uchars[tid])\n        print(f\"  pos {pos_id}: [{t3}] -> \\'{uchars[tid]}\\'\")\n    print(f\"  Result: \\'{''.join(chars)}\\'\\n\")",
   "id": "cell-40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Temperature Comparison ===\\n\")\nfor temp in [0.3, 0.5, 0.8, 1.0, 1.5]:\n    names = [generate(temp) for _ in range(10)]\n    label = {0.3:\"conservative\",0.5:\"balanced\",0.8:\"exploratory\",1.0:\"creative\",1.5:\"wild\"}[temp]\n    print(f\"  temp={temp} ({label:12s}): {', '.join(names)}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"  20 Generated Names (temperature=0.5)\")\nprint(f\"{'='*60}\\n\")\ngen = []\nfor i in range(20):\n    name = generate(0.5); gen.append(name); print(f\"  {i+1:2d}. {name}\")\nprint(f\"\\nUnique: {len(set(gen))}/20  Avg len: {sum(len(n) for n in gen)/len(gen):.1f}\")",
   "id": "cell-41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 9: Analysis & Experiments",
   "id": "cell-42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Model: Real Names vs Nonsense ===\\n\")\nfor name in [\"emma\",\"liam\",\"olivia\",\"noah\",\"ava\",\"zzzzz\",\"qqqq\",\"aeiou\",\"xyzw\"]:\n    tokens = [BOS]+encode(name)+[BOS]; n = len(tokens)-1\n    kt, vt = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n    total = 0.\n    for pos_id in range(n):\n        logits = gpt(tokens[pos_id], pos_id, kt, vt)\n        probs = softmax(logits)\n        total += -probs[tokens[pos_id+1]].log().data\n    avg = total/n; ppl = math.exp(avg)\n    print(f\"  \\'{name:8s}\\': loss={avg:.3f}  ppl={ppl:6.1f}  {'#'*min(40,int(avg*8))}\")\nprint(f\"\\nReal names have lower loss than nonsense.\")",
   "id": "cell-43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Attention Patterns (\\'{}\\'): ===\\n\".format(\"sarah\"))\nname = \"sarah\"; tokens_v = [BOS]+encode(name); chars_v = ['BOS']+list(name)\nkv, vv = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\nall_attn = []\nfor pos_id in range(len(tokens_v)):\n    tid = tokens_v[pos_id]\n    x = [t+p for t,p in zip(state_dict['wte'][tid], state_dict['wpe'][pos_id])]\n    x = rmsnorm(x); li = 0; x = rmsnorm(x)\n    q = linear(x, state_dict[f'layer{li}.attn_wq'])\n    k = linear(x, state_dict[f'layer{li}.attn_wk'])\n    kv[li].append(k); vv[li].append(linear(x, state_dict[f'layer{li}.attn_wv']))\n    pa = []\n    for h in range(n_head):\n        hs = h*head_dim; q_h = q[hs:hs+head_dim]\n        k_h = [ki[hs:hs+head_dim] for ki in kv[li]]\n        al = [sum(q_h[j]*k_h[t][j] for j in range(head_dim))/head_dim**0.5\n              for t in range(len(k_h))]\n        pa.append([w.data for w in softmax(al)])\n    all_attn.append(pa)\nfor h in range(n_head):\n    print(f\"Head {h}:\")\n    print(\"       \"+\"\".join(f\"{c:>7s}\" for c in chars_v))\n    for pi in range(len(tokens_v)):\n        row = f\"  {chars_v[pi]:>4s}: \"\n        for t in range(pi+1):\n            w = all_attn[pi][h][t]\n            if w > 0.3: row += f\" [{w:.2f}]\"\n            elif w > 0.1: row += f\"  {w:.2f} \"\n            else: row += f\"  .    \"\n        print(row)\n    print()",
   "id": "cell-44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Gradient Flow by Layer ===\\n\")\ndoc = docs[0]\ntokens = [BOS]+[uchars.index(ch) for ch in doc]+[BOS]; n = min(block_size, len(tokens)-1)\nkg, vg = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\nlosses = []\nfor pos_id in range(n):\n    logits = gpt(tokens[pos_id], pos_id, kg, vg)\n    probs = softmax(logits); losses.append(-probs[tokens[pos_id+1]].log())\nloss = (1/n)*sum(losses); loss.backward()\nfor name, mat in state_dict.items():\n    g = [abs(p.grad) for row in mat for p in row]\n    mg = sum(g)/len(g); mx = max(g)\n    print(f\"  {name:25s}: mean|g|={mg:.6f}  max={mx:.6f}  {'#'*min(30,int(mg*500))}\")\nfor p in params: p.grad = 0\nprint(f\"\\nLarger gradients = more learning in that component\")",
   "id": "cell-45"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 10: Exercises\n\n### Exercise 1: Predict the Gradient\nBefore running: `f(x) = x^2 + 3x + 1` at `x = 2`. What is `df/dx`?",
   "id": "cell-46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "x = Value(2.0); f = x**2 + x*3 + 1; f.backward()\nprint(f\"f(2) = {f.data} (expected 11)  df/dx = {x.grad} (expected 7)\")",
   "id": "cell-47"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Count the Math\nHow many operations for 'emma'? Estimate, then run:",
   "id": "cell-48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "tokens = [BOS]+encode(\"emma\")+[BOS]; n = len(tokens)-1\nkc, vc = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\nlc = []\nfor pos_id in range(n):\n    logits = gpt(tokens[pos_id], pos_id, kc, vc)\n    probs = softmax(logits); lc.append(-probs[tokens[pos_id+1]].log())\nst = count_graph_nodes((1/n)*sum(lc))\nprint(f\"'emma' ({n} predictions): {st['total']:,} graph nodes, {st['ops']:,} operations\")\nprint(f\"Over {num_steps} training steps: ~{st['ops']*num_steps:,} total ops\")\nprint(f\"All differentiated by our 30-line autograd engine!\")",
   "id": "cell-49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: Experiment\nTry changing hyperparameters (re-run from Part 4):\n```python\nn_embd = 32       # more capacity\nn_layer = 2       # deeper\nlearning_rate = 0.1   # too high?\nnum_steps = 2000  # more training\ntemperature = 0.1 # very conservative\ntemperature = 2.0 # very random\n```",
   "id": "cell-50"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\n**The 6 atomic operations:** `+`, `*`, `**`, `log`, `exp`, `relu`\n\n**Everything is composition:**\nLinear = mul + add | Softmax = sub + exp + add + div | RMSNorm = mul + add + pow\nAttention = linear + softmax + linear | MLP = linear + relu + linear | Loss = softmax + log\n\n**What's different in production LLMs (GPT-4, Claude)?**\nSame architecture, billions of parameters, GPU tensors instead of scalar Values,\nCUDA kernels instead of Python loops. **But the math is identical.**\n\n> *\"This file is the complete algorithm. Everything else is just efficiency.\"*\n\n**Next steps:**\n- [Karpathy: Zero to Hero (YouTube)](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n- Implement in PyTorch for 100x speedup",
   "id": "cell-51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}