{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smkalle/ai_projects/blob/main/tutorials/microgpt/microgpt_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "# MicroGPT: Building a GPT from Scratch in Pure Python\n",
        "\n",
        "> *\"The most atomic way to train and inference a GPT in pure, dependency-free Python.*\n",
        "> *This file is the complete algorithm. Everything else is just efficiency.\"*\n",
        "> -- **Andrej Karpathy**\n",
        "\n",
        "## The Core Insight\n",
        "\n",
        "A GPT language model -- the same architecture behind ChatGPT, Claude, and every modern LLM --\n",
        "can be reduced to **six atomic math operations** on individual scalar numbers:\n",
        "\n",
        "| Operation | Code | Derivative | Purpose |\n",
        "|-----------|------|-----------|---------|\n",
        "| Addition | `a + b` | `da=1, db=1` | Combine values |\n",
        "| Multiplication | `a * b` | `da=b, db=a` | Scale values |\n",
        "| Power | `a ** n` | `da = n * a^(n-1)` | Non-linear transform |\n",
        "| Logarithm | `log(a)` | `da = 1/a` | Compress range (loss) |\n",
        "| Exponential | `exp(a)` | `da = exp(a)` | Expand range (softmax) |\n",
        "| ReLU | `max(0, a)` | `da = 1 if a>0 else 0` | Activation gate |\n",
        "\n",
        "**That's it.** The entire architecture -- embeddings, attention, MLP, normalization,\n",
        "loss function -- is built from just these six operations on scalars. A tiny autograd\n",
        "engine (30 lines) tracks every operation and computes gradients via the chain rule.\n",
        "Adam optimizer updates the parameters. The model learns.\n",
        "\n",
        "**No PyTorch. No TensorFlow. No NumPy. Just Python + these 6 operations.**"
      ],
      "id": "cell-0"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-1",
        "outputId": "0075c6ef-5067-41d0-edf5-ab4b8914a836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports: os, math, random, datetime\n",
            "No PyTorch, TensorFlow, NumPy, or any other library needed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Imports: os, math, random, datetime\")\n",
        "print(\"No PyTorch, TensorFlow, NumPy, or any other library needed.\")"
      ],
      "id": "cell-1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-2"
      },
      "source": [
        "---\n",
        "## Part 1: The Autograd Engine\n",
        "\n",
        "The `Value` class wraps a single number and tracks the computation graph.\n",
        "When you do `a + b`, it records that `a` and `b` were the inputs, and stores the\n",
        "derivative rule so gradients can be computed later by `backward()`.\n",
        "\n",
        "Each of the 6 operations stores its **local derivative** (from calculus):\n",
        "- `d(a+b)/da = 1` -- adding doesn't change the rate\n",
        "- `d(a*b)/da = b` -- scaling by b\n",
        "- `d(a^n)/da = n * a^(n-1)` -- power rule\n",
        "- `d(log a)/da = 1/a`\n",
        "- `d(exp a)/da = exp(a)` -- the exponential is its own derivative\n",
        "- `d(relu a)/da = 1 if a > 0 else 0` -- pass-through or block"
      ],
      "id": "cell-2"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-3",
        "outputId": "6797279c-5556-4938-9434-58f6ccd834b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value class defined -- the complete autograd engine\n",
            "6 atomic ops: +, *, **, log, exp, relu\n"
          ]
        }
      ],
      "source": [
        "class Value:\n",
        "    \"\"\"A scalar value with automatic differentiation.\n",
        "\n",
        "    This is the ENTIRE autograd engine. Every number in the neural network\n",
        "    is a Value. Math on Values builds a computation graph. Then .backward()\n",
        "    walks that graph in reverse to compute gradients via the chain rule.\n",
        "    \"\"\"\n",
        "    __slots__ = ('data', 'grad', '_children', '_local_grads')\n",
        "\n",
        "    def __init__(self, data, children=(), local_grads=()):\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        self._children = children\n",
        "        self._local_grads = local_grads\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value({self.data:.6f})\"\n",
        "\n",
        "    # --- THE 6 ATOMIC OPERATIONS ---\n",
        "\n",
        "    def __add__(self, other):            # a + b    -> da=1, db=1\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data + other.data, (self, other), (1, 1))\n",
        "\n",
        "    def __mul__(self, other):            # a * b    -> da=b, db=a\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
        "\n",
        "    def __pow__(self, other):            # a ** n   -> da = n * a^(n-1)\n",
        "        return Value(self.data**other, (self,), (other * self.data**(other-1),))\n",
        "\n",
        "    def log(self):                       # log(a)   -> da = 1/a\n",
        "        return Value(math.log(self.data), (self,), (1/self.data,))\n",
        "\n",
        "    def exp(self):                       # exp(a)   -> da = exp(a)\n",
        "        return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
        "\n",
        "    def relu(self):                      # max(0,a) -> da = 1 if a>0 else 0\n",
        "        return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
        "\n",
        "    # --- DERIVED OPERATIONS (built from the 6 above) ---\n",
        "    def __neg__(self): return self * -1\n",
        "    def __radd__(self, other): return self + other\n",
        "    def __sub__(self, other): return self + (-other)\n",
        "    def __rsub__(self, other): return other + (-self)\n",
        "    def __rmul__(self, other): return self * other\n",
        "    def __truediv__(self, other): return self * other**-1\n",
        "    def __rtruediv__(self, other): return other * self**-1\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"Backpropagate gradients through the entire computation graph.\"\"\"\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._children:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            for child, local_grad in zip(v._children, v._local_grads):\n",
        "                child.grad += local_grad * v.grad\n",
        "        return len(topo)\n",
        "\n",
        "print(\"Value class defined -- the complete autograd engine\")\n",
        "print(\"6 atomic ops: +, *, **, log, exp, relu\")"
      ],
      "id": "cell-3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-4"
      },
      "source": [
        "### Test: Forward Computation\n",
        "Verify all 6 operations compute correct values."
      ],
      "id": "cell-4"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-5",
        "outputId": "c226d905-d105-49aa-f8f6-304353d695f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Testing the 6 Atomic Operations ===\n",
            "\n",
            "  add   : got   7.000000  expected   7.000000  [PASS]\n",
            "  mul   : got  12.000000  expected  12.000000  [PASS]\n",
            "  pow   : got   9.000000  expected   9.000000  [PASS]\n",
            "  log   : got   0.693147  expected   0.693147  [PASS]\n",
            "  exp   : got   2.718282  expected   2.718282  [PASS]\n",
            "  relu+ : got   5.000000  expected   5.000000  [PASS]\n",
            "  relu- : got   0.000000  expected   0.000000  [PASS]\n",
            "\n",
            "=== Derived Operations ===\n",
            "\n",
            "  neg   : got  -6.000000  expected  -6.000000  [PASS]\n",
            "  sub   : got   4.000000  expected   4.000000  [PASS]\n",
            "  div   : got   2.000000  expected   2.000000  [PASS]\n",
            "\n",
            "All forward tests passed.\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Testing the 6 Atomic Operations ===\\n\")\n",
        "\n",
        "a, b = Value(3.0), Value(4.0)\n",
        "tests = [\n",
        "    (\"add\",    (a + b).data,             3.0 + 4.0),\n",
        "    (\"mul\",    (a * b).data,             3.0 * 4.0),\n",
        "    (\"pow\",    (a ** 2).data,            3.0 ** 2),\n",
        "    (\"log\",    Value(2.0).log().data,    math.log(2.0)),\n",
        "    (\"exp\",    Value(1.0).exp().data,    math.exp(1.0)),\n",
        "    (\"relu+\",  Value(5.0).relu().data,   5.0),\n",
        "    (\"relu-\",  Value(-3.0).relu().data,  0.0),\n",
        "]\n",
        "all_pass = True\n",
        "for name, got, expected in tests:\n",
        "    ok = abs(got - expected) < 1e-10\n",
        "    all_pass = all_pass and ok\n",
        "    print(f\"  {name:6s}: got {got:10.6f}  expected {expected:10.6f}  [{'PASS' if ok else 'FAIL'}]\")\n",
        "\n",
        "print(\"\\n=== Derived Operations ===\\n\")\n",
        "x = Value(6.0)\n",
        "for name, got, expected in [(\"neg\", (-x).data, -6.0), (\"sub\", (Value(10.0)-x).data, 4.0), (\"div\", (x/Value(3.0)).data, 2.0)]:\n",
        "    ok = abs(got - expected) < 1e-10\n",
        "    all_pass = all_pass and ok\n",
        "    print(f\"  {name:6s}: got {got:10.6f}  expected {expected:10.6f}  [{'PASS' if ok else 'FAIL'}]\")\n",
        "\n",
        "assert all_pass\n",
        "print(\"\\nAll forward tests passed.\")"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-6"
      },
      "source": [
        "### Test: Gradient Computation (Backward Pass)\n",
        "\n",
        "**Exercise**: For each test, try to compute the gradient BY HAND before reading the answer.\n",
        "Remember the chain rule: if `f = g(h(x))`, then `df/dx = dg/dh * dh/dx`"
      ],
      "id": "cell-6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-7",
        "outputId": "5f8ac4a6-10ea-45f0-be35-e19fee234bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Gradient Tests ===\n",
            "Predict each gradient BEFORE reading the output!\n",
            "\n",
            "Test 1: f = x + y       df/dx = 1 (rule: 1)  df/dy = 1 (rule: 1)\n",
            "Test 2: f = x * y       df/dx = 4.0 (rule: b=4)  df/dy = 3.0 (rule: a=3)\n",
            "Test 3: f = x^3         df/dx = 12.0 (power rule: 3*2^2=12)\n",
            "Test 4: f = (2x+1)^2    df/dx = 28.0 (chain: 2*(2*3+1)*2 = 28)\n",
            "Test 5: f = log(exp(x)+1)  df/dx = 0.731059 (sigmoid = 0.731059)\n",
            "\n",
            "All gradient tests passed!\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Gradient Tests ===\")\n",
        "print(\"Predict each gradient BEFORE reading the output!\\n\")\n",
        "\n",
        "# Test 1: f = x + y\n",
        "x, y = Value(3.0), Value(4.0); f = x + y; f.backward()\n",
        "print(f\"Test 1: f = x + y       df/dx = {x.grad} (rule: 1)  df/dy = {y.grad} (rule: 1)\")\n",
        "assert abs(x.grad - 1.0) < 1e-10\n",
        "\n",
        "# Test 2: f = x * y\n",
        "x, y = Value(3.0), Value(4.0); f = x * y; f.backward()\n",
        "print(f\"Test 2: f = x * y       df/dx = {x.grad} (rule: b=4)  df/dy = {y.grad} (rule: a=3)\")\n",
        "assert abs(x.grad - 4.0) < 1e-10\n",
        "\n",
        "# Test 3: f = x^3\n",
        "x = Value(2.0); f = x ** 3; f.backward()\n",
        "print(f\"Test 3: f = x^3         df/dx = {x.grad} (power rule: 3*2^2=12)\")\n",
        "assert abs(x.grad - 12.0) < 1e-10\n",
        "\n",
        "# Test 4: CHAIN RULE\n",
        "x = Value(3.0); f = (x * 2 + 1) ** 2; f.backward()\n",
        "print(f\"Test 4: f = (2x+1)^2    df/dx = {x.grad} (chain: 2*(2*3+1)*2 = 28)\")\n",
        "assert abs(x.grad - 28.0) < 1e-10\n",
        "\n",
        "# Test 5: DEEP CHAIN\n",
        "x = Value(1.0); f = (x.exp() + 1).log(); f.backward()\n",
        "expected = math.exp(1) / (math.exp(1) + 1)\n",
        "print(f\"Test 5: f = log(exp(x)+1)  df/dx = {x.grad:.6f} (sigmoid = {expected:.6f})\")\n",
        "assert abs(x.grad - expected) < 1e-6\n",
        "\n",
        "print(\"\\nAll gradient tests passed!\")"
      ],
      "id": "cell-7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-8"
      },
      "source": [
        "### Verifying Autograd: Numerical Gradient Checking\n",
        "\n",
        "We verify autograd against **numerical differentiation** (brute-force ground truth):\n",
        "```\n",
        "df/dx ~ (f(x+h) - f(x-h)) / (2h)    where h ~ 1e-5\n",
        "```\n",
        "Slow but reliable. If both match, our engine is correct."
      ],
      "id": "cell-8"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-9",
        "outputId": "c91ace00-a52b-41e6-a859-f90df41f9893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Numerical Gradient Verification ===\n",
            "\n",
            "Test 1: f(x,y) = x^2 * y + y^3\n",
            "     Input      Autograd     Numerical   Match\n",
            "  --------------------------------------------\n",
            "         x     12.000000     12.000000      OK\n",
            "         y     21.000000     21.000000      OK\n",
            "\n",
            "Test 2: f(x) = relu(log(exp(x)+1) * x^2)  [all 6 ops]\n",
            "     Input      Autograd     Numerical   Match\n",
            "  --------------------------------------------\n",
            "         x      6.943782      6.943782      OK\n",
            "\n",
            "Test 3: cross-entropy loss (the EXACT loss we train with)\n",
            "     Input      Autograd     Numerical   Match\n",
            "  --------------------------------------------\n",
            "         a     -0.909969     -0.909969      OK\n",
            "         b      0.244728      0.244728      OK\n",
            "         c      0.665241      0.665241      OK\n",
            "\n",
            "All numerical checks passed -- autograd is correct!\n"
          ]
        }
      ],
      "source": [
        "def check_gradients(build_fn, inputs, names=None, h=1e-5):\n",
        "    \"\"\"Verify autograd vs numerical finite differences.\"\"\"\n",
        "    if names is None: names = [f\"x{i}\" for i in range(len(inputs))]\n",
        "    vals = [Value(x) for x in inputs]\n",
        "    output = build_fn(vals); output.backward()\n",
        "    auto_grads = [v.grad for v in vals]\n",
        "    num_grads = []\n",
        "    for i in range(len(inputs)):\n",
        "        vp = [Value(x) for x in inputs]; vp[i].data += h\n",
        "        vm = [Value(x) for x in inputs]; vm[i].data -= h\n",
        "        num_grads.append((build_fn(vp).data - build_fn(vm).data) / (2*h))\n",
        "    print(f\"  {'Input':>8s}  {'Autograd':>12s}  {'Numerical':>12s}  {'Match':>6s}\")\n",
        "    print(f\"  {'-'*44}\")\n",
        "    ok = True\n",
        "    for nm, ag, ng in zip(names, auto_grads, num_grads):\n",
        "        m = abs(ag - ng) < h * 100; ok = ok and m\n",
        "        print(f\"  {nm:>8s}  {ag:12.6f}  {ng:12.6f}  {'OK' if m else 'FAIL':>6s}\")\n",
        "    return ok\n",
        "\n",
        "print(\"=== Numerical Gradient Verification ===\\n\")\n",
        "print(\"Test 1: f(x,y) = x^2 * y + y^3\")\n",
        "ok1 = check_gradients(lambda v: v[0]**2 * v[1] + v[1]**3, [3.0, 2.0], ['x','y'])\n",
        "print(\"\\nTest 2: f(x) = relu(log(exp(x)+1) * x^2)  [all 6 ops]\")\n",
        "ok2 = check_gradients(lambda v: ((v[0].exp()+1).log() * v[0]**2).relu(), [1.5], ['x'])\n",
        "print(\"\\nTest 3: cross-entropy loss (the EXACT loss we train with)\")\n",
        "def xent(v):\n",
        "    mx = max(vi.data for vi in v)\n",
        "    exps = [(vi - mx).exp() for vi in v]\n",
        "    return -(exps[0] / sum(exps)).log()\n",
        "ok3 = check_gradients(xent, [1.0, 2.0, 3.0], ['a','b','c'])\n",
        "assert ok1 and ok2 and ok3\n",
        "print(\"\\nAll numerical checks passed -- autograd is correct!\")"
      ],
      "id": "cell-9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-10"
      },
      "source": [
        "### Counting the Computation Graph\n",
        "Every operation creates a node. Let's count them to understand scale."
      ],
      "id": "cell-10"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-11",
        "outputId": "d899da8a-5f5f-4678-e4d0-3d0a158225e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f = (x+y)*(x-y) = -7.0\n",
            "  Graph: 7 nodes = 4 ops + 3 leaves\n",
            "\n",
            "f = log(x^2 + 3x + 1) = 2.397895\n",
            "  Graph: 8 nodes = 5 ops + 3 leaves\n",
            "\n",
            "Every node has a tracked derivative. backward() walks them all.\n"
          ]
        }
      ],
      "source": [
        "def count_graph_nodes(v):\n",
        "    \"\"\"Count nodes in the computation graph.\"\"\"\n",
        "    visited = set(); ops = 0; leaves = 0\n",
        "    def walk(node):\n",
        "        nonlocal ops, leaves\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            if node._children:\n",
        "                ops += 1\n",
        "                for c in node._children: walk(c)\n",
        "            else: leaves += 1\n",
        "    walk(v)\n",
        "    return {'total': len(visited), 'ops': ops, 'leaves': leaves}\n",
        "\n",
        "x, y = Value(3.0), Value(4.0)\n",
        "f = (x + y) * (x - y)\n",
        "s = count_graph_nodes(f)\n",
        "print(f\"f = (x+y)*(x-y) = {f.data}\")\n",
        "print(f\"  Graph: {s['total']} nodes = {s['ops']} ops + {s['leaves']} leaves\")\n",
        "\n",
        "x = Value(2.0); f = (x**2 + x*3 + 1).log()\n",
        "s = count_graph_nodes(f)\n",
        "print(f\"\\nf = log(x^2 + 3x + 1) = {f.data:.6f}\")\n",
        "print(f\"  Graph: {s['total']} nodes = {s['ops']} ops + {s['leaves']} leaves\")\n",
        "print(f\"\\nEvery node has a tracked derivative. backward() walks them all.\")"
      ],
      "id": "cell-11"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-12"
      },
      "source": [
        "---\n",
        "## Part 2: Dataset & Tokenization\n",
        "\n",
        "We train on ~32K human names. **Character-level tokenization**: each unique character\n",
        "gets an ID (`a`=0, `b`=1, ..., `z`=25), plus a **BOS** (Beginning of Sequence) token."
      ],
      "id": "cell-12"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-13",
        "outputId": "3a08bf3a-6964-474b-8420-c5253842739a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 32,033 names\n",
            "Vocabulary: 27 tokens = 26 chars + BOS(=26)\n",
            "Sample: ['yuheng', 'diondre', 'xavien', 'jori', 'juanluis', 'erandi', 'phia', 'samatha']\n",
            "\n",
            "Encode/decode roundtrip verified.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('input.txt'):\n",
        "    print(\"Downloading names dataset...\")\n",
        "    import urllib.request\n",
        "    url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'\n",
        "    urllib.request.urlretrieve(url, 'input.txt')\n",
        "docs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()]\n",
        "random.shuffle(docs)\n",
        "\n",
        "uchars = sorted(set(''.join(docs)))\n",
        "BOS = len(uchars)\n",
        "vocab_size = len(uchars) + 1\n",
        "\n",
        "def encode(text): return [uchars.index(ch) for ch in text]\n",
        "def decode(tokens): return ''.join(uchars[t] if t < len(uchars) else '<BOS>' for t in tokens)\n",
        "\n",
        "print(f\"Dataset: {len(docs):,} names\")\n",
        "print(f\"Vocabulary: {vocab_size} tokens = {len(uchars)} chars + BOS(={BOS})\")\n",
        "print(f\"Sample: {docs[:8]}\")\n",
        "for name in [\"alice\", \"bob\", \"zara\"]:\n",
        "    assert decode(encode(name)) == name\n",
        "print(\"\\nEncode/decode roundtrip verified.\")"
      ],
      "id": "cell-13"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-14"
      },
      "source": [
        "### Training Sequence Format\n",
        "For `emma`: `[BOS, e, m, m, a, BOS]`. Model predicts each next character:\n",
        "```\n",
        "BOS->'e'  'e'->'m'  'm'->'m'  'm'->'a'  'a'->BOS\n",
        "```"
      ],
      "id": "cell-14"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-15",
        "outputId": "f690d0d1-e39f-4888-bfdc-ab4b460783aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training Sequence Examples ===\n",
            "\n",
            "'emma' -> [26, 4, 12, 12, 0, 26]\n",
            "    pos 0: 'BOS' -> 'e'\n",
            "    pos 1: 'e' -> 'm'\n",
            "    pos 2: 'm' -> 'm'\n",
            "    pos 3: 'm' -> 'a'\n",
            "    pos 4: 'a' -> 'BOS'\n",
            "\n",
            "'bob' -> [26, 1, 14, 1, 26]\n",
            "    pos 0: 'BOS' -> 'b'\n",
            "    pos 1: 'b' -> 'o'\n",
            "    pos 2: 'o' -> 'b'\n",
            "    pos 3: 'b' -> 'BOS'\n",
            "\n",
            "'lily' -> [26, 11, 8, 11, 24, 26]\n",
            "    pos 0: 'BOS' -> 'l'\n",
            "    pos 1: 'l' -> 'i'\n",
            "    pos 2: 'i' -> 'l'\n",
            "    pos 3: 'l' -> 'y'\n",
            "    pos 4: 'y' -> 'BOS'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Training Sequence Examples ===\\n\")\n",
        "for name in [\"emma\", \"bob\", \"lily\"]:\n",
        "    tokens = [BOS] + encode(name) + [BOS]\n",
        "    print(f\"\\'{name}\\' -> {tokens}\")\n",
        "    for i in range(len(tokens)-1):\n",
        "        ic = uchars[tokens[i]] if tokens[i] < len(uchars) else 'BOS'\n",
        "        tc = uchars[tokens[i+1]] if tokens[i+1] < len(uchars) else 'BOS'\n",
        "        print(f\"    pos {i}: \\'{ic}\\' -> \\'{tc}\\'\")\n",
        "    print()"
      ],
      "id": "cell-15"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-16"
      },
      "source": [
        "---\n",
        "## Part 3: Building Blocks (All from Atomic Operations)\n",
        "\n",
        "Every component is a composition of the 6 atomic ops. Autograd tracks every one.\n",
        "\n",
        "### 3.1: Linear Layer\n",
        "`output = W @ x` -- matrix-vector multiply. For 16x16: ~496 atomic ops."
      ],
      "id": "cell-16"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-17",
        "outputId": "b9f932f0-579a-40b8-f0a1-6f46bd560ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear: identity test passed\n",
            "\n",
            "Gradient check:\n",
            "     Input      Autograd     Numerical   Match\n",
            "  --------------------------------------------\n",
            "        x0      0.700000      0.700000      OK\n",
            "        x1      0.700000      0.700000      OK\n",
            "        x2      0.700000      0.700000      OK\n",
            "       w00      1.000000      1.000000      OK\n",
            "       w01      2.000000      2.000000      OK\n",
            "       w02      3.000000      3.000000      OK\n",
            "       w10      1.000000      1.000000      OK\n",
            "       w11      2.000000      2.000000      OK\n",
            "       w12      3.000000      3.000000      OK\n",
            "\n",
            "linear(16->16): 528 atomic ops\n"
          ]
        }
      ],
      "source": [
        "def linear(x, w):\n",
        "    \"\"\"Matrix-vector multiply. Built from multiply + add.\"\"\"\n",
        "    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n",
        "\n",
        "# Test\n",
        "x_t = [Value(1.0), Value(2.0), Value(3.0)]\n",
        "w_t = [[Value(1),Value(0),Value(0)], [Value(0),Value(1),Value(0)]]\n",
        "out = linear(x_t, w_t)\n",
        "assert out[0].data == 1.0 and out[1].data == 2.0\n",
        "print(\"linear: identity test passed\")\n",
        "\n",
        "print(\"\\nGradient check:\")\n",
        "def lin_fn(v):\n",
        "    x = v[:3]; w = [[v[3],v[4],v[5]], [v[6],v[7],v[8]]]\n",
        "    return sum(linear(x, w))\n",
        "check_gradients(lin_fn, [1.,2.,3.,.5,.3,.1,.2,.4,.6],\n",
        "                ['x0','x1','x2','w00','w01','w02','w10','w11','w12'])\n",
        "\n",
        "x16 = [Value(0.)]*16; w16 = [[Value(0.)]*16 for _ in range(16)]\n",
        "s = sum(linear(x16, w16)); st = count_graph_nodes(s)\n",
        "print(f\"\\nlinear(16->16): {st['ops']} atomic ops\")"
      ],
      "id": "cell-17"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-18"
      },
      "source": [
        "### 3.2: Softmax\n",
        "Converts logits to probabilities summing to 1. Uses: subtract, exp, add, divide."
      ],
      "id": "cell-18"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-19",
        "outputId": "7d080d36-a384-4c89-c82d-38d7008a8787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax([1,2,3]) = [0.0900, 0.2447, 0.6652]  sum=1.000000\n",
            "\n",
            "Gradient check (-log softmax = cross-entropy):\n",
            "     Input      Autograd     Numerical   Match\n",
            "  --------------------------------------------\n",
            "         a     -0.909969     -0.909969      OK\n",
            "         b      0.244728      0.244728      OK\n",
            "         c      0.665241      0.665241      OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def softmax(logits):\n",
        "    \"\"\"Logits -> probabilities (sum to 1).\"\"\"\n",
        "    max_val = max(val.data for val in logits)\n",
        "    exps = [(val - max_val).exp() for val in logits]\n",
        "    total = sum(exps)\n",
        "    return [e / total for e in exps]\n",
        "\n",
        "probs = softmax([Value(1.0), Value(2.0), Value(3.0)])\n",
        "psum = sum(p.data for p in probs)\n",
        "assert abs(psum - 1.0) < 1e-6\n",
        "print(f\"softmax([1,2,3]) = [{', '.join(f'{p.data:.4f}' for p in probs)}]  sum={psum:.6f}\")\n",
        "\n",
        "print(\"\\nGradient check (-log softmax = cross-entropy):\")\n",
        "check_gradients(lambda v: -(softmax(v)[0]).log(), [1.,2.,3.], ['a','b','c'])"
      ],
      "id": "cell-19"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-20"
      },
      "source": [
        "### 3.3: RMSNorm\n",
        "Normalize to consistent scale: `output = x / sqrt(mean(x^2) + eps)`"
      ],
      "id": "cell-20"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-21",
        "outputId": "04350e44-3ea2-43f2-c1cf-951e91a39c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmsnorm([1,2,3,4]) = [0.3651, 0.7303, 1.0954, 1.4606]\n",
            "  Mean of squares: 0.999999 (should be ~1.0)\n",
            "\n",
            "Gradient check:\n",
            "     Input      Autograd     Numerical   Match\n",
            "  --------------------------------------------\n",
            "        x0      0.243432      0.243432      OK\n",
            "        x1      0.121716      0.121716      OK\n",
            "        x2      0.000000      0.000000      OK\n",
            "        x3     -0.121715     -0.121715      OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def rmsnorm(x):\n",
        "    \"\"\"Root Mean Square normalization.\"\"\"\n",
        "    ms = sum(xi * xi for xi in x) / len(x)\n",
        "    scale = (ms + 1e-5) ** -0.5\n",
        "    return [xi * scale for xi in x]\n",
        "\n",
        "x_t = [Value(1.), Value(2.), Value(3.), Value(4.)]\n",
        "x_n = rmsnorm(x_t)\n",
        "ms = sum(v.data**2 for v in x_n) / len(x_n)\n",
        "print(f\"rmsnorm([1,2,3,4]) = [{', '.join(f'{v.data:.4f}' for v in x_n)}]\")\n",
        "print(f\"  Mean of squares: {ms:.6f} (should be ~1.0)\")\n",
        "\n",
        "print(\"\\nGradient check:\")\n",
        "check_gradients(lambda v: sum(rmsnorm(v)), [1.,2.,3.,4.], ['x0','x1','x2','x3'])"
      ],
      "id": "cell-21"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-22"
      },
      "source": [
        "### Operation Count Summary"
      ],
      "id": "cell-22"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-23",
        "outputId": "ed14b02f-49c8-4120-db64-36ae70e1a7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Atomic Operations per Component ===\n",
            "\n",
            "  linear(16->16)      :   528 ops\n",
            "  linear(16->64)      :  2112 ops\n",
            "  softmax(27)         :   162 ops\n",
            "  rmsnorm(16)         :    67 ops\n",
            "\n",
            "A full forward pass chains THOUSANDS of these together.\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Atomic Operations per Component ===\\n\")\n",
        "for label, fn in [\n",
        "    (\"linear(16->16)\", lambda: sum(linear([Value(0.)]*16, [[Value(0.)]*16 for _ in range(16)]))),\n",
        "    (\"linear(16->64)\", lambda: sum(linear([Value(0.)]*16, [[Value(0.)]*16 for _ in range(64)]))),\n",
        "    (\"softmax(27)\",    lambda: sum(softmax([Value(.1*i) for i in range(27)]))),\n",
        "    (\"rmsnorm(16)\",    lambda: sum(rmsnorm([Value(.1*i) for i in range(16)]))),\n",
        "]:\n",
        "    s = count_graph_nodes(fn())\n",
        "    print(f\"  {label:20s}: {s['ops']:5d} ops\")\n",
        "print(f\"\\nA full forward pass chains THOUSANDS of these together.\")"
      ],
      "id": "cell-23"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-24"
      },
      "source": [
        "---\n",
        "## Part 4: Model Parameters\n",
        "\n",
        "~8,000 learnable Values. Config: 16-dim embedding, 4 heads, 1 layer, 16 context."
      ],
      "id": "cell-24"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-25",
        "outputId": "77f440bb-7e13-47c0-ff0f-5879144c58f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Model Parameters ===\n",
            "\n",
            "  wte                         27 x  16 =   432\n",
            "  wpe                         16 x  16 =   256\n",
            "  lm_head                     27 x  16 =   432\n",
            "  layer0.attn_wq              16 x  16 =   256\n",
            "  layer0.attn_wk              16 x  16 =   256\n",
            "  layer0.attn_wv              16 x  16 =   256\n",
            "  layer0.attn_wo              16 x  16 =   256\n",
            "  layer0.mlp_fc1              64 x  16 =  1024\n",
            "  layer0.mlp_fc2              16 x  64 =  1024\n",
            "  TOTAL                                4192\n"
          ]
        }
      ],
      "source": [
        "n_embd = 16; n_head = 4; n_layer = 1; block_size = 16\n",
        "head_dim = n_embd // n_head\n",
        "\n",
        "def matrix(nout, nin, std=0.08):\n",
        "    return [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n",
        "\n",
        "state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd),\n",
        "              'lm_head': matrix(vocab_size, n_embd)}\n",
        "for i in range(n_layer):\n",
        "    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc1'] = matrix(4*n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4*n_embd)\n",
        "\n",
        "params = [p for mat in state_dict.values() for row in mat for p in row]\n",
        "\n",
        "print(\"=== Model Parameters ===\\n\")\n",
        "total = 0\n",
        "for name, mat in state_dict.items():\n",
        "    r, c = len(mat), len(mat[0]); n = r*c; total += n\n",
        "    print(f\"  {name:25s}  {r:3d} x {c:3d} = {n:5d}\")\n",
        "print(f\"  {'TOTAL':25s}           {total:5d}\")"
      ],
      "id": "cell-25"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-26"
      },
      "source": [
        "---\n",
        "## Part 5: The GPT Architecture\n",
        "```\n",
        "Token -> [Embed + Position] -> RMSNorm\n",
        "      -> [Multi-Head Attention] + Residual -> RMSNorm\n",
        "      -> [MLP: Linear->ReLU->Linear] + Residual\n",
        "      -> [Linear -> Logits]\n",
        "```\n",
        "**Attention**: Q*K^T/sqrt(d) -> softmax -> weighted V. 4 heads in parallel.\n",
        "**Residual**: out = layer(x) + x. **KV Cache**: past keys/values saved."
      ],
      "id": "cell-26"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-27",
        "outputId": "c1faad60-31dc-4d2c-a241-bd9db6497afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT defined: 1 layer, 4 heads, 16-dim\n"
          ]
        }
      ],
      "source": [
        "def gpt(token_id, pos_id, keys, values):\n",
        "    \"\"\"GPT forward pass for one token. Every op is atomic, tracked by autograd.\"\"\"\n",
        "    tok_emb = state_dict['wte'][token_id]\n",
        "    pos_emb = state_dict['wpe'][pos_id]\n",
        "    x = [t + p for t, p in zip(tok_emb, pos_emb)]\n",
        "    x = rmsnorm(x)\n",
        "    for li in range(n_layer):\n",
        "        # Attention\n",
        "        x_res = x; x = rmsnorm(x)\n",
        "        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
        "        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
        "        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n",
        "        keys[li].append(k); values[li].append(v)\n",
        "        x_attn = []\n",
        "        for h in range(n_head):\n",
        "            hs = h * head_dim\n",
        "            q_h = q[hs:hs+head_dim]\n",
        "            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n",
        "            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n",
        "            al = [sum(q_h[j]*k_h[t][j] for j in range(head_dim))/head_dim**0.5\n",
        "                  for t in range(len(k_h))]\n",
        "            aw = softmax(al)\n",
        "            x_attn.extend([sum(aw[t]*v_h[t][j] for t in range(len(v_h)))\n",
        "                           for j in range(head_dim)])\n",
        "        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n",
        "        x = [a+b for a, b in zip(x, x_res)]\n",
        "        # MLP\n",
        "        x_res = x; x = rmsnorm(x)\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n",
        "        x = [xi.relu() for xi in x]\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n",
        "        x = [a+b for a, b in zip(x, x_res)]\n",
        "    return linear(x, state_dict['lm_head'])\n",
        "\n",
        "print(f\"GPT defined: {n_layer} layer, {n_head} heads, {n_embd}-dim\")"
      ],
      "id": "cell-27"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-28"
      },
      "source": [
        "### Test: Forward Pass & Computation Graph"
      ],
      "id": "cell-28"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-29",
        "outputId": "21fbcc8f-c860-4e62-a2bd-07e8a0f8abb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Forward Pass Test ===\n",
            "\n",
            "Top 5 predictions (untrained = random):\n",
            "  1. 'o' = 0.0617\n",
            "  2. 's' = 0.0545\n",
            "  3. 'c' = 0.0541\n",
            "  4. 'z' = 0.0506\n",
            "  5. 'k' = 0.0497\n",
            "\n",
            "Computation graph (1 token): 11,223 nodes (7,446 ops)\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Forward Pass Test ===\\n\")\n",
        "keys_t, values_t = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "logits = gpt(BOS, 0, keys_t, values_t)\n",
        "probs = softmax(logits)\n",
        "assert len(logits) == vocab_size\n",
        "assert abs(sum(p.data for p in probs) - 1.0) < 1e-6\n",
        "\n",
        "top5 = sorted(range(len(probs)), key=lambda i: probs[i].data, reverse=True)[:5]\n",
        "print(\"Top 5 predictions (untrained = random):\")\n",
        "for r, idx in enumerate(top5, 1):\n",
        "    ch = uchars[idx] if idx < len(uchars) else 'BOS'\n",
        "    print(f\"  {r}. \\'{ch}\\' = {probs[idx].data:.4f}\")\n",
        "\n",
        "st = count_graph_nodes(-probs[0].log())\n",
        "print(f\"\\nComputation graph (1 token): {st['total']:,} nodes ({st['ops']:,} ops)\")"
      ],
      "id": "cell-29"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-30"
      },
      "source": [
        "### Full Sequence Graph Analysis"
      ],
      "id": "cell-30"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-31",
        "outputId": "7def9241-62a5-4da1-b921-7c08fa9de3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Processing 'emma' token by token ===\n",
            "\n",
            "  pos 0: 'BOS'->'e'  loss=3.1097  graph=11,223 nodes\n",
            "  pos 1: 'e'->'m'  loss=4.2117  graph=12,535 nodes\n",
            "  pos 2: 'm'->'m'  loss=3.6083  graph=13,847 nodes\n",
            "  pos 3: 'm'->'a'  loss=3.5067  graph=15,143 nodes\n",
            "  pos 4: 'a'->'BOS'  loss=3.0409  graph=16,455 nodes\n",
            "\n",
            "Total loss: 3.4955\n",
            "Total graph: 43,091 nodes (38,116 ops)\n",
            "Backward traversed 43,091 nodes\n",
            "Params with gradients: 3616/4192\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Processing \\'emma\\' token by token ===\\n\")\n",
        "tokens = [BOS] + encode(\"emma\") + [BOS]; n = len(tokens)-1\n",
        "keys_a, values_a = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "losses_a = []\n",
        "for pos_id in range(n):\n",
        "    tid, tgt = tokens[pos_id], tokens[pos_id+1]\n",
        "    logits = gpt(tid, pos_id, keys_a, values_a)\n",
        "    probs = softmax(logits); lt = -probs[tgt].log(); losses_a.append(lt)\n",
        "    ic = uchars[tid] if tid<len(uchars) else 'BOS'\n",
        "    tc = uchars[tgt] if tgt<len(uchars) else 'BOS'\n",
        "    st = count_graph_nodes(lt)\n",
        "    print(f\"  pos {pos_id}: \\'{ic}\\'->\\'{tc}\\'  loss={lt.data:.4f}  graph={st['total']:,} nodes\")\n",
        "total_loss = (1/n) * sum(losses_a)\n",
        "st = count_graph_nodes(total_loss)\n",
        "print(f\"\\nTotal loss: {total_loss.data:.4f}\")\n",
        "print(f\"Total graph: {st['total']:,} nodes ({st['ops']:,} ops)\")\n",
        "nn = total_loss.backward()\n",
        "print(f\"Backward traversed {nn:,} nodes\")\n",
        "print(f\"Params with gradients: {sum(1 for p in params if p.grad!=0)}/{len(params)}\")\n",
        "for p in params: p.grad = 0"
      ],
      "id": "cell-31"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-32"
      },
      "source": [
        "---\n",
        "## Part 6: Anatomy of a Training Step\n",
        "\n",
        "Before training 1000 steps, let's dissect ONE step:\n",
        "1. **Forward**: name -> predictions -> loss\n",
        "2. **Backward**: gradients via chain rule through entire graph\n",
        "3. **Adam update**: adjust each parameter\n",
        "4. **Zero**: reset gradients"
      ],
      "id": "cell-32"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-33",
        "outputId": "d512967f-bbc9-4683-c384-871155c10239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== One Training Step (Detailed) ===\n",
            "\n",
            "Name: 'yuheng'  (7 predictions)\n",
            "\n",
            "FORWARD:\n",
            "  'BOS'->'y' loss=3.628  top3=['o':0.06 's':0.05 'c':0.05]  \n",
            "  'y'->'u' loss=3.215  top3=['q':0.06 'd':0.05 'r':0.05]  \n",
            "  'u'->'h' loss=3.561  top3=['q':0.07 'e':0.07 'g':0.06]  \n",
            "  'h'->'e' loss=3.542  top3=['k':0.08 'l':0.06 'c':0.05]  \n",
            "  'e'->'n' loss=3.074  top3=['z':0.07 'e':0.06 'b':0.05]  \n",
            "  'n'->'g' loss=3.238  top3=['e':0.06 'q':0.06 'n':0.05]  \n",
            "  'g'->'BOS' loss=3.304  top3=['z':0.06 't':0.05 'q':0.05]  \n",
            "  Avg loss: 3.3660\n",
            "\n",
            "BACKWARD:\n",
            "  59,647 nodes traversed\n",
            "  3728/4192 params got gradients\n",
            "  |grad| range: [0.000001, 0.341059]\n",
            "\n",
            "ADAM UPDATE (example: param[0]):\n",
            "  grad=0.000000 -> update=0.000000\n",
            "  -0.042732 -> -0.042732\n"
          ]
        }
      ],
      "source": [
        "print(\"=== One Training Step (Detailed) ===\\n\")\n",
        "doc = docs[0]\n",
        "tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n",
        "n = min(block_size, len(tokens)-1)\n",
        "print(f\"Name: \\'{doc}\\'  ({n} predictions)\\n\")\n",
        "\n",
        "# Forward\n",
        "print(\"FORWARD:\")\n",
        "ks, vs = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "losses = []\n",
        "for pos_id in range(n):\n",
        "    tid, tgt = tokens[pos_id], tokens[pos_id+1]\n",
        "    logits = gpt(tid, pos_id, ks, vs); probs = softmax(logits)\n",
        "    lt = -probs[tgt].log(); losses.append(lt)\n",
        "    ic = uchars[tid] if tid<len(uchars) else 'BOS'\n",
        "    tc = uchars[tgt] if tgt<len(uchars) else 'BOS'\n",
        "    top3 = sorted(range(len(probs)), key=lambda i: probs[i].data, reverse=True)[:3]\n",
        "    t3 = \" \".join(f\"\\'{uchars[i] if i<len(uchars) else 'BOS'}\\':{probs[i].data:.2f}\" for i in top3)\n",
        "    print(f\"  \\'{ic}\\'->\\'{tc}\\' loss={lt.data:.3f}  top3=[{t3}]  {'HIT' if tgt in top3 else ''}\")\n",
        "loss = (1/n)*sum(losses)\n",
        "print(f\"  Avg loss: {loss.data:.4f}\\n\")\n",
        "\n",
        "# Backward\n",
        "print(\"BACKWARD:\")\n",
        "nn = loss.backward()\n",
        "grads = [abs(p.grad) for p in params if p.grad!=0]\n",
        "print(f\"  {nn:,} nodes traversed\")\n",
        "print(f\"  {len(grads)}/{len(params)} params got gradients\")\n",
        "if grads: print(f\"  |grad| range: [{min(grads):.6f}, {max(grads):.6f}]\\n\")\n",
        "\n",
        "# Adam demo\n",
        "print(\"ADAM UPDATE (example: param[0]):\")\n",
        "p0 = params[0]; old = p0.data\n",
        "mh = (0.15*p0.grad)/(1-0.85); vh = (0.01*p0.grad**2)/(1-0.99)\n",
        "upd = 0.01*mh/(vh**0.5+1e-8)\n",
        "print(f\"  grad={p0.grad:.6f} -> update={upd:.6f}\")\n",
        "print(f\"  {old:.6f} -> {old-upd:.6f}\")\n",
        "for p in params: p.grad = 0"
      ],
      "id": "cell-33"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-34"
      },
      "source": [
        "---\n",
        "## Part 7: Training\n",
        "\n",
        "Adam optimizer with linear LR decay. 1000 steps. Watch the loss decrease."
      ],
      "id": "cell-34"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-35",
        "outputId": "c992dcd1-876a-4636-ee05-66f0e406efb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 1000 steps on 32,033 names...\n",
            "\n",
            "  step    1/1000 | loss 3.3660 | avg 3.3660 | lr 0.0100 | 2.4 s/s | 'yuheng'\n",
            "  step  100/1000 | loss 3.3669 | avg 2.7725 | lr 0.0090 | 2.7 s/s | 'fatumata'\n",
            "  step  200/1000 | loss 2.3097 | avg 2.5346 | lr 0.0080 | 2.8 s/s | 'ameliarose'\n",
            "  step  300/1000 | loss 2.3178 | avg 2.4640 | lr 0.0070 | 2.9 s/s | 'dmani'\n",
            "  step  400/1000 | loss 2.3428 | avg 2.4443 | lr 0.0060 | 2.9 s/s | 'bryler'\n",
            "  step  500/1000 | loss 2.0645 | avg 2.4640 | lr 0.0050 | 3.0 s/s | 'soraia'\n",
            "  step  600/1000 | loss 2.4851 | avg 2.4243 | lr 0.0040 | 3.0 s/s | 'brayven'\n",
            "  step  700/1000 | loss 2.3357 | avg 2.4133 | lr 0.0030 | 3.0 s/s | 'neely'\n",
            "  step  800/1000 | loss 2.2632 | avg 2.3896 | lr 0.0020 | 3.0 s/s | 'ayzlin'\n",
            "  step  900/1000 | loss 2.7785 | avg 2.3340 | lr 0.0010 | 3.0 s/s | 'boyce'\n",
            "  step 1000/1000 | loss 2.6497 | avg 2.2761 | lr 0.0000 | 3.0 s/s | 'akio'\n",
            "\n",
            "Done in 334s. Final loss: 2.6497\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.01; beta1, beta2, eps_adam = 0.85, 0.99, 1e-8\n",
        "m = [0.0]*len(params); v = [0.0]*len(params)\n",
        "num_steps = 1000; loss_history = []\n",
        "\n",
        "print(f\"Training {num_steps} steps on {len(docs):,} names...\\n\")\n",
        "start_time = datetime.now()\n",
        "\n",
        "for step in range(num_steps):\n",
        "    doc = docs[step % len(docs)]\n",
        "    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n",
        "    n = min(block_size, len(tokens)-1)\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    losses = []\n",
        "    for pos_id in range(n):\n",
        "        token_id, target_id = tokens[pos_id], tokens[pos_id+1]\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax(logits)\n",
        "        losses.append(-probs[target_id].log())\n",
        "    loss = (1/n)*sum(losses); loss_history.append(loss.data)\n",
        "    loss.backward()\n",
        "    lr_t = learning_rate * (1 - step/num_steps)\n",
        "    for i, p in enumerate(params):\n",
        "        m[i] = beta1*m[i] + (1-beta1)*p.grad\n",
        "        v[i] = beta2*v[i] + (1-beta2)*p.grad**2\n",
        "        m_hat = m[i]/(1-beta1**(step+1)); v_hat = v[i]/(1-beta2**(step+1))\n",
        "        p.data -= lr_t * m_hat / (v_hat**0.5 + eps_adam)\n",
        "        p.grad = 0\n",
        "    if (step+1) % 100 == 0 or step == 0:\n",
        "        elapsed = (datetime.now()-start_time).total_seconds()\n",
        "        avg = sum(loss_history[max(0,step-99):step+1])/min(step+1,100)\n",
        "        print(f\"  step {step+1:4d}/{num_steps} | loss {loss.data:.4f} | avg {avg:.4f} | \"\n",
        "              f\"lr {lr_t:.4f} | {(step+1)/elapsed:.1f} s/s | \\'{doc}\\'\")\n",
        "\n",
        "elapsed = (datetime.now()-start_time).total_seconds()\n",
        "print(f\"\\nDone in {elapsed:.0f}s. Final loss: {loss_history[-1]:.4f}\")"
      ],
      "id": "cell-35"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-36"
      },
      "source": [
        "### Training Analysis"
      ],
      "id": "cell-36"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-37",
        "outputId": "a85dbb0e-d986-4fea-929d-593b30bce4d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training Analysis ===\n",
            "\n",
            "  Steps    1- 100: 2.7725  ###########################\n",
            "  Steps  101- 300: 2.4993  ########################\n",
            "  Steps  301- 500: 2.4542  ########################\n",
            "  Steps  501- 700: 2.4188  ########################\n",
            "  Steps  701- 900: 2.3618  #######################\n",
            "  Steps  901-1000: 2.2761  ######################\n",
            "\n",
            "Improvement: 3.3660 -> 2.6497 (21.3%)\n",
            "Final perplexity: 9.7 (random=27, lower=better)\n",
            "Model is 2.8x better than random guessing\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Training Analysis ===\\n\")\n",
        "for s, e in [(0,100),(100,300),(300,500),(500,700),(700,900),(900,1000)]:\n",
        "    avg = sum(loss_history[s:e])/(e-s)\n",
        "    print(f\"  Steps {s+1:4d}-{e:4d}: {avg:.4f}  {'#'*int(avg*10)}\")\n",
        "imp = (loss_history[0]-loss_history[-1])/loss_history[0]*100\n",
        "ppl = math.exp(sum(loss_history[-100:])/100)\n",
        "print(f\"\\nImprovement: {loss_history[0]:.4f} -> {loss_history[-1]:.4f} ({imp:.1f}%)\")\n",
        "print(f\"Final perplexity: {ppl:.1f} (random={vocab_size}, lower=better)\")\n",
        "print(f\"Model is {vocab_size/ppl:.1f}x better than random guessing\")"
      ],
      "id": "cell-37"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-38"
      },
      "source": [
        "---\n",
        "## Part 8: Generation\n",
        "\n",
        "Sample from trained model: BOS -> predict -> sample -> feed back -> repeat.\n",
        "**Temperature**: low=conservative, high=creative."
      ],
      "id": "cell-38"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-39",
        "outputId": "b9951f72-ddee-4cc8-85bc-4fdc5c3ae4b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generate() defined\n"
          ]
        }
      ],
      "source": [
        "def generate(temperature=0.5):\n",
        "    \"\"\"Generate a name by sampling from the trained model.\"\"\"\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    token_id = BOS; chars = []\n",
        "    for pos_id in range(block_size):\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax([l/temperature for l in logits])\n",
        "        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n",
        "        if token_id == BOS: break\n",
        "        chars.append(uchars[token_id])\n",
        "    return ''.join(chars)\n",
        "\n",
        "print(\"generate() defined\")"
      ],
      "id": "cell-39"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-40",
        "outputId": "78f91969-1cb6-43bb-beaf-ee6a73c4db2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Step-by-step Generation ===\n",
            "\n",
            "Sample 1:\n",
            "  pos 0: ['a':0.31 'k':0.12 'j':0.10] -> 'k'\n",
            "  pos 1: ['a':0.77 'e':0.14 'i':0.05] -> 'a'\n",
            "  pos 2: ['r':0.37 'n':0.27 'l':0.07] -> 'm'\n",
            "  pos 3: ['i':0.44 'a':0.31 'e':0.17] -> 'o'\n",
            "  pos 4: ['n':0.83 'r':0.07 'l':0.03] -> 'n'\n",
            "  pos 5: ['BOS':0.84 'a':0.08 'n':0.06] -> BOS (done)\n",
            "  Result: 'kamon'\n",
            "\n",
            "Sample 2:\n",
            "  pos 0: ['a':0.31 'k':0.12 'j':0.10] -> 'a'\n",
            "  pos 1: ['n':0.41 'l':0.23 'r':0.16] -> 'n'\n",
            "  pos 2: ['a':0.57 'e':0.11 'n':0.08] -> 'n'\n",
            "  pos 3: ['a':0.58 'n':0.09 'e':0.08] -> BOS (done)\n",
            "  Result: 'ann'\n",
            "\n",
            "Sample 3:\n",
            "  pos 0: ['a':0.31 'k':0.12 'j':0.10] -> 'k'\n",
            "  pos 1: ['a':0.77 'e':0.14 'i':0.05] -> 'a'\n",
            "  pos 2: ['r':0.37 'n':0.27 'l':0.07] -> 'r'\n",
            "  pos 3: ['i':0.46 'a':0.25 'e':0.15] -> 'a'\n",
            "  pos 4: ['n':0.64 'BOS':0.17 'r':0.06] -> 'i'\n",
            "  pos 5: ['BOS':0.45 'n':0.23 'a':0.16] -> BOS (done)\n",
            "  Result: 'karai'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Step-by-step Generation ===\\n\")\n",
        "for si in range(3):\n",
        "    kg, vg = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    tid = BOS; chars = []\n",
        "    print(f\"Sample {si+1}:\")\n",
        "    for pos_id in range(block_size):\n",
        "        logits = gpt(tid, pos_id, kg, vg)\n",
        "        probs = softmax([l/0.5 for l in logits])\n",
        "        top3 = sorted(range(len(probs)), key=lambda i: probs[i].data, reverse=True)[:3]\n",
        "        t3 = \" \".join(f\"\\'{uchars[i] if i<len(uchars) else 'BOS'}\\':{probs[i].data:.2f}\" for i in top3)\n",
        "        tid = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n",
        "        if tid == BOS:\n",
        "            print(f\"  pos {pos_id}: [{t3}] -> BOS (done)\"); break\n",
        "        chars.append(uchars[tid])\n",
        "        print(f\"  pos {pos_id}: [{t3}] -> \\'{uchars[tid]}\\'\")\n",
        "    print(f\"  Result: \\'{''.join(chars)}\\'\\n\")"
      ],
      "id": "cell-40"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-41",
        "outputId": "59084b83-87ed-45c2-a3db-87fe09558174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Temperature Comparison ===\n",
            "\n",
            "  temp=0.3 (conservative): anare, sanara, elina, saran, anan, areli, jana, saya, anar, jarie\n",
            "  temp=0.5 (balanced    ): manna, erilina, anchio, erien, onana, alela, anton, kamin, reoron, jana\n",
            "  temp=0.8 (exploratory ): saiann, sha, jaighn, kasann, acahah, ganiy, d, ahsso, iara, sakna\n",
            "  temp=1.0 (creative    ): ketts, teron, wynya, sana, jaynann, jibyn, moree, amabem, irlil, enngyn\n",
            "  temp=1.5 (wild        ): macesuy, mhimara, viienny, raujeo, mij, ricina, paaplr, fafelas, panaml, ri\n",
            "\n",
            "============================================================\n",
            "  20 Generated Names (temperature=0.5)\n",
            "============================================================\n",
            "\n",
            "   1. delen\n",
            "   2. sari\n",
            "   3. dana\n",
            "   4. cajana\n",
            "   5. melezan\n",
            "   6. elana\n",
            "   7. tarar\n",
            "   8. jaran\n",
            "   9. annar\n",
            "  10. arana\n",
            "  11. maban\n",
            "  12. azin\n",
            "  13. jaren\n",
            "  14. jaiel\n",
            "  15. aniah\n",
            "  16. aler\n",
            "  17. jona\n",
            "  18. tazen\n",
            "  19. jainen\n",
            "  20. madann\n",
            "\n",
            "Unique: 20/20  Avg len: 5.0\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Temperature Comparison ===\\n\")\n",
        "for temp in [0.3, 0.5, 0.8, 1.0, 1.5]:\n",
        "    names = [generate(temp) for _ in range(10)]\n",
        "    label = {0.3:\"conservative\",0.5:\"balanced\",0.8:\"exploratory\",1.0:\"creative\",1.5:\"wild\"}[temp]\n",
        "    print(f\"  temp={temp} ({label:12s}): {', '.join(names)}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  20 Generated Names (temperature=0.5)\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "gen = []\n",
        "for i in range(20):\n",
        "    name = generate(0.5); gen.append(name); print(f\"  {i+1:2d}. {name}\")\n",
        "print(f\"\\nUnique: {len(set(gen))}/20  Avg len: {sum(len(n) for n in gen)/len(gen):.1f}\")"
      ],
      "id": "cell-41"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-42"
      },
      "source": [
        "---\n",
        "## Part 9: Analysis & Experiments"
      ],
      "id": "cell-42"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-43",
        "outputId": "b86365d7-a981-4967-9908-246b0db3935e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Model: Real Names vs Nonsense ===\n",
            "\n",
            "  'emma    ': loss=2.726  ppl=  15.3  #####################\n",
            "  'liam    ': loss=2.315  ppl=  10.1  ##################\n",
            "  'olivia  ': loss=2.248  ppl=   9.5  #################\n",
            "  'noah    ': loss=2.513  ppl=  12.3  ####################\n",
            "  'ava     ': loss=2.582  ppl=  13.2  ####################\n",
            "  'zzzzz   ': loss=4.173  ppl=  64.9  #################################\n",
            "  'qqqq    ': loss=7.187  ppl=1322.3  ########################################\n",
            "  'aeiou   ': loss=3.533  ppl=  34.2  ############################\n",
            "  'xyzw    ': loss=4.324  ppl=  75.5  ##################################\n",
            "\n",
            "Real names have lower loss than nonsense.\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Model: Real Names vs Nonsense ===\\n\")\n",
        "for name in [\"emma\",\"liam\",\"olivia\",\"noah\",\"ava\",\"zzzzz\",\"qqqq\",\"aeiou\",\"xyzw\"]:\n",
        "    tokens = [BOS]+encode(name)+[BOS]; n = len(tokens)-1\n",
        "    kt, vt = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    total = 0.\n",
        "    for pos_id in range(n):\n",
        "        logits = gpt(tokens[pos_id], pos_id, kt, vt)\n",
        "        probs = softmax(logits)\n",
        "        total += -probs[tokens[pos_id+1]].log().data\n",
        "    avg = total/n; ppl = math.exp(avg)\n",
        "    print(f\"  \\'{name:8s}\\': loss={avg:.3f}  ppl={ppl:6.1f}  {'#'*min(40,int(avg*8))}\")\n",
        "print(f\"\\nReal names have lower loss than nonsense.\")"
      ],
      "id": "cell-43"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-44",
        "outputId": "b25c234e-4437-4fc8-cf43-3e0b9a3be69b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Attention Patterns ('sarah'): ===\n",
            "\n",
            "Head 0:\n",
            "           BOS      s      a      r      a      h\n",
            "   BOS:  [1.00]\n",
            "     s:  [0.54] [0.46]\n",
            "     a:  [0.34] [0.35] [0.30]\n",
            "     r:   0.21  [0.32]  0.27   0.21 \n",
            "     a:   0.24   0.19   0.22   0.17   0.18 \n",
            "     h:   0.18   0.17   0.16   0.15   0.17   0.17 \n",
            "\n",
            "Head 1:\n",
            "           BOS      s      a      r      a      h\n",
            "   BOS:  [1.00]\n",
            "     s:  [0.48] [0.52]\n",
            "     a:   0.21  [0.58]  0.20 \n",
            "     r:   0.22   0.23   0.25   0.29 \n",
            "     a:   0.11   0.16   .     [0.32] [0.32]\n",
            "     h:   0.24   0.14   0.15   0.13   0.17   0.16 \n",
            "\n",
            "Head 2:\n",
            "           BOS      s      a      r      a      h\n",
            "   BOS:  [1.00]\n",
            "     s:  [0.58] [0.42]\n",
            "     a:   .     [0.90]  .    \n",
            "     r:   .     [0.58]  .      0.27 \n",
            "     a:   .      0.21   .     [0.58]  0.12 \n",
            "     h:  [0.39]  .     [0.35]  .      0.14   .    \n",
            "\n",
            "Head 3:\n",
            "           BOS      s      a      r      a      h\n",
            "   BOS:  [1.00]\n",
            "     s:  [0.45] [0.55]\n",
            "     a:  [0.30] [0.69]  .    \n",
            "     r:   0.24   0.27   0.27   0.22 \n",
            "     a:   0.14   0.28   .     [0.50]  .    \n",
            "     h:   .      .     [0.72]  .      0.14   .    \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Attention Patterns (\\'{}\\'): ===\\n\".format(\"sarah\"))\n",
        "name = \"sarah\"; tokens_v = [BOS]+encode(name); chars_v = ['BOS']+list(name)\n",
        "kv, vv = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "all_attn = []\n",
        "for pos_id in range(len(tokens_v)):\n",
        "    tid = tokens_v[pos_id]\n",
        "    x = [t+p for t,p in zip(state_dict['wte'][tid], state_dict['wpe'][pos_id])]\n",
        "    x = rmsnorm(x); li = 0; x = rmsnorm(x)\n",
        "    q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
        "    k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
        "    kv[li].append(k); vv[li].append(linear(x, state_dict[f'layer{li}.attn_wv']))\n",
        "    pa = []\n",
        "    for h in range(n_head):\n",
        "        hs = h*head_dim; q_h = q[hs:hs+head_dim]\n",
        "        k_h = [ki[hs:hs+head_dim] for ki in kv[li]]\n",
        "        al = [sum(q_h[j]*k_h[t][j] for j in range(head_dim))/head_dim**0.5\n",
        "              for t in range(len(k_h))]\n",
        "        pa.append([w.data for w in softmax(al)])\n",
        "    all_attn.append(pa)\n",
        "for h in range(n_head):\n",
        "    print(f\"Head {h}:\")\n",
        "    print(\"       \"+\"\".join(f\"{c:>7s}\" for c in chars_v))\n",
        "    for pi in range(len(tokens_v)):\n",
        "        row = f\"  {chars_v[pi]:>4s}: \"\n",
        "        for t in range(pi+1):\n",
        "            w = all_attn[pi][h][t]\n",
        "            if w > 0.3: row += f\" [{w:.2f}]\"\n",
        "            elif w > 0.1: row += f\"  {w:.2f} \"\n",
        "            else: row += f\"  .    \"\n",
        "        print(row)\n",
        "    print()"
      ],
      "id": "cell-44"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-45",
        "outputId": "ed5bfb98-5f4d-478a-8bd2-308e9520c805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Gradient Flow by Layer ===\n",
            "\n",
            "  wte                      : mean|g|=0.016977  max=0.273229  ########\n",
            "  wpe                      : mean|g|=0.028649  max=0.273229  ##############\n",
            "  lm_head                  : mean|g|=0.042936  max=0.468020  #####################\n",
            "  layer0.attn_wq           : mean|g|=0.002691  max=0.030330  #\n",
            "  layer0.attn_wk           : mean|g|=0.002738  max=0.026094  #\n",
            "  layer0.attn_wv           : mean|g|=0.014620  max=0.076216  #######\n",
            "  layer0.attn_wo           : mean|g|=0.016016  max=0.077669  ########\n",
            "  layer0.mlp_fc1           : mean|g|=0.009036  max=0.132025  ####\n",
            "  layer0.mlp_fc2           : mean|g|=0.005368  max=0.082592  ##\n",
            "\n",
            "Larger gradients = more learning in that component\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Gradient Flow by Layer ===\\n\")\n",
        "doc = docs[0]\n",
        "tokens = [BOS]+[uchars.index(ch) for ch in doc]+[BOS]; n = min(block_size, len(tokens)-1)\n",
        "kg, vg = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "losses = []\n",
        "for pos_id in range(n):\n",
        "    logits = gpt(tokens[pos_id], pos_id, kg, vg)\n",
        "    probs = softmax(logits); losses.append(-probs[tokens[pos_id+1]].log())\n",
        "loss = (1/n)*sum(losses); loss.backward()\n",
        "for name, mat in state_dict.items():\n",
        "    g = [abs(p.grad) for row in mat for p in row]\n",
        "    mg = sum(g)/len(g); mx = max(g)\n",
        "    print(f\"  {name:25s}: mean|g|={mg:.6f}  max={mx:.6f}  {'#'*min(30,int(mg*500))}\")\n",
        "for p in params: p.grad = 0\n",
        "print(f\"\\nLarger gradients = more learning in that component\")"
      ],
      "id": "cell-45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-46"
      },
      "source": [
        "---\n",
        "## Part 10: Exercises\n",
        "\n",
        "### Exercise 1: Predict the Gradient\n",
        "Before running: `f(x) = x^2 + 3x + 1` at `x = 2`. What is `df/dx`?"
      ],
      "id": "cell-46"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-47",
        "outputId": "d1d9cc56-ebea-4140-af90-dc092df62711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f(2) = 11.0 (expected 11)  df/dx = 7.0 (expected 7)\n"
          ]
        }
      ],
      "source": [
        "x = Value(2.0); f = x**2 + x*3 + 1; f.backward()\n",
        "print(f\"f(2) = {f.data} (expected 11)  df/dx = {x.grad} (expected 7)\")"
      ],
      "id": "cell-47"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-48"
      },
      "source": [
        "### Exercise 2: Count the Math\n",
        "How many operations for 'emma'? Estimate, then run:"
      ],
      "id": "cell-48"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-49",
        "outputId": "d69e091e-1a1a-4fad-8a7d-60f21780693f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'emma' (5 predictions): 43,091 graph nodes, 38,116 operations\n",
            "Over 1000 training steps: ~38,116,000 total ops\n",
            "All differentiated by our 30-line autograd engine!\n"
          ]
        }
      ],
      "source": [
        "tokens = [BOS]+encode(\"emma\")+[BOS]; n = len(tokens)-1\n",
        "kc, vc = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "lc = []\n",
        "for pos_id in range(n):\n",
        "    logits = gpt(tokens[pos_id], pos_id, kc, vc)\n",
        "    probs = softmax(logits); lc.append(-probs[tokens[pos_id+1]].log())\n",
        "st = count_graph_nodes((1/n)*sum(lc))\n",
        "print(f\"'emma' ({n} predictions): {st['total']:,} graph nodes, {st['ops']:,} operations\")\n",
        "print(f\"Over {num_steps} training steps: ~{st['ops']*num_steps:,} total ops\")\n",
        "print(f\"All differentiated by our 30-line autograd engine!\")"
      ],
      "id": "cell-49"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-50"
      },
      "source": [
        "### Exercise 3: Experiment\n",
        "Try changing hyperparameters (re-run from Part 4):\n",
        "```python\n",
        "n_embd = 32       # more capacity\n",
        "n_layer = 2       # deeper\n",
        "learning_rate = 0.1   # too high?\n",
        "num_steps = 2000  # more training\n",
        "temperature = 0.1 # very conservative\n",
        "temperature = 2.0 # very random\n",
        "```"
      ],
      "id": "cell-50"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-51"
      },
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "**The 6 atomic operations:** `+`, `*`, `**`, `log`, `exp`, `relu`\n",
        "\n",
        "**Everything is composition:**\n",
        "Linear = mul + add | Softmax = sub + exp + add + div | RMSNorm = mul + add + pow\n",
        "Attention = linear + softmax + linear | MLP = linear + relu + linear | Loss = softmax + log\n",
        "\n",
        "**What's different in production LLMs (GPT-4, Claude)?**\n",
        "Same architecture, billions of parameters, GPU tensors instead of scalar Values,\n",
        "CUDA kernels instead of Python loops. **But the math is identical.**\n",
        "\n",
        "> *\"This file is the complete algorithm. Everything else is just efficiency.\"*\n",
        "\n",
        "**Next steps:**\n",
        "- [Karpathy: Zero to Hero (YouTube)](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n",
        "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "- Implement in PyTorch for 100x speedup"
      ],
      "id": "cell-51"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}