{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Agents Part 2: Progressive Filtering - Quality Control at Scale\n",
    "\n",
    "**Time**: 45 minutes | **Level**: Intermediate | **Prerequisite**: Part 1\n",
    "\n",
    "## The Problem We're Solving\n",
    "\n",
    "You have 10,000 trajectories from multi-agent systems. But:\n",
    "- Some are garbage (agents went off-topic)\n",
    "- Some are redundant (same solution 100 times)\n",
    "- Some are gold (perfect agent collaboration)\n",
    "\n",
    "**Bad training data = Bad AFM**\n",
    "\n",
    "Progressive filtering finds the gold. Let's build it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Realistic Trajectories (Good and Bad)\n",
    "\n",
    "First, let's create trajectories with varying quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def generate_trajectory(task, quality=\"good\"):\n",
    "    \"\"\"Generate trajectories of different quality levels\"\"\"\n",
    "    \n",
    "    if quality == \"good\":\n",
    "        # Good trajectory: clear, complete, collaborative\n",
    "        trajectory = [\n",
    "            {\"agent\": \"Planner\", \"output\": f\"Breaking down '{task}' into 3 clear steps...\"},\n",
    "            {\"agent\": \"Coder\", \"output\": f\"Implementing solution with proper error handling...\"},\n",
    "            {\"agent\": \"Reviewer\", \"output\": f\"Code reviewed. Tests pass. Ready for production.\"}\n",
    "        ]\n",
    "    elif quality == \"bad\":\n",
    "        # Bad trajectory: incomplete, errors, off-topic\n",
    "        trajectory = [\n",
    "            {\"agent\": \"Planner\", \"output\": f\"I don't understand '{task}'...\"},\n",
    "            {\"agent\": \"Coder\", \"output\": f\"Error: undefined variable...\"},\n",
    "            {\"agent\": \"Reviewer\", \"output\": f\"This doesn't work.\"}\n",
    "        ]\n",
    "    else:  # medium\n",
    "        # Medium trajectory: okay but not great\n",
    "        trajectory = [\n",
    "            {\"agent\": \"Planner\", \"output\": f\"Working on '{task}'...\"},\n",
    "            {\"agent\": \"Coder\", \"output\": f\"Basic implementation done.\"},\n",
    "            {\"agent\": \"Reviewer\", \"output\": f\"Looks okay.\"}\n",
    "        ]\n",
    "    \n",
    "    return {\n",
    "        \"task\": task,\n",
    "        \"trajectory\": trajectory,\n",
    "        \"quality\": quality  # Ground truth for testing\n",
    "    }\n",
    "\n",
    "# Generate a mix of trajectories\n",
    "tasks = [\"Build API\", \"Fix bug\", \"Add feature\", \"Write tests\", \"Deploy app\"]\n",
    "trajectories = []\n",
    "\n",
    "for _ in range(30):  # Generate 30 trajectories\n",
    "    task = random.choice(tasks)\n",
    "    quality = random.choice([\"good\", \"good\", \"medium\", \"bad\"])  # 50% good, 25% medium, 25% bad\n",
    "    trajectories.append(generate_trajectory(task, quality))\n",
    "\n",
    "# Show distribution\n",
    "quality_counts = {}\n",
    "for t in trajectories:\n",
    "    q = t['quality']\n",
    "    quality_counts[q] = quality_counts.get(q, 0) + 1\n",
    "\n",
    "print(\"📊 Generated Trajectory Quality Distribution:\")\n",
    "for quality, count in quality_counts.items():\n",
    "    bar = '█' * (count // 2)\n",
    "    print(f\"  {quality:7} {bar:15} {count} trajectories\")\n",
    "print(f\"\\nTotal: {len(trajectories)} trajectories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Quality Metrics\n",
    "\n",
    "What makes a trajectory good for training an AFM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trajectory_metrics(trajectory_data):\n",
    "    \"\"\"Calculate quality metrics for a trajectory\"\"\"\n",
    "    \n",
    "    trajectory = trajectory_data['trajectory']\n",
    "    metrics = {}\n",
    "    \n",
    "    # Metric 1: Total output length (longer usually = more detailed)\n",
    "    total_length = sum(len(step['output']) for step in trajectory)\n",
    "    metrics['output_length'] = total_length\n",
    "    \n",
    "    # Metric 2: Agent diversity (all agents should contribute)\n",
    "    unique_agents = len(set(step['agent'] for step in trajectory))\n",
    "    metrics['agent_diversity'] = unique_agents / len(trajectory) if trajectory else 0\n",
    "    \n",
    "    # Metric 3: Completion indicators (look for success signals)\n",
    "    final_output = trajectory[-1]['output'].lower() if trajectory else \"\"\n",
    "    success_words = ['complete', 'success', 'ready', 'pass', 'done', 'works']\n",
    "    metrics['has_completion'] = any(word in final_output for word in success_words)\n",
    "    \n",
    "    # Metric 4: Error indicators (look for failure signals)\n",
    "    all_outputs = ' '.join(step['output'].lower() for step in trajectory)\n",
    "    error_words = ['error', 'fail', 'undefined', \"doesn't work\", \"don't understand\"]\n",
    "    metrics['has_errors'] = any(word in all_outputs for word in error_words)\n",
    "    \n",
    "    # Metric 5: Progressive depth (each step should build on previous)\n",
    "    if len(trajectory) > 1:\n",
    "        lengths = [len(step['output']) for step in trajectory]\n",
    "        metrics['is_progressive'] = lengths[-1] >= lengths[0]  # Last step more detailed\n",
    "    else:\n",
    "        metrics['is_progressive'] = False\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Test on one trajectory\n",
    "example = trajectories[0]\n",
    "metrics = calculate_trajectory_metrics(example)\n",
    "\n",
    "print(f\"📏 Metrics for trajectory (quality={example['quality']}):\")\n",
    "print(json.dumps(metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Simple Scoring Function\n",
    "\n",
    "Combine metrics into a single quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_trajectory(trajectory_data):\n",
    "    \"\"\"Score trajectory from 0-100 based on quality metrics\"\"\"\n",
    "    \n",
    "    metrics = calculate_trajectory_metrics(trajectory_data)\n",
    "    score = 50  # Start at neutral\n",
    "    \n",
    "    # Positive factors\n",
    "    if metrics['output_length'] > 100:\n",
    "        score += 10\n",
    "    if metrics['output_length'] > 200:\n",
    "        score += 10\n",
    "    if metrics['agent_diversity'] > 0.8:\n",
    "        score += 15\n",
    "    if metrics['has_completion']:\n",
    "        score += 20\n",
    "    if metrics['is_progressive']:\n",
    "        score += 10\n",
    "    \n",
    "    # Negative factors\n",
    "    if metrics['has_errors']:\n",
    "        score -= 30\n",
    "    if metrics['output_length'] < 50:\n",
    "        score -= 20\n",
    "    if metrics['agent_diversity'] < 0.5:\n",
    "        score -= 15\n",
    "    \n",
    "    # Clamp to 0-100\n",
    "    return max(0, min(100, score))\n",
    "\n",
    "# Score all trajectories\n",
    "for t in trajectories:\n",
    "    t['score'] = score_trajectory(t)\n",
    "\n",
    "# Show score distribution by quality\n",
    "print(\"📊 Average Scores by Quality:\")\n",
    "for quality in ['good', 'medium', 'bad']:\n",
    "    scores = [t['score'] for t in trajectories if t['quality'] == quality]\n",
    "    if scores:\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        print(f\"  {quality:7} → {avg_score:.1f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Progressive Filtering Pipeline\n",
    "\n",
    "Now the magic: filter in stages, getting stricter each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressive_filter(trajectories, stages=3):\n",
    "    \"\"\"Apply progressive filtering to trajectories\"\"\"\n",
    "    \n",
    "    filtered = trajectories.copy()\n",
    "    \n",
    "    print(\"🔄 PROGRESSIVE FILTERING PIPELINE\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Starting with {len(filtered)} trajectories\\n\")\n",
    "    \n",
    "    # Define filtering thresholds for each stage\n",
    "    thresholds = [30, 50, 70]  # Increasingly strict\n",
    "    \n",
    "    for stage, threshold in enumerate(thresholds[:stages], 1):\n",
    "        print(f\"📍 Stage {stage}: Score threshold >= {threshold}\")\n",
    "        \n",
    "        # Filter based on score\n",
    "        before_count = len(filtered)\n",
    "        filtered = [t for t in filtered if t['score'] >= threshold]\n",
    "        after_count = len(filtered)\n",
    "        \n",
    "        # Show what happened\n",
    "        removed = before_count - after_count\n",
    "        removal_rate = (removed / before_count * 100) if before_count > 0 else 0\n",
    "        \n",
    "        print(f\"  Removed: {removed} ({removal_rate:.1f}%)\")\n",
    "        print(f\"  Remaining: {after_count}\")\n",
    "        \n",
    "        # Show quality distribution at this stage\n",
    "        quality_dist = {}\n",
    "        for t in filtered:\n",
    "            q = t['quality']\n",
    "            quality_dist[q] = quality_dist.get(q, 0) + 1\n",
    "        print(f\"  Quality: {quality_dist}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"✅ Final: {len(filtered)}/{len(trajectories)} trajectories kept\")\n",
    "    print(f\"📈 Quality improvement: {(len(filtered)/len(trajectories)*100):.1f}% survival rate\")\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Apply progressive filtering\n",
    "filtered_trajectories = progressive_filter(trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Filtering Effects\n",
    "\n",
    "Let's see how filtering improves quality distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filtering_effect(original, filtered):\n",
    "    \"\"\"Visualize the effect of filtering on quality distribution\"\"\"\n",
    "    \n",
    "    print(\"\\n📊 FILTERING EFFECT VISUALIZATION\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Calculate distributions\n",
    "    def get_distribution(trajectory_list):\n",
    "        dist = {'good': 0, 'medium': 0, 'bad': 0}\n",
    "        for t in trajectory_list:\n",
    "            dist[t['quality']] += 1\n",
    "        return dist\n",
    "    \n",
    "    orig_dist = get_distribution(original)\n",
    "    filt_dist = get_distribution(filtered)\n",
    "    \n",
    "    # Display side by side\n",
    "    print(\"BEFORE FILTERING:\")\n",
    "    for quality in ['good', 'medium', 'bad']:\n",
    "        count = orig_dist[quality]\n",
    "        pct = (count / len(original) * 100) if original else 0\n",
    "        bar = '█' * int(pct / 2)\n",
    "        print(f\"  {quality:7} {bar:25} {count:2} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nAFTER FILTERING:\")\n",
    "    for quality in ['good', 'medium', 'bad']:\n",
    "        count = filt_dist[quality]\n",
    "        pct = (count / len(filtered) * 100) if filtered else 0\n",
    "        bar = '█' * int(pct / 2)\n",
    "        print(f\"  {quality:7} {bar:25} {count:2} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    if filtered:\n",
    "        good_before = orig_dist['good'] / len(original) * 100\n",
    "        good_after = filt_dist['good'] / len(filtered) * 100\n",
    "        improvement = good_after - good_before\n",
    "        \n",
    "        print(f\"\\n✨ Quality Improvement:\")\n",
    "        print(f\"  Good trajectories: {good_before:.1f}% → {good_after:.1f}%\")\n",
    "        print(f\"  Improvement: +{improvement:.1f}% good trajectories\")\n",
    "\n",
    "visualize_filtering_effect(trajectories, filtered_trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Advanced Filtering - Diversity & Deduplication\n",
    "\n",
    "Good trajectories aren't enough. We need DIVERSE good trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(traj1, traj2):\n",
    "    \"\"\"Calculate similarity between two trajectories (0-1)\"\"\"\n",
    "    \n",
    "    # Simple approach: compare outputs\n",
    "    outputs1 = ' '.join(s['output'] for s in traj1['trajectory'])\n",
    "    outputs2 = ' '.join(s['output'] for s in traj2['trajectory'])\n",
    "    \n",
    "    # Character-level similarity (simplified)\n",
    "    common_chars = sum(1 for c1, c2 in zip(outputs1[:100], outputs2[:100]) if c1 == c2)\n",
    "    max_len = max(len(outputs1[:100]), len(outputs2[:100]))\n",
    "    \n",
    "    return common_chars / max_len if max_len > 0 else 0\n",
    "\n",
    "def diversity_filter(trajectories, min_diversity=0.3):\n",
    "    \"\"\"Keep only diverse trajectories (remove near-duplicates)\"\"\"\n",
    "    \n",
    "    print(\"\\n🌈 DIVERSITY FILTERING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    diverse_set = []\n",
    "    \n",
    "    for i, traj in enumerate(trajectories):\n",
    "        # Check similarity with existing diverse set\n",
    "        is_diverse = True\n",
    "        \n",
    "        for existing in diverse_set:\n",
    "            similarity = calculate_similarity(traj, existing)\n",
    "            if similarity > (1 - min_diversity):  # Too similar\n",
    "                is_diverse = False\n",
    "                break\n",
    "        \n",
    "        if is_diverse:\n",
    "            diverse_set.append(traj)\n",
    "            print(f\"  ✓ Added trajectory {i+1} (unique)\")\n",
    "        else:\n",
    "            print(f\"  ✗ Skipped trajectory {i+1} (too similar)\")\n",
    "    \n",
    "    print(f\"\\nResult: {len(diverse_set)}/{len(trajectories)} diverse trajectories\")\n",
    "    return diverse_set\n",
    "\n",
    "# Apply diversity filtering\n",
    "diverse_trajectories = diversity_filter(filtered_trajectories[:10])  # Demo on first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: The Complete Pipeline\n",
    "\n",
    "Put it all together: score → filter → diversify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryFilter:\n",
    "    \"\"\"Complete progressive filtering pipeline for CoA\"\"\"\n",
    "    \n",
    "    def __init__(self, min_score=50, diversity_threshold=0.3):\n",
    "        self.min_score = min_score\n",
    "        self.diversity_threshold = diversity_threshold\n",
    "        self.stats = {}\n",
    "    \n",
    "    def process(self, trajectories):\n",
    "        \"\"\"Apply complete filtering pipeline\"\"\"\n",
    "        \n",
    "        print(\"🚀 COMPLETE FILTERING PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Calculate scores\n",
    "        print(\"\\n1️⃣ Scoring trajectories...\")\n",
    "        for t in trajectories:\n",
    "            t['score'] = score_trajectory(t)\n",
    "        avg_score = sum(t['score'] for t in trajectories) / len(trajectories)\n",
    "        print(f\"   Average score: {avg_score:.1f}\")\n",
    "        \n",
    "        # Step 2: Quality filtering\n",
    "        print(\"\\n2️⃣ Quality filtering...\")\n",
    "        quality_filtered = [t for t in trajectories if t['score'] >= self.min_score]\n",
    "        print(f\"   Kept: {len(quality_filtered)}/{len(trajectories)}\")\n",
    "        \n",
    "        # Step 3: Diversity filtering\n",
    "        print(\"\\n3️⃣ Diversity filtering...\")\n",
    "        final_set = []\n",
    "        for t in quality_filtered:\n",
    "            is_unique = True\n",
    "            for existing in final_set:\n",
    "                if calculate_similarity(t, existing) > (1 - self.diversity_threshold):\n",
    "                    is_unique = False\n",
    "                    break\n",
    "            if is_unique:\n",
    "                final_set.append(t)\n",
    "        print(f\"   Kept: {len(final_set)}/{len(quality_filtered)}\")\n",
    "        \n",
    "        # Step 4: Final statistics\n",
    "        print(\"\\n4️⃣ Final statistics:\")\n",
    "        self.stats = {\n",
    "            'input_count': len(trajectories),\n",
    "            'output_count': len(final_set),\n",
    "            'retention_rate': len(final_set) / len(trajectories) * 100,\n",
    "            'avg_score_before': avg_score,\n",
    "            'avg_score_after': sum(t['score'] for t in final_set) / len(final_set) if final_set else 0\n",
    "        }\n",
    "        \n",
    "        for key, value in self.stats.items():\n",
    "            if 'count' in key:\n",
    "                print(f\"   {key}: {value}\")\n",
    "            else:\n",
    "                print(f\"   {key}: {value:.1f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"✅ Filtering complete: {len(final_set)} high-quality trajectories\")\n",
    "        \n",
    "        return final_set\n",
    "\n",
    "# Run the complete pipeline\n",
    "filter_pipeline = TrajectoryFilter(min_score=50, diversity_threshold=0.3)\n",
    "final_trajectories = filter_pipeline.process(trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Why This Matters for AFM Performance\n",
    "\n",
    "Let's simulate how filtering affects final model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_afm_performance(training_trajectories):\n",
    "    \"\"\"Simulate AFM performance based on training data quality\"\"\"\n",
    "    \n",
    "    # Calculate quality metrics\n",
    "    quality_scores = [t['score'] for t in training_trajectories]\n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    \n",
    "    # Count quality distribution\n",
    "    good_count = sum(1 for t in training_trajectories if t['quality'] == 'good')\n",
    "    bad_count = sum(1 for t in training_trajectories if t['quality'] == 'bad')\n",
    "    \n",
    "    # Simulate performance (simplified model)\n",
    "    base_performance = 0.40  # Base GAIA score\n",
    "    \n",
    "    # Good trajectories improve performance\n",
    "    performance_boost = (good_count / len(training_trajectories)) * 0.15 if training_trajectories else 0\n",
    "    \n",
    "    # Bad trajectories hurt performance\n",
    "    performance_penalty = (bad_count / len(training_trajectories)) * 0.10 if training_trajectories else 0\n",
    "    \n",
    "    # Quality bonus\n",
    "    quality_bonus = (avg_quality / 100) * 0.05\n",
    "    \n",
    "    final_performance = base_performance + performance_boost - performance_penalty + quality_bonus\n",
    "    \n",
    "    return min(final_performance, 0.60)  # Cap at 60% for realism\n",
    "\n",
    "# Compare performance with and without filtering\n",
    "print(\"🎯 AFM PERFORMANCE SIMULATION\\n\" + \"=\"*50)\n",
    "\n",
    "# Without filtering\n",
    "perf_unfiltered = simulate_afm_performance(trajectories)\n",
    "print(f\"Without filtering:\")\n",
    "print(f\"  Training size: {len(trajectories)} trajectories\")\n",
    "print(f\"  GAIA score: {perf_unfiltered:.3f} ({perf_unfiltered*100:.1f}%)\")\n",
    "\n",
    "# With filtering\n",
    "perf_filtered = simulate_afm_performance(final_trajectories)\n",
    "print(f\"\\nWith progressive filtering:\")\n",
    "print(f\"  Training size: {len(final_trajectories)} trajectories\")\n",
    "print(f\"  GAIA score: {perf_filtered:.3f} ({perf_filtered*100:.1f}%)\")\n",
    "\n",
    "# Show improvement\n",
    "improvement = (perf_filtered - perf_unfiltered) / perf_unfiltered * 100\n",
    "print(f\"\\n📈 Improvement: +{improvement:.1f}% relative performance\")\n",
    "print(f\"\\n💡 This is why CoA achieves 55.3% on GAIA!\")\n",
    "print(f\"   Quality data > Quantity of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Build Your Own Quality Metric 🎯\n",
    "\n",
    "Can you create a better quality metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_quality_metric(trajectory_data):\n",
    "    \"\"\"Design your own trajectory quality metric\"\"\"\n",
    "    \n",
    "    # TODO: Implement your metric\n",
    "    # Ideas to try:\n",
    "    # - Semantic coherence between steps\n",
    "    # - Increasing complexity across agents\n",
    "    # - Task completion indicators\n",
    "    # - Code quality signals (for coding tasks)\n",
    "    # - Proper handoffs between agents\n",
    "    \n",
    "    score = 50  # Your implementation here\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Test your metric\n",
    "print(\"Testing your metric:\")\n",
    "for t in trajectories[:3]:\n",
    "    your_score = your_quality_metric(t)\n",
    "    original_score = t['score']\n",
    "    print(f\"Task: {t['task']}, Quality: {t['quality']}\")\n",
    "    print(f\"  Original score: {original_score:.1f}\")\n",
    "    print(f\"  Your score: {your_score:.1f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Adaptive Filtering 🔄\n",
    "\n",
    "Make filtering adapt based on data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_filter(trajectories, target_count=10):\n",
    "    \"\"\"Adaptively filter to get exactly target_count best trajectories\"\"\"\n",
    "    \n",
    "    # TODO: Implement adaptive filtering\n",
    "    # Requirements:\n",
    "    # 1. Always return exactly target_count trajectories\n",
    "    # 2. Maximize average quality\n",
    "    # 3. Maintain diversity\n",
    "    # 4. Adjust thresholds automatically\n",
    "    \n",
    "    # Your implementation here\n",
    "    filtered = trajectories[:target_count]  # Placeholder\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Test adaptive filtering\n",
    "result = adaptive_filter(trajectories, target_count=5)\n",
    "print(f\"Adaptive filter returned {len(result)} trajectories\")\n",
    "print(f\"Target was 5: {'✅ PASS' if len(result) == 5 else '❌ FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Filter Efficiency Challenge ⚡\n",
    "\n",
    "Can you filter 10,000 trajectories in under 1 second?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def fast_filter(trajectories):\n",
    "    \"\"\"Ultra-fast filtering for large-scale datasets\"\"\"\n",
    "    \n",
    "    # TODO: Implement fast filtering\n",
    "    # Hints:\n",
    "    # - Pre-compute metrics\n",
    "    # - Use vectorized operations\n",
    "    # - Early stopping\n",
    "    # - Approximate algorithms\n",
    "    \n",
    "    # Your implementation here\n",
    "    filtered = trajectories  # Placeholder\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Generate large dataset\n",
    "large_dataset = trajectories * 100  # 3000 trajectories\n",
    "\n",
    "# Time your implementation\n",
    "start = time.time()\n",
    "result = fast_filter(large_dataset)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Filtered {len(large_dataset)} → {len(result)} trajectories\")\n",
    "print(f\"Time: {elapsed:.3f} seconds\")\n",
    "print(f\"Speed: {len(large_dataset)/elapsed:.0f} trajectories/second\")\n",
    "print(f\"Target: < 1 second for 10,000: {'✅ FAST' if elapsed < 1 else '❌ TOO SLOW'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways 🎓\n",
    "\n",
    "1. **Quality > Quantity**: 10 good trajectories beat 1000 bad ones\n",
    "2. **Progressive filtering**: Start loose, get stricter gradually\n",
    "3. **Diversity matters**: Similar trajectories don't add value\n",
    "4. **Metrics are key**: Good metrics = good filtering\n",
    "5. **Performance impact**: Filtering can improve GAIA scores by 10-15%!\n",
    "\n",
    "## The CoA Secret Sauce 🌟\n",
    "\n",
    "Progressive filtering is WHY Chain-of-Agents works:\n",
    "- **Traditional**: Train on everything → mediocre model\n",
    "- **CoA**: Train on filtered gold → state-of-the-art model\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "Part 3: **SFT - Distilling into AFM**. We'll take these filtered trajectories and train a single model that can simulate ALL agents!\n",
    "\n",
    "## Homework 📝\n",
    "\n",
    "1. Filter a dataset of 10,000+ trajectories\n",
    "2. Experiment with different quality metrics\n",
    "3. Plot quality distribution before/after filtering\n",
    "4. Implement semantic similarity (not just character-level)\n",
    "5. Read the CoA paper section on progressive filtering\n",
    "\n",
    "Remember: **Good data is the foundation of good AI!** 🏗️"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}