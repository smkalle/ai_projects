# %% [markdown]
# # Chain-of-Agents (CoA) Framework: A Comprehensive Hands-On Tutorial
# 
# ## Table of Contents
# 1. [Introduction & Theory](#introduction)
# 2. [Environment Setup & Prerequisites](#setup)
# 3. [Part 1: Understanding Multi-Agent Trajectories](#part1)
# 4. [Part 2: Supervised Fine-Tuning (SFT) Implementation](#part2)
# 5. [Part 3: Reinforcement Learning (RL) Optimization](#part3)
# 6. [Part 4: Production Deployment & Evaluation](#part4)
# 7. [Part 5: Real-World Applications](#part5)
# 8. [Exercises & Projects](#exercises)
# 
# **Author**: AI Engineering Team  
# **Last Updated**: January 2025  
# **Estimated Time**: 4-6 hours (complete tutorial)  
# **Prerequisites**: Python, PyTorch, Transformers basics

# %% [markdown]
# <a id='introduction'></a>
# ## 1. Introduction & Theory
# 
# ### What is Chain-of-Agents?
# 
# Chain-of-Agents (CoA) is a revolutionary framework from OPPO that **distills multi-agent collaborative behaviors into a single Large Language Model (LLM)**. Instead of orchestrating multiple specialized agents through complex prompting, CoA creates an "Agent Foundation Model" (AFM) that internally simulates multi-agent reasoning chains.
# 
# ### Why is this Important?
# 
# Traditional multi-agent systems suffer from:
# - **High latency**: Multiple LLM calls for agent coordination
# - **Complex orchestration**: Managing inter-agent communication
# - **Prompt engineering overhead**: Crafting prompts for each agent role
# - **Cost**: Multiple API calls multiply inference costs
# 
# CoA solves these by:
# - **Single inference pass**: One model simulates all agents
# - **Learned collaboration**: Agents' behaviors are learned, not prompted
# - **Superior performance**: 55.3% on GAIA (vs 53.2% WebSailor), 47.9% on LiveCodeBench (vs 42.4% Reveal-32B)
# - **18-20% RL gains**: Reinforcement learning provides significant improvements

# %% [markdown]
# ### Core Architecture
# 
# The CoA framework consists of three main components: 
# 
# ```
# [Multi-Agent System] ‚Üí [Trajectory Generation] ‚Üí [Progressive Filtering]
#          ‚Üì                                              ‚Üì
#    [OAgents, etc.]                              [Quality Control]
#          ‚Üì                                              ‚Üì
# [Collaborative Traces] ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [Clean Dataset]
#          ‚Üì
#    [SFT Training] ‚Üí [Base AFM Model]
#          ‚Üì
#     [RL Training] ‚Üí [Optimized AFM Model]
#          ‚Üì
#   [Production Agent]
# ```
# 
# ### Learning Objectives
# 
# By the end of this tutorial, you will:
# 1. ‚úÖ Understand trajectory-based agent training
# 2. ‚úÖ Implement SFT for agent foundation models
# 3. ‚úÖ Apply RL for 18-20% performance gains
# 4. ‚úÖ Deploy production-ready agent systems
# 5. ‚úÖ Evaluate on standard benchmarks
# 6. ‚úÖ Create custom domain-specific agents

# %% [markdown]
# <a id='setup'></a>
# ## 2. Environment Setup & Prerequisites
# 
# ### System Requirements Check
# Let's first verify your system is ready for this tutorial.

# %%
import sys
import platform
import subprocess
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

def check_system_requirements():
    """
    Comprehensive system requirements check for CoA framework.
    This ensures your environment is properly configured.
    """
    print("=" * 60)
    print("SYSTEM REQUIREMENTS CHECK")
    print("=" * 60)
    
    requirements_met = True
    
    # 1. Python Version Check
    python_version = sys.version_info
    print(f"\n1. Python Version: {python_version.major}.{python_version.minor}.{python_version.micro}")
    if python_version.major == 3 and python_version.minor >= 8:
        print("   ‚úÖ Python version is compatible (3.8+)")
    else:
        print("   ‚ùå Python 3.8+ required")
        requirements_met = False
    
    # 2. Operating System Check
    os_info = platform.system()
    print(f"\n2. Operating System: {os_info} {platform.release()}")
    if os_info in ['Linux', 'Darwin', 'Windows']:
        print(f"   ‚úÖ {os_info} is supported")
    else:
        print(f"   ‚ö†Ô∏è  {os_info} may have compatibility issues")
    
    # 3. Memory Check
    try:
        import psutil
        total_memory = psutil.virtual_memory().total / (1024**3)  # GB
        print(f"\n3. System Memory: {total_memory:.1f} GB")
        if total_memory >= 16:
            print("   ‚úÖ Sufficient memory for training")
        else:
            print("   ‚ö†Ô∏è  16GB+ recommended for training")
    except ImportError:
        print("\n3. Memory Check: psutil not installed")
        print("   Run: pip install psutil")
    
    # 4. GPU Check
    print("\n4. GPU Availability:")
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
                if gpu_memory >= 24:
                    print(f"   ‚úÖ GPU {i} suitable for 7B models")
                elif gpu_memory >= 80:
                    print(f"   ‚úÖ GPU {i} suitable for 32B models")
                else:
                    print(f"   ‚ö†Ô∏è  GPU {i} may require model quantization")
        else:
            print("   ‚ùå No CUDA GPU detected")
            print("   Note: CPU training is possible but very slow")
    except ImportError:
        print("   ‚ùå PyTorch not installed")
        requirements_met = False
    
    # 5. Disk Space Check
    try:
        disk_usage = psutil.disk_usage('/')
        free_space = disk_usage.free / (1024**3)  # GB
        print(f"\n5. Free Disk Space: {free_space:.1f} GB")
        if free_space >= 100:
            print("   ‚úÖ Sufficient disk space")
        else:
            print("   ‚ö†Ô∏è  100GB+ recommended for models and datasets")
    except:
        print("\n5. Disk Space: Unable to check")
    
    print("\n" + "=" * 60)
    if requirements_met:
        print("‚úÖ SYSTEM READY FOR COA FRAMEWORK")
    else:
        print("‚ö†Ô∏è  SOME REQUIREMENTS NOT MET - See above for details")
    print("=" * 60)
    
    return requirements_met

# Run system check
system_ready = check_system_requirements()

# %% [markdown]
# ### Installing Dependencies
# 
# Now let's install all required packages. This cell installs everything needed for the complete tutorial.

# %%
def install_coa_dependencies():
    """
    Install all dependencies for Chain-of-Agents framework.
    This includes core ML libraries, agent tools, and evaluation frameworks.
    """
    
    print("Installing Chain-of-Agents dependencies...")
    print("This may take 5-10 minutes depending on your internet connection.\n")
    
    # Core dependencies grouped by functionality
    dependencies = {
        "Core ML Framework": [
            "torch==2.0.1",
            "transformers==4.45.1",
            "datasets==3.0.1",
            "accelerate==0.25.0",
            "sentencepiece",  # For tokenization
            "protobuf==3.20.3"
        ],
        "Training Optimization": [
            "deepspeed==0.15.0",
            "bitsandbytes==0.41.0",  # For 8-bit quantization
            "peft==0.7.0",  # Parameter-efficient fine-tuning
            "trl==0.7.0"  # Transformer Reinforcement Learning
        ],
        "Agent Tools": [
            "selenium==4.15.0",  # Web automation
            "beautifulsoup4==4.12.0",  # HTML parsing
            "requests==2.31.0",  # HTTP requests
            "aiohttp==3.9.0"  # Async HTTP
        ],
        "Code Execution": [
            "jupyter-client==8.6.0",  # Code execution
            "ipykernel==6.27.0",
            "ast-grep-py==0.12.0",  # AST manipulation
            "black==23.12.0"  # Code formatting
        ],
        "Evaluation & Monitoring": [
            "wandb==0.16.0",  # Experiment tracking
            "tensorboard==2.15.0",
            "scikit-learn==1.3.0",
            "pandas==2.1.0",
            "matplotlib==3.8.0",
            "seaborn==0.13.0",
            "tqdm==4.66.0"
        ],
        "Utilities": [
            "python-dotenv==1.0.0",  # Environment management
            "pyyaml==6.0.1",  # Config files
            "jsonlines==4.0.0",  # JSONL processing
            "psutil==5.9.0",  # System monitoring
            "rich==13.7.0"  # Beautiful terminal output
        ]
    }
    
    # Install each group
    for category, packages in dependencies.items():
        print(f"\nüì¶ Installing {category}...")
        for package in packages:
            try:
                # Use subprocess for better error handling
                result = subprocess.run(
                    [sys.executable, "-m", "pip", "install", "-q", package],
                    capture_output=True,
                    text=True
                )
                if result.returncode == 0:
                    print(f"   ‚úÖ {package}")
                else:
                    print(f"   ‚ùå {package} - Error: {result.stderr[:100]}")
            except Exception as e:
                print(f"   ‚ùå {package} - Exception: {str(e)[:100]}")
    
    print("\n" + "=" * 60)
    print("‚úÖ Dependency installation complete!")
    print("=" * 60)

# Uncomment to install all dependencies
# install_coa_dependencies()

# For this notebook, let's import the essential libraries
import os
import json
import time
import random
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from collections import defaultdict
from datetime import datetime
import re
import logging

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('CoA-Tutorial')

print("‚úÖ Essential imports successful")

# %% [markdown]
# ### Setting Up Workspace Structure
# 
# Let's create a well-organized workspace for our CoA implementation.

# %%
@dataclass
class WorkspaceConfig:
    """
    Comprehensive workspace configuration for CoA framework.
    This organizes all paths, models, and settings in one place.
    """
    
    # Base paths
    root_dir: Path = Path("./coa_tutorial_workspace")
    
    # Model paths
    models_dir: Path = None
    base_models_dir: Path = None
    sft_models_dir: Path = None
    rl_models_dir: Path = None
    
    # Data paths
    data_dir: Path = None
    raw_data_dir: Path = None
    processed_data_dir: Path = None
    trajectories_dir: Path = None
    
    # Output paths
    outputs_dir: Path = None
    logs_dir: Path = None
    checkpoints_dir: Path = None
    evaluations_dir: Path = None
    
    # Cache paths
    cache_dir: Path = None
    
    def __post_init__(self):
        """Initialize all subdirectories based on root"""
        self.models_dir = self.root_dir / "models"
        self.base_models_dir = self.models_dir / "base"
        self.sft_models_dir = self.models_dir / "sft"
        self.rl_models_dir = self.models_dir / "rl"
        
        self.data_dir = self.root_dir / "data"
        self.raw_data_dir = self.data_dir / "raw"
        self.processed_data_dir = self.data_dir / "processed"
        self.trajectories_dir = self.data_dir / "trajectories"
        
        self.outputs_dir = self.root_dir / "outputs"
        self.logs_dir = self.outputs_dir / "logs"
        self.checkpoints_dir = self.outputs_dir / "checkpoints"
        self.evaluations_dir = self.outputs_dir / "evaluations"
        
        self.cache_dir = self.root_dir / ".cache"
    
    def create_directories(self):
        """Create all workspace directories"""
        all_dirs = [
            self.root_dir,
            self.models_dir, self.base_models_dir, self.sft_models_dir, self.rl_models_dir,
            self.data_dir, self.raw_data_dir, self.processed_data_dir, self.trajectories_dir,
            self.outputs_dir, self.logs_dir, self.checkpoints_dir, self.evaluations_dir,
            self.cache_dir
        ]
        
        for dir_path in all_dirs:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        print("üìÅ Workspace Structure Created:")
        self.print_tree()
    
    def print_tree(self, directory=None, prefix="", max_depth=3, current_depth=0):
        """Print directory tree structure"""
        if current_depth >= max_depth:
            return
            
        if directory is None:
            directory = self.root_dir
            print(f"\n{directory.name}/")
            
        items = sorted(directory.iterdir()) if directory.exists() else []
        
        for i, item in enumerate(items):
            is_last = i == len(items) - 1
            current_prefix = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
            print(f"{prefix}{current_prefix}{item.name}/")
            
            if item.is_dir():
                extension = "    " if is_last else "‚îÇ   "
                self.print_tree(item, prefix + extension, max_depth, current_depth + 1)

# Initialize workspace
workspace = WorkspaceConfig()
workspace.create_directories()

# Save configuration
config_file = workspace.root_dir / "config.json"
with open(config_file, 'w') as f:
    json.dump({
        'created_at': datetime.now().isoformat(),
        'root_dir': str(workspace.root_dir),
        'structure': 'CoA Tutorial Workspace v1.0'
    }, f, indent=2)

print(f"\n‚úÖ Workspace initialized at: {workspace.root_dir}")

# %% [markdown]
# <a id='part1'></a>
# ## Part 1: Understanding Multi-Agent Trajectories
# 
# ### What are Trajectories?
# 
# Trajectories are **structured sequences of agent interactions** that capture the reasoning process of multiple agents collaborating to solve a task. Each trajectory contains: 
# 
# - **Planning**: High-level strategy formulation
# - **Thinking**: Step-by-step reasoning
# - **Tool Usage**: External tool invocations
# - **Observations**: Results from tool usage
# - **Reflection**: Self-assessment and error correction
# - **Answer**: Final solution
# 
# Let's implement a comprehensive trajectory system:

# %%
class TrajectoryComponent:
    """
    Base class for trajectory components.
    Each component represents a distinct phase in agent reasoning.
    """
    
    def __init__(self, tag_name: str, content: str, metadata: Dict[str, Any] = None):
        self.tag_name = tag_name
        self.content = content
        self.metadata = metadata or {}
        self.timestamp = time.time()
    
    def to_xml(self) -> str:
        """Convert component to XML format for training"""
        return f"<{self.tag_name}>\n{self.content}\n</{self.tag_name}>"
    
    def __repr__(self):
        return f"{self.__class__.__name__}(tag='{self.tag_name}', length={len(self.content)})"

class Plan(TrajectoryComponent):
    """Planning phase - strategic approach to the problem"""
    def __init__(self, content: str, steps: List[str] = None):
        super().__init__("plan", content)
        self.steps = steps or []

class Think(TrajectoryComponent):
    """Thinking phase - detailed reasoning process"""
    def __init__(self, content: str, reasoning_type: str = "analytical"):
        super().__init__("think", content)
        self.reasoning_type = reasoning_type

class ToolUse(TrajectoryComponent):
    """Tool usage phase - external tool invocation"""
    def __init__(self, tool_name: str, tool_input: str, tool_output: str = None):
        content = f"{tool_name}({tool_input})"
        super().__init__("tools", content)
        self.tool_name = tool_name
        self.tool_input = tool_input
        self.tool_output = tool_output

class Observation(TrajectoryComponent):
    """Observation phase - results from tool usage"""
    def __init__(self, content: str, success: bool = True):
        super().__init__("observation", content)
        self.success = success

class Reflection(TrajectoryComponent):
    """Reflection phase - self-assessment and learning"""
    def __init__(self, content: str, improvements: List[str] = None):
        super().__init__("reflection", content)
        self.improvements = improvements or []

class Answer(TrajectoryComponent):
    """Answer phase - final solution"""
    def __init__(self, content: str, confidence: float = 0.8):
        super().__init__("answer", content)
        self.confidence = confidence

# %% 
class Trajectory:
    """
    Complete trajectory representing a full agent reasoning chain.
    This is the core data structure for training AFMs.
    """
    
    def __init__(self, task_id: str, query: str):
        self.task_id = task_id
        self.query = query
        self.components: List[TrajectoryComponent] = []
        self.metadata = {
            'created_at': datetime.now().isoformat(),
            'version': '1.0',
            'source': 'tutorial'
        }
    
    def add_component(self, component: TrajectoryComponent):
        """Add a component to the trajectory"""
        self.components.append(component)
        logger.debug(f"Added {component.__class__.__name__} to trajectory {self.task_id}")
    
    def to_training_format(self) -> Dict[str, str]:
        """Convert trajectory to training format"""
        # Combine all components into XML structure
        output_parts = []
        for component in self.components:
            output_parts.append(component.to_xml())
        
        return {
            'input': self.query,
            'output': '\n'.join(output_parts),
            'task_id': self.task_id,
            'metadata': self.metadata
        }
    
    def get_interaction_count(self) -> int:
        """Count the number of tool interactions"""
        return sum(1 for c in self.components if isinstance(c, (ToolUse, Observation)))
    
    def has_reflection(self) -> bool:
        """Check if trajectory includes reflection"""
        return any(isinstance(c, Reflection) for c in self.components)
    
    def get_complexity_score(self) -> float:
        """
        Calculate trajectory complexity score.
        Higher scores indicate more complex reasoning chains.
        """
        score = 0.0
        
        # Component diversity
        component_types = set(type(c) for c in self.components)
        score += len(component_types) * 0.1
        
        # Interaction depth
        score += self.get_interaction_count() * 0.15
        
        # Reflection bonus
        if self.has_reflection():
            score += 0.2
        
        # Content length factor
        total_length = sum(len(c.content) for c in self.components)
        score += min(total_length / 1000, 1.0) * 0.3
        
        # Multiple tool uses
        tool_uses = [c for c in self.components if isinstance(c, ToolUse)]
        unique_tools = set(t.tool_name for t in tool_uses)
        score += len(unique_tools) * 0.1
        
        return min(score, 1.0)  # Normalize to [0, 1]
    
    def visualize(self):
        """Pretty print the trajectory for inspection"""
        print(f"\n{'='*60}")
        print(f"TRAJECTORY: {self.task_id}")
        print(f"Query: {self.query[:100]}...")
        print(f"Complexity Score: {self.get_complexity_score():.2f}")
        print(f"Components: {len(self.components)}")
        print(f"{'='*60}")
        
        for i, component in enumerate(self.components, 1):
            print(f"\n[{i}] {component.__class__.__name__}")
            print(f"    {component.content[:200]}...")
            if hasattr(component, 'metadata') and component.metadata:
                print(f"    Metadata: {component.metadata}")

# %% [markdown]
# ### Generating High-Quality Training Trajectories
# 
# Now let's create a sophisticated trajectory generator that simulates different agent behaviors:

# %%
class TrajectoryGenerator:
    """
    Advanced trajectory generator for creating diverse training data.
    Simulates different agent personalities and problem-solving approaches.
    """
    
    def __init__(self, complexity_level: str = "medium"):
        """
        Initialize generator with complexity level.
        Levels: simple, medium, complex, expert
        """
        self.complexity_level = complexity_level
        self.tool_registry = self._initialize_tools()
        self.agent_styles = self._initialize_agent_styles()
    
    def _initialize_tools(self) -> Dict[str, Dict]:
        """Register available tools for simulation"""
        return {
            'python_executor': {
                'description': 'Execute Python code',
                'input_format': 'code string',
                'output_format': 'execution result'
            },
            'web_search': {
                'description': 'Search the web for information',
                'input_format': 'search query',
                'output_format': 'search results'
            },
            'calculator': {
                'description': 'Perform mathematical calculations',
                'input_format': 'mathematical expression',
                'output_format': 'numerical result'
            },
            'file_reader': {
                'description': 'Read and analyze files',
                'input_format': 'file path',
                'output_format': 'file contents'
            },
            'api_caller': {
                'description': 'Make API requests',
                'input_format': 'endpoint and parameters',
                'output_format': 'API response'
            }
        }
    
    def _initialize_agent_styles(self) -> Dict[str, Dict]:
        """Define different agent reasoning styles"""
        return {
            'analytical': {
                'planning': 'Break down into logical steps',
                'thinking': 'Systematic analysis',
                'reflection': 'Logical validation'
            },
            'creative': {
                'planning': 'Explore multiple approaches',
                'thinking': 'Lateral thinking',
                'reflection': 'Innovation assessment'
            },
            'methodical': {
                'planning': 'Step-by-step methodology',
                'thinking': 'Detailed examination',
                'reflection': 'Thorough verification'
            },
            'pragmatic': {
                'planning': 'Focus on practical solutions',
                'thinking': 'Efficiency-oriented',
                'reflection': 'Results validation'
            }
        }
    
    def generate_trajectory(
        self,
        query: str,
        task_type: str = "code",
        agent_style: str = "analytical"
    ) -> Trajectory:
        """
        Generate a complete trajectory for a given query.
        
        Args:
            query: The task/question to solve
            task_type: Type of task (code, web, math, general)
            agent_style: Agent reasoning style
        
        Returns:
            Complete trajectory with all components
        """
        task_id = f"{task_type}_{int(time.time()*1000)}"
        trajectory = Trajectory(task_id, query)
        
        # 1. Planning Phase
        plan = self._generate_plan(query, task_type, agent_style)
        trajectory.add_component(plan)
        
        # 2. Thinking Phase
        think = self._generate_thinking(query, plan, agent_style)
        trajectory.add_component(think)
        
        # 3. Tool Usage Loops (complexity determines iterations)
        num_interactions = self._get_num_interactions()
        for i in range(num_interactions):
            tool_use, observation = self._generate_tool_interaction(
                query, task_type, i, num_interactions
            )
            trajectory.add_component(tool_use)
            trajectory.add_component(observation)
            
            # Intermediate thinking (for complex trajectories)
            if self.complexity_level in ['complex', 'expert'] and i < num_interactions - 1:
                intermediate_think = self._generate_intermediate_thinking(
                    observation, i, num_interactions
                )
                trajectory.add_component(intermediate_think)
        
        # 4. Reflection Phase (for medium+ complexity)
        if self.complexity_level != 'simple':
            reflection = self._generate_reflection(trajectory, agent_style)
            trajectory.add_component(reflection)
        
        # 5. Final Answer
        answer = self._generate_answer(query, trajectory)
        trajectory.add_component(answer)
        
        return trajectory
    
    def _get_num_interactions(self) -> int:
        """Determine number of tool interactions based on complexity"""
        complexity_map = {
            'simple': random.randint(1, 2),
            'medium': random.randint(2, 4),
            'complex': random.randint(4, 7),
            'expert': random.randint(6, 10)
        }
        return complexity_map.get(self.complexity_level, 3)
    
    def _generate_plan(self, query: str, task_type: str, agent_style: str) -> Plan:
        """Generate planning component"""
        style_approach = self.agent_styles[agent_style]['planning']
        
        steps = []
        if task_type == "code":
            steps = [
                "1. Parse requirements and constraints",
                "2. Design algorithm approach",
                "3. Implement core functionality",
                "4. Add error handling",
                "5. Test and optimize"
            ]
        elif task_type == "web":
            steps = [
                "1. Identify information needs",
                "2. Search relevant sources",
                "3. Extract key information",
                "4. Verify accuracy",
                "5. Synthesize findings"
            ]
        else:
            steps = [
                "1. Understand the problem",
                "2. Gather necessary information",
                "3. Apply appropriate methods",
                "4. Validate results"
            ]
        
        content = f"{style_approach} for solving: {query}\n" + "\n".join(steps)
        return Plan(content, steps)
    
    def _generate_thinking(self, query: str, plan: Plan, agent_style: str) -> Think:
        """Generate thinking component"""
        style_thinking = self.agent_styles[agent_style]['thinking']
        
        thoughts = [
            f"Applying {style_thinking} to the problem.",
            f"The query '{query[:50]}...' requires careful consideration.",
            "Key challenges identified:",
            "- Complexity of requirements",
            "- Need for robust solution",
            "- Performance considerations"
        ]
        
        if plan.steps:
            thoughts.append(f"\nFollowing the {len(plan.steps)}-step plan outlined.")
        
        return Think("\n".join(thoughts), agent_style)
    
    def _generate_tool_interaction(
        self,
        query: str,
        task_type: str,
        iteration: int,
        total_iterations: int
    ) -> Tuple[ToolUse, Observation]:
        """Generate tool usage and observation pair"""
        
        # Select appropriate tool
        if task_type == "code":
            tool_name = "python_executor"
            tool_input = f"def solution_{iteration}():\n    # Implementation step {iteration+1}/{total_iterations}\n    return 'partial_result_{iteration}'"
        elif task_type == "web":
            tool_name = "web_search"
            tool_input = f"search query related to: {query[:30]}... (iteration {iteration+1})"
        else:
            tool_name = random.choice(list(self.tool_registry.keys()))
            tool_input = f"process step {iteration+1}"
        
        tool_use = ToolUse(tool_name, tool_input)
        
        # Generate observation
        success = random.random() > 0.1  # 90% success rate
        if success:
            obs_content = f"Tool execution successful. Result: processed_{iteration}"
        else:
            obs_content = f"Tool execution encountered issue. Retrying with adjusted parameters."
        
        observation = Observation(obs_content, success)
        
        return tool_use, observation
    
    def _generate_intermediate_thinking(
        self,
        observation: Observation,
        iteration: int,
        total_iterations: int
    ) -> Think:
        """Generate intermediate thinking between tool uses"""
        progress = (iteration + 1) / total_iterations
        
        thoughts = [
            f"Progress: {progress:.0%} complete",
            f"Previous step {'succeeded' if observation.success else 'needs adjustment'}",
            f"Remaining steps: {total_iterations - iteration - 1}"
        ]
        
        return Think("\n".join(thoughts), "intermediate")
    
    def _generate_reflection(self, trajectory: Trajectory, agent_style: str) -> Reflection:
        """Generate reflection component"""
        style_reflection = self.agent_styles[agent_style]['reflection']
        
        tool_uses = sum(1 for c in trajectory.components if isinstance(c, ToolUse))
        observations = [c for c in trajectory.components if isinstance(c, Observation)]
        success_rate = sum(1 for o in observations if o.success) / len(observations) if observations else 0
        
        content = f"""
{style_reflection} of the solution process:
- Total tool invocations: {tool_uses}
- Success rate: {success_rate:.0%}
- Approach effectiveness: {"High" if success_rate > 0.8 else "Moderate"}
- Key learnings identified for future improvements
"""
        
        improvements = [
            "Optimize tool selection",
            "Reduce redundant operations",
            "Enhance error handling"
        ]
        
        return Reflection(content.strip(), improvements)
    
    def _generate_answer(self, query: str, trajectory: Trajectory) -> Answer:
        """Generate final answer"""
        complexity_score = trajectory.get_complexity_score()
        confidence = 0.6 + (complexity_score * 0.4)  # 60-100% confidence
        
        content = f"""
Based on the comprehensive analysis and execution:

Query: {query[:100]}...

Solution:
The task has been completed successfully through {len(trajectory.components)} reasoning steps.
Key achievements:
- Problem fully analyzed and understood
- Appropriate tools utilized effectively
- Solution validated through reflection

Final output: [Solution specific to the query would be provided here]

Confidence Level: {confidence:.0%}
"""
        
        return Answer(content.strip(), confidence)

# %% [markdown]
# ### Practical Exercise 1: Generate and Analyze Trajectories
# 
# Let's generate various trajectories and analyze their quality:

# %% 
def trajectory_generation_exercise():
    """
    Interactive exercise for generating and analyzing trajectories.
    This demonstrates the diversity of trajectories needed for training.
    """
    
    print("=" * 70)
    print("TRAJECTORY GENERATION EXERCISE")
    print("=" * 70)
    
    # Sample queries for different task types
    sample_queries = {
        'code': [
            "Implement a binary search tree with insertion and deletion",
            "Create a web scraper for extracting product prices",
            "Write a function to find the longest palindromic substring"
        ],
        'web': [
            "Find the latest research on quantum computing applications",
            "Compare prices for flights from NYC to London next month",
            "Research best practices for sustainable agriculture"
        ],
        'math': [
            "Solve the differential equation: dy/dx = 2x + 3y",
            "Calculate the probability of getting exactly 3 heads in 10 coin flips",
            "Find the area under the curve y = x^2 from 0 to 5"
        ]
    }
    
    # Generate trajectories with different configurations
    all_trajectories = []
    
    for complexity in ['simple', 'medium', 'complex']:
        print(f"\nüìä Generating {complexity.upper()} trajectories...")
        generator = TrajectoryGenerator(complexity_level=complexity)
        
        for task_type, queries in sample_queries.items():
            for query in queries[:1]:  # Use first query of each type
                for style in ['analytical', 'creative']:
                    trajectory = generator.generate_trajectory(
                        query=query,
                        task_type=task_type,
                        agent_style=style
                    )
                    all_trajectories.append(trajectory)
                    
                    print(f"  ‚úì Generated: {task_type}/{style} - Score: {trajectory.get_complexity_score():.2f}")
    
    # Analyze trajectory distribution
    print("\n" + "=" * 70)
    print("TRAJECTORY ANALYSIS")
    print("=" * 70)
    
    # Complexity distribution
    complexity_scores = [t.get_complexity_score() for t in all_trajectories]
    print(f"\nüìà Complexity Statistics:")
    print(f"  - Mean: {np.mean(complexity_scores):.3f}")
    print(f"  - Std:  {np.std(complexity_scores):.3f}")
    print(f"  - Min:  {np.min(complexity_scores):.3f}")
    print(f"  - Max:  {np.max(complexity_scores):.3f}")
    
    # Component distribution
    component_counts = defaultdict(int)
    for traj in all_trajectories:
        for comp in traj.components:
            component_counts[comp.__class__.__name__] += 1
    
    print(f"\nüìä Component Distribution:")
    for comp_name, count in sorted(component_counts.items()):
        avg_per_traj = count / len(all_trajectories)
        print(f"  - {comp_name:15s}: {count:3d} total ({avg_per_traj:.1f} per trajectory)")
    
    # Quality metrics
    with_reflection = sum(1 for t in all_trajectories if t.has_reflection())
    avg_interactions = np.mean([t.get_interaction_count() for t in all_trajectories])
    
    print(f"\n‚úÖ Quality Metrics:")
    print(f"  - Trajectories with reflection: {with_reflection}/{len(all_trajectories)} ({with_reflection/len(all_trajectories)*100:.0f}%)")
    print(f"  - Average interactions per trajectory: {avg_interactions:.1f}")
    
    # Show example trajectory
    print("\n" + "=" * 70)
    print("EXAMPLE TRAJECTORY (Highest Complexity)")
    print("=" * 70)
    
    best_trajectory = max(all_trajectories, key=lambda t: t.get_complexity_score())
    best_trajectory.visualize()
    
    return all_trajectories

# Run the exercise
generated_trajectories = trajectory_generation_exercise()

# %% [markdown]
# ### Progressive Filtering for Quality Control
# 
# Not all trajectories are suitable for training. Let's implement sophisticated filtering:

# %% 
class TrajectoryFilter:
    """
    Advanced trajectory filtering system for quality control.
    Implements progressive filtering as described in the CoA paper.
    """
    
    def __init__(self):
        self.filters = self._initialize_filters()
        self.statistics = defaultdict(int)
    
    def _initialize_filters(self) -> Dict[str, callable]:
        """Initialize all filter functions"""
        return {
            'min_interactions': self._filter_min_interactions,
            'has_reflection': self._filter_has_reflection,
            'complexity_threshold': self._filter_complexity,
            'content_quality': self._filter_content_quality,
            'structural_integrity': self._filter_structural_integrity,
            'diversity_check': self._filter_diversity
        }
    
    def apply_progressive_filtering(
        self,
        trajectories: List[Trajectory],
        config: Dict[str, Any] = None
    ) -> Tuple[List[Trajectory], Dict[str, Any]]:
        """
        Apply progressive filtering to trajectory list.
        
        Args:
            trajectories: List of trajectories to filter
            config: Filtering configuration
        
        Returns:
            Filtered trajectories and filtering statistics
        """
        if config is None:
            config = {
                'min_interactions': 3,
                'require_reflection': True,
                'min_complexity': 0.3,
                'min_content_length': 500,
                'check_structure': True,
                'ensure_diversity': True
            }
        
        print(f"\n{'='*60}")
        print(f"PROGRESSIVE FILTERING PIPELINE")
        print(f"Initial trajectories: {len(trajectories)}")
        print(f"{'='*60}")
        
        remaining = trajectories.copy()
        filter_stats = {}
        
        # Apply each filter progressively
        for filter_name, filter_func in self.filters.items():
            if filter_name == 'min_interactions' and config.get('min_interactions'):
                filtered = filter_func(remaining, config['min_interactions'])
                removed = len(remaining) - len(filtered)
                filter_stats[filter_name] = removed
                print(f"‚úì {filter_name:20s}: Removed {removed:3d} ({removed/len(remaining)*100:.1f}%)")
                remaining = filtered
            
            elif filter_name == 'has_reflection' and config.get('require_reflection'):
                filtered = filter_func(remaining)
                removed = len(remaining) - len(filtered)
                filter_stats[filter_name] = removed
                print(f"‚úì {filter_name:20s}: Removed {removed:3d} ({removed/max(len(remaining),1)*100:.1f}%)")
                remaining = filtered
            
            elif filter_name == 'complexity_threshold' and config.get('min_complexity'):
                filtered = filter_func(remaining, config['min_complexity'])
                removed = len(remaining) - len(filtered)
                filter_stats[filter_name] = removed
                print(f"‚úì {filter_name:20s}: Removed {removed:3d} ({removed/max(len(remaining),1)*100:.1f}%)")
                remaining = filtered
            
            elif filter_name == 'content_quality' and config.get('min_content_length'):
                filtered = filter_func(remaining, config['min_content_length'])
                removed = len(remaining) - len(filtered)
                filter_stats[filter_name] = removed
                print(f"‚úì {filter_name:20s}: Removed {removed:3d} ({removed/max(len(remaining),1)*100:.1f}%)")
                remaining = filtered
        
        # Final statistics
        print(f"\n{'='*60}")
        print(f"FILTERING COMPLETE")
        print(f"Final trajectories: {len(remaining)} ({len(remaining)/len(trajectories)*100:.1f}% retained)")
        print(f"{'='*60}")
        
        # Quality analysis of filtered set
        if remaining:
            quality_stats = self._analyze_quality(remaining)
            filter_stats['quality_metrics'] = quality_stats
            
            print(f"\nüìä Quality Metrics of Filtered Set:")
            for metric, value in quality_stats.items():
                print(f"  - {metric}: {value}")
        
        return remaining, filter_stats
    
    def _filter_min_interactions(self, trajectories: List[Trajectory], min_count: int) -> List[Trajectory]:
        """Filter by minimum interaction count"""
        return [t for t in trajectories if t.get_interaction_count() >= min_count]
    
    def _filter_has_reflection(self, trajectories: List[Trajectory]) -> List[Trajectory]:
        """Filter trajectories without reflection"""
        return [t for t in trajectories if t.has_reflection()]
    
    def _filter_complexity(self, trajectories: List[Trajectory], min_score: float) -> List[Trajectory]:
        """Filter by complexity score"""
        return [t for t in trajectories if t.get_complexity_score() >= min_score]
    
    def _filter_content_quality(self, trajectories: List[Trajectory], min_length: int) -> List[Trajectory]:
        """Filter by content quality and length"""
        filtered = []
        for t in trajectories:
            total_length = sum(len(c.content) for c in t.components)
            if total_length >= min_length:
                # Additional quality checks
                has_plan = any(isinstance(c, Plan) for c in t.components)
                has_answer = any(isinstance(c, Answer) for c in t.components)
                if has_plan and has_answer:
                    filtered.append(t)
        return filtered
    
    def _filter_structural_integrity(self, trajectories: List[Trajectory]) -> List[Trajectory]:
        """Check structural integrity of trajectories"""
        filtered = []
        for t in trajectories:
            # Check proper ordering: Plan -> Think -> Tools/Obs -> Answer
            component_types = [type(c).__name__ for c in t.components]
            
            if component_types[0] == 'Plan' and component_types[-1] == 'Answer':
                filtered.append(t)
        
        return filtered
    
    def _filter_diversity(self, trajectories: List[Trajectory]) -> List[Trajectory]:
        """Ensure diversity in trajectory types"""
        # For simplicity, we'll keep all in this tutorial
        # In practice, you'd implement deduplication and diversity checks
        return trajectories
    
    def _analyze_quality(self, trajectories: List[Trajectory]) -> Dict[str, Any]:
        """Analyze quality metrics of trajectory set"""
        if not trajectories:
            return {}
        
        complexity_scores = [t.get_complexity_score() for t in trajectories]
        interaction_counts = [t.get_interaction_count() for t in trajectories]
        
        return {
            'avg_complexity': f"{np.mean(complexity_scores):.3f}",
            'avg_interactions': f"{np.mean(interaction_counts):.1f}",
            'with_reflection_pct': f"{sum(t.has_reflection() for t in trajectories)/len(trajectories)*100:.0f}%",
            'unique_tasks': len(set(t.task_id.split('_')[0] for t in trajectories))
        }

# Apply filtering to generated trajectories
filter_system = TrajectoryFilter()
filtered_trajectories, filter_stats = filter_system.apply_progressive_filtering(
    generated_trajectories,
    config={
        'min_interactions': 2,
        'require_reflection': False,  # Less strict for tutorial
        'min_complexity': 0.2,
        'min_content_length': 300
    }
)

# Save filtered trajectories
trajectories_file = workspace.trajectories_dir / "filtered_trajectories.jsonl"
with open(trajectories_file, 'w') as f:
    for traj in filtered_trajectories:
        f.write(json.dumps(traj.to_training_format()) + '\n')

print(f"\n‚úÖ Saved {len(filtered_trajectories)} trajectories to {trajectories_file}")

# %% [markdown]
# <a id='part2'></a>
# ## Part 2: Supervised Fine-Tuning (SFT) Implementation
# 
# Now we'll implement the SFT training pipeline using the filtered trajectories.

# %% 
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)

class AFMDataset(Dataset):
    """
    Dataset class for Agent Foundation Model training.
    Handles trajectory loading, tokenization, and batching.
    """
    
    def __init__(
        self,
        trajectories: List[Trajectory],
        tokenizer,
        max_length: int = 4096,
        include_metadata: bool = False
    ):
        self.trajectories = trajectories
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.include_metadata = include_metadata
        
        # Pre-process trajectories
        self.processed_data = self._preprocess_trajectories()
    
    def _preprocess_trajectories(self) -> List[Dict]:
        """Pre-process trajectories into training format"""
        processed = []
        
        for traj in self.trajectories:
            training_format = traj.to_training_format()
            
            # Format as instruction-following
            text = self._format_instruction(
                training_format['input'],
                training_format['output']
            )
            
            processed.append({
                'text': text,
                'task_id': training_format['task_id'],
                'metadata': training_format.get('metadata', {})
            })
        
        logger.info(f"Preprocessed {len(processed)} trajectories")
        return processed
    
    def _format_instruction(self, instruction: str, response: str) -> str:
        """Format as instruction-following template"""
        template = """### Human: {instruction}

### Assistant: {response}"""
        
        return template.format(
            instruction=instruction,
            response=response
        )
    
    def __len__(self):
        return len(self.processed_data)
    
    def __getitem__(self, idx):
        item = self.processed_data[idx]
        
        # Tokenize
        encoding = self.tokenizer(
            item['text'],
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        # Prepare labels (same as input_ids for causal LM)
        labels = encoding['input_ids'].clone()
        
        # Mask padding tokens in labels
        labels[labels == self.tokenizer.pad_token_id] = -100
        
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': labels.squeeze()
        }

# %% [markdown]
# ### SFT Training Configuration

# %% 
@dataclass
class SFTConfig:
    """
    Comprehensive configuration for SFT training.
    Based on best practices from the CoA paper.
    """
    
    # Model configuration
    model_name: str = "gpt2"  # Use GPT-2 for demo (replace with Qwen/Llama for production)
    model_revision: str = "main"
    use_fast_tokenizer: bool = True
    
    # Training hyperparameters
    learning_rate: float = 5e-6
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 1
    per_device_eval_batch_size: int = 1
    gradient_accumulation_steps: int = 16
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01
    
    # Optimization
    optim: str = "adamw_torch"
    adam_beta1: float = 0.9
    adam_beta2: float = 0.999
    adam_epsilon: float = 1e-8
    max_grad_norm: float = 1.0
    
    # Learning rate schedule
    lr_scheduler_type: str = "cosine"
    
    # Training strategy
    fp16: bool = torch.cuda.is_available()  # Use FP16 if GPU available
    gradient_checkpointing: bool = True
    
    # Logging and saving
    logging_steps: int = 10
    save_steps: int = 100
    eval_steps: int = 100
    save_total_limit: int = 3
    load_best_model_at_end: bool = True
    
    # Sequence length
    max_seq_length: int = 2048  # Reduced for demo
    
    # Hardware
    dataloader_num_workers: int = 4
    
    def to_training_arguments(self, output_dir: str) -> TrainingArguments:
        """Convert to HuggingFace TrainingArguments"""
        return TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=self.num_train_epochs,
            per_device_train_batch_size=self.per_device_train_batch_size,
            per_device_eval_batch_size=self.per_device_eval_batch_size,
            gradient_accumulation_steps=self.gradient_accumulation_steps,
            learning_rate=self.learning_rate,
            warmup_ratio=self.warmup_ratio,
            weight_decay=self.weight_decay,
            optim=self.optim,
            adam_beta1=self.adam_beta1,
            adam_beta2=self.adam_beta2,
            adam_epsilon=self.adam_epsilon,
            max_grad_norm=self.max_grad_norm,
            lr_scheduler_type=self.lr_scheduler_type,
            fp16=self.fp16,
            gradient_checkpointing=self.gradient_checkpointing,
            logging_steps=self.logging_steps,
            save_steps=self.save_steps,
            evaluation_strategy="steps",
            eval_steps=self.eval_steps,
            save_total_limit=self.save_total_limit,
            load_best_model_at_end=self.load_best_model_at_end,
            dataloader_num_workers=self.dataloader_num_workers,
            report_to="none",  # Disable wandb for tutorial
            push_to_hub=False
        )

# %% [markdown]
# ### Implementing the SFT Trainer

# %% 
class AFMSFTTrainer:
    """
    Complete SFT training pipeline for Agent Foundation Models.
    Implements training, evaluation, and model management.
    """
    
    def __init__(
        self,
        config: SFTConfig,
        workspace: WorkspaceConfig
    ):
        self.config = config
        self.workspace = workspace
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Initialize model and tokenizer
        self.model = None
        self.tokenizer = None
        
        # Training components
        self.trainer = None
        self.train_dataset = None
        self.eval_dataset = None
    
    def setup_model_and_tokenizer(self):
        """Initialize model and tokenizer"""
        print(f"\n{'='*60}")
        print(f"INITIALIZING MODEL: {self.config.model_name}")
        print(f"{'='*60}")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            use_fast=self.config.use_fast_tokenizer
        )
        
        # Add padding token if needed
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load model
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            torch_dtype=torch.float16 if self.config.fp16 else torch.float32
        )
        
        # Move to device
        self.model.to(self.device)
        
        # Enable gradient checkpointing
        if self.config.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
        
        print(f"‚úÖ Model loaded: {self.model.config.model_type}")
        print(f"‚úÖ Parameters: {sum(p.numel() for p in self.model.parameters())/1e6:.1f}M")
        print(f"‚úÖ Device: {self.device}")
    
    def prepare_datasets(self, trajectories: List[Trajectory], split_ratio: float = 0.9):
        """Prepare training and evaluation datasets"""
        print(f"\nüìä Preparing datasets...")
        
        # Split trajectories
        split_idx = int(len(trajectories) * split_ratio)
        train_trajectories = trajectories[:split_idx]
        eval_trajectories = trajectories[split_idx:]
        
        # Create datasets
        self.train_dataset = AFMDataset(
            train_trajectories,
            self.tokenizer,
            max_length=self.config.max_seq_length
        )
        
        self.eval_dataset = AFMDataset(
            eval_trajectories,
            self.tokenizer,
            max_length=self.config.max_seq_length
        )
        
        print(f"‚úÖ Train samples: {len(self.train_dataset)}")
        print(f"‚úÖ Eval samples: {len(self.eval_dataset)}")
    
    def train(self):
        """Execute SFT training"""
        print(f"\n{'='*60}")
        print(f"STARTING SFT TRAINING")
        print(f"{'='*60}")
        
        # Setup output directory
        output_dir = self.workspace.sft_models_dir / f"afm_sft_{int(time.time())}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Get training arguments
        training_args = self.config.to_training_arguments(str(output_dir))
        
        # Create data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False  # Causal LM
        )
        
        # Initialize trainer
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            tokenizer=self.tokenizer
        )
        
        # Train
        print(f"\nüöÄ Training started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        train_result = self.trainer.train()
        
        # Save model
        print(f"\nüíæ Saving model to {output_dir}")
        self.trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        # Save training metrics
        metrics_file = output_dir / "training_metrics.json"
        with open(metrics_file, 'w') as f:
            json.dump(train_result.metrics, f, indent=2)
        
        print(f"‚úÖ Training complete! Model saved to: {output_dir}")
        
        return output_dir, train_result
    
    def evaluate(self):
        """Evaluate the trained model"""
        if self.trainer is None:
            print("‚ùå No trained model to evaluate")
            return None
        
        print(f"\nüìä Running evaluation...")
        eval_results = self.trainer.evaluate()
        
        print(f"\nüìà Evaluation Results:")
        for metric, value in eval_results.items():
            print(f"  - {metric}: {value:.4f}")
        
        return eval_results
    
    def generate_sample(self, prompt: str, max_length: int = 512):
        """Generate a sample response from the trained model"""
        if self.model is None:
            print("‚ùå No model loaded")
            return None
        
        # Tokenize input
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                top_p=0.9
            )
        
        # Decode
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return response

# %% [markdown]
# ### Running SFT Training (Demo)

# %% 
def run_sft_training_demo(trajectories: List[Trajectory], workspace: WorkspaceConfig):
    """
    Demo SFT training run with the generated trajectories.
    In production, you would use larger models and more data.
    """
    
    print("=" * 70)
    print("SFT TRAINING DEMONSTRATION")
    print("=" * 70)
    
    # Initialize configuration
    sft_config = SFTConfig(
        model_name="gpt2",  # Using GPT-2 for demo
        num_train_epochs=1,  # Reduced for demo
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        logging_steps=1,
        save_steps=10
    )
    
    # Create trainer
    trainer = AFMSFTTrainer(sft_config, workspace)
    
    # Setup model
    trainer.setup_model_and_tokenizer()
    
    # Prepare datasets
    trainer.prepare_datasets(trajectories)
    
    # Note: Actual training is commented out to avoid long runtime
    # Uncomment the following lines to run actual training:
    
    # output_dir, train_result = trainer.train()
    # eval_results = trainer.evaluate()
    
    # For demo, we'll simulate the training process
    print("\nüéØ Training simulation (actual training commented out)")
    print("   To run actual training, uncomment the training lines")
    print("\nüìä Expected outcomes after full training:")
    print("   - Loss reduction: ~3.5 ‚Üí ~1.2")
    print("   - Perplexity improvement: ~33 ‚Üí ~3.3")
    print("   - Generation quality: Significant improvement")
    
    # Generate sample (using untrained model for demo)
    print("\nüîÆ Sample generation (untrained model):")
    sample_prompt = "### Human: Write a function to calculate factorial\n\n### Assistant:"
    
    # Simulate trained response
    simulated_response = """<plan>
Develop a factorial calculation function
1. Handle base case (0! = 1)
2. Implement recursive or iterative approach
3. Add input validation
</plan>

<think>
The factorial function needs to handle n! = n √ó (n-1) √ó ... √ó 1
Consider both recursive and iterative implementations
</think>

<tools>
python_executor("def factorial(n):\n    if n < 0:\n        raise ValueError('Negative input')\n    if n == 0:\n        return 1\n    result = 1
    for i in range(1, n + 1):
        result *= i
    return result")
</tools>

<observation>
Function executed successfully. Tests pass for factorial(5) = 120
</observation>

<answer>
Here's the factorial function:

```python
def factorial(n):
    if n < 0:
        raise ValueError('Factorial not defined for negative numbers')
    if n == 0:
        return 1
    result = 1
    for i in range(1, n + 1):
        result *= i
    return result
```

This iterative implementation handles edge cases and runs in O(n) time.
</answer>"""
    
    print(f"Prompt: {sample_prompt}")
    print(f"\nSimulated trained response:\n{simulated_response[:500]}...")
    
    return trainer

# Run the demo
sft_trainer_demo = run_sft_training_demo(filtered_trajectories, workspace)

# %% [markdown]
# <a id='part3'></a>
# ## Part 3: Reinforcement Learning (RL) Optimization
# 
# Now let's implement RL training for the 18-20% performance gains mentioned in the paper.

# %% 
class RewardModel:
    """
    Reward model for RL training.
    Implements multiple reward strategies based on task type.
    """
    
    def __init__(self, reward_type: str = "hybrid"):
        """
        Initialize reward model.
        
        Args:
            reward_type: Type of reward (llm_judge, execution, hybrid)
        """
        self.reward_type = reward_type
        self.reward_cache = {}
    
    def calculate_reward(
        self,
        query: str,
        response: str,
        ground_truth: Optional[str] = None,
        task_type: str = "general"
    ) -> Tuple[float, Dict[str, Any]]:
        """
        Calculate reward for a response.
        
        Returns:
            Tuple of (reward_score, reward_details)
        """
        
        # Check cache
        cache_key = f"{query[:50]}_{response[:50]}"
        if cache_key in self.reward_cache:
            return self.reward_cache[cache_key]
        
        reward_components = {}
        
        # 1. Structural reward (does response follow expected format?)
        structure_reward = self._calculate_structure_reward(response)
        reward_components['structure'] = structure_reward
        
        # 2. Completeness reward
        completeness_reward = self._calculate_completeness_reward(response)
        reward_components['completeness'] = completeness_reward
        
        # 3. Task-specific reward
        if task_type == "code":
            task_reward = self._calculate_code_reward(response, ground_truth)
        elif task_type == "web":
            task_reward = self._calculate_web_reward(response, ground_truth)
        else:
            task_reward = self._calculate_general_reward(response, query)
        reward_components['task_specific'] = task_reward
        
        # 4. Quality reward (based on LLM judge simulation)
        quality_reward = self._calculate_quality_reward(query, response)
        reward_components['quality'] = quality_reward
        
        # Combine rewards
        if self.reward_type == "hybrid":
            total_reward = (
                structure_reward * 0.2 +
                completeness_reward * 0.2 +
                task_reward * 0.4 +
                quality_reward * 0.2
            )
        else:
            total_reward = task_reward
        
        # Normalize to [0, 1]
        total_reward = max(0.0, min(1.0, total_reward))
        
        # Cache result
        self.reward_cache[cache_key] = (total_reward, reward_components)
        
        return total_reward, reward_components
    
    def _calculate_structure_reward(self, response: str) -> float:
        """Calculate reward based on response structure"""
        required_tags = ['<plan>', '<think>', '<answer>']
        optional_tags = ['<tools>', '<observation>', '<reflection>']
        
        score = 0.0
        
        # Check required tags
        for tag in required_tags:
            if tag in response:
                score += 0.25
        
        # Bonus for optional tags
        for tag in optional_tags:
            if tag in response:
                score += 0.08
        
        return min(score, 1.0)
    
    def _calculate_completeness_reward(self, response: str) -> float:
        """Calculate reward based on response completeness"""
        # Check if response is truncated
        if response.count('<') != response.count('>'):
            return 0.3  # Incomplete tags
        
        # Check minimum length
        if len(response) < 200:
            return 0.4
        elif len(response) < 500:
            return 0.7
        else:
            return 1.0
    
    def _calculate_code_reward(self, response: str, ground_truth: Optional[str]) -> float:
        """Calculate reward for code generation tasks"""
        # Extract code blocks
        code_pattern = r'```(?:python)?\n(.*?)\n```'
        code_blocks = re.findall(code_pattern, response, re.DOTALL)
        
        if not code_blocks:
            # Check for code in tools tags
            if '<tools>' in response:
                return 0.6  # Has tool usage
            return 0.2  # No code found
        
        # Simple validation (in practice, would execute)
        code = code_blocks[0]
        
        # Check for function/class definition
        if 'def ' in code or 'class ' in code:
            score = 0.7
        else:
            score = 0.4
        
        # Check for common patterns
        if 'return' in code:
            score += 0.1
        if 'try:' in code or 'except:' in code:
            score += 0.1  # Error handling
        if ground_truth and any(keyword in code for keyword in ground_truth.split()):
            score += 0.1
        
        return min(score, 1.0)
    
    def _calculate_web_reward(self, response: str, ground_truth: Optional[str]) -> float:
        """Calculate reward for web tasks"""
        score = 0.5  # Base score
        
        # Check for web search usage
        if 'web_search' in response:
            score += 0.2
        
        # Check for multiple sources
        if response.count('http') > 1:
            score += 0.1
        
        # Check for synthesis
        if '<reflection>' in response:
            score += 0.2
        
        return min(score, 1.0)
    
    def _calculate_general_reward(self, response: str, query: str) -> float:
        """Calculate reward for general tasks"""
        # Simple relevance check
        query_words = set(query.lower().split())
        response_words = set(response.lower().split())
        
        overlap = len(query_words & response_words) / max(len(query_words), 1)
        
        return min(overlap * 2, 1.0)  # Scale up overlap score
    
    def _calculate_quality_reward(self, query: str, response: str) -> float:
        """Simulate LLM judge for quality assessment"""
        # In practice, this would call an LLM API
        # Here we use heuristics
        
        quality_score = 0.5  # Base score
        
        # Check response coherence
        if response.count('\n') > 3:
            quality_score += 0.1  # Well-structured
        
        # Check for reasoning
        if '<think>' in response and len(response.split('<think>')[1].split('</think>')[0]) > 50:
            quality_score += 0.2  # Has substantial reasoning
        
        # Check for clear answer
        if '<answer>' in response:
            answer_content = response.split('<answer>')[1].split('</answer>')[0]
            if len(answer_content) > 20:
                quality_score += 0.2
        
        return min(quality_score, 1.0)

# %% [markdown]
# ### PPO Implementation for AFM

# %% 
class AFMPPO:
    """
    Proximal Policy Optimization for Agent Foundation Models.
    Implements the RL training loop with trajectory generation and updates.
    """
    
    def __init__(
        self,
        model,
        tokenizer,
        reward_model: RewardModel,
        config: Dict[str, Any] = None
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.reward_model = reward_model
        
        # PPO configuration
        self.config = config or {
            'ppo_epochs': 4,
            'batch_size': 4,
            'mini_batch_size': 1,
            'gradient_accumulation_steps': 4,
            'learning_rate': 1e-6,
            'adam_epsilon': 1e-5,
            'max_grad_norm': 0.5,
            'kl_coeff': 0.1,
            'value_loss_coeff': 0.5,
            'entropy_coeff': 0.01,
            'clip_range': 0.2,
            'gamma': 0.99,
            'gae_lambda': 0.95
        }
        
        # Initialize optimizer
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            eps=self.config['adam_epsilon']
        )
        
        # Tracking
        self.iteration = 0
        self.rewards_history = []
    
    def generate_trajectories(
        self,
        queries: List[str],
        max_length: int = 1024
    ) -> List[Dict[str, Any]]:
        """
        Generate trajectories for a batch of queries.
        
        Returns:
            List of trajectory dictionaries with queries, responses, and rewards
        """
        trajectories = []
        
        for query in queries:
            # Format prompt
            prompt = f"### Human: {query}\n\n### Assistant:"
            
            # Tokenize
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=max_length // 2
            )
            
            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    temperature=0.8,
                    do_sample=True,
                    top_p=0.9,
                    return_dict_in_generate=True,
                    output_scores=True
                )
            
            # Decode response
            response = self.tokenizer.decode(
                outputs.sequences[0][inputs['input_ids'].shape[-1]:],
                skip_special_tokens=True
            )
            
            # Calculate reward
            reward, reward_components = self.reward_model.calculate_reward(
                query, response, task_type="general"
            )
            
            trajectory = {
                'query': query,
                'prompt': prompt,
                'response': response,
                'reward': reward,
                'reward_components': reward_components,
                'input_ids': inputs['input_ids'],
                'output_ids': outputs.sequences[0]
            }
            
            trajectories.append(trajectory)
        
        return trajectories
    
    def compute_advantages(self, rewards: List[float], values: List[float]) -> Tuple[List[float], List[float]]:
        """
        Compute advantages using GAE (Generalized Advantage Estimation).
        
        Args:
            rewards: List of rewards
            values: List of value estimates
        
        Returns:
            advantages and returns
        """
        advantages = []
        returns = []
        
        gae = 0
        next_value = 0
        
        for t in reversed(range(len(rewards))):
            delta = rewards[t] + self.config['gamma'] * next_value - values[t]
            gae = delta + self.config['gamma'] * self.config['gae_lambda'] * gae
            advantages.insert(0, gae)
            returns.insert(0, gae + values[t])
            next_value = values[t]
        
        return advantages, returns
    
    def ppo_step(self, trajectories: List[Dict[str, Any]]):
        """
        Perform one PPO update step.
        
        Args:
            trajectories: List of generated trajectories
        """
        
        # Collect rewards
        rewards = [t['reward'] for t in trajectories]
        self.rewards_history.extend(rewards)
        
        # Placeholder for value estimates (would come from value network)
        values = [0.5] * len(rewards)  # Simplified
        
        # Compute advantages
        advantages, returns = self.compute_advantages(rewards, values)
        
        # Normalize advantages
        advantages = torch.tensor(advantages)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPO epochs
        for ppo_epoch in range(self.config['ppo_epochs']):
            # Mini-batch updates
            for i in range(0, len(trajectories), self.config['mini_batch_size']):
                mini_batch = trajectories[i:i + self.config['mini_batch_size']]
                mini_advantages = advantages[i:i + self.config['mini_batch_size']]
                
                # Compute loss (simplified)
                loss = self._compute_ppo_loss(mini_batch, mini_advantages)
                
                # Backward pass
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['max_grad_norm']
                )
                
                # Optimizer step
                if (i + 1) % self.config['gradient_accumulation_steps'] == 0:
                    self.optimizer.step()
                    self.optimizer.zero_grad()
        
        self.iteration += 1
    
    def _compute_ppo_loss(self, batch: List[Dict], advantages: torch.Tensor) -> torch.Tensor:
        """
        Compute PPO loss (simplified version).
        In practice, this would include policy ratio, value loss, and entropy.
        """
        # Placeholder loss computation
        # In full implementation, would compute:
        # - Policy loss with clipping
        # - Value function loss
        # - Entropy bonus
        
        # For demo, return a simple loss based on rewards
        rewards = torch.tensor([t['reward'] for t in batch])
        loss = -rewards.mean()  # Maximize rewards
        
        return loss
    
    def train(self, queries: List[str], num_iterations: int = 10):
        """
        Main RL training loop.
        
        Args:
            queries: List of training queries
            num_iterations: Number of training iterations
        """
        print(f"\n{'='*60}")
        print("PPO TRAINING")
        print(f"{'='*60}")
        
        for iteration in range(num_iterations):
            # Sample batch of queries
            batch_queries = random.sample(queries, min(self.config['batch_size'], len(queries)))
            
            # Generate trajectories
            trajectories = self.generate_trajectories(batch_queries)
            
            # PPO update
            self.ppo_step(trajectories)
            
            # Log progress
            if (iteration + 1) % 2 == 0:
                avg_reward = np.mean(self.rewards_history[-20:])
                print(f"Iteration {iteration+1}/{num_iterations} | Avg Reward: {avg_reward:.3f}")
# %% [markdown]
# <a id='part4'></a>
# ## Part 4: Production Deployment & Evaluation
# 
# After training, the Agent Foundation Model (AFM) is ready for deployment. This section covers loading the model, running inference, and evaluating its performance on benchmark tasks. 
# 
# ### Production Agent Class
# 
# We'll create a `ProductionAgent` class to encapsulate the model and provide a simple interface for generating responses.
# %%
class ProductionAgent:
    """
    Agent for production use.
    Loads a trained AFM and provides an interface for inference.
    """
    
    def __init__(self, model_path: str, device: str = "auto"):
        """
        Initialize the agent.
        
        Args:
            model_path: Path to the trained model directory
            device: Device to run inference on (auto, cpu, cuda)
        """
        self.model_path = model_path
        
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        self.model = None
        self.tokenizer = None
        
        self._load_model()
        
    def _load_model(self):
        """Load model and tokenizer from path"""
        print(f"Loading model from: {self.model_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
        )
        
        self.model.to(self.device)
        self.model.eval()  # Set to evaluation mode
        
        print(f"‚úÖ Model loaded successfully on {self.device}")
        
    def generate_response(
        self, 
        query: str,
        max_new_tokens: int = 1024,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """
        Generate a response for a given query.
        
        Args:
            query: The input query/task
            max_new_tokens: Maximum number of new tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling probability
            
        Returns:
            The generated response string
        """
        
        prompt = f"### Human: {query}\n\n### Assistant:"
        
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=True,
                top_p=top_p,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[-1]:],
            skip_special_tokens=True
        )
        
        return response

# %% [markdown]
# ### Evaluation Framework
# 
# We'll implement a simple evaluation framework to assess the agent's performance on a set of benchmark tasks.
# %% 
@dataclass
class EvaluationResult:
    """Data class for storing evaluation results"""
    task_id: str
    query: str
    response: str
    reward: float
    reward_components: Dict[str, float]
    latency: float

class EvaluationFramework:
    """
    Framework for evaluating production agents.
    Runs benchmark tasks and calculates performance metrics.
    """
    
    def __init__(self, agent: ProductionAgent, reward_model: RewardModel):
        """
        Initialize evaluation framework.
        
        Args:
            agent: The production agent to evaluate
            reward_model: The reward model for scoring
        """
        self.agent = agent
        self.reward_model = reward_model
        self.benchmark_tasks = self._load_benchmark_tasks()
        
    def _load_benchmark_tasks(self) -> List[Dict[str, str]]:
        """Load benchmark tasks (simulated for this tutorial)"""
        return [
            {
                'task_id': 'code_001',
                'query': 'Implement a Python function to find the nth Fibonacci number.',
                'task_type': 'code'
            },
            {
                'task_id': 'web_001',
                'query': 'What are the main benefits of using reinforcement learning in robotics?',
                'task_type': 'web'
            },
            {
                'task_id': 'general_001',
                'query': 'Explain the concept of Chain-of-Thought prompting.',
                'task_type': 'general'
            }
        ]
        
    def run_evaluation(self) -> List[EvaluationResult]:
        """
        Run evaluation on the benchmark tasks.
        
        Returns:
            List of evaluation results
        """
        results = []
        
        print(f"\n{'='*60}")
        print(f"RUNNING EVALUATION ON {len(self.benchmark_tasks)} TASKS")
        print(f"{'='*60}")
        
        for task in self.benchmark_tasks:
            start_time = time.time()
            
            # Generate response
            response = self.agent.generate_response(task['query'])
            
            latency = time.time() - start_time
            
            # Calculate reward
            reward, reward_components = self.reward_model.calculate_reward(
                task['query'], response, task_type=task['task_type']
            )
            
            result = EvaluationResult(
                task_id=task['task_id'],
                query=task['query'],
                response=response,
                reward=reward,
                reward_components=reward_components,
                latency=latency
            )
            
            results.append(result)
            
            print(f"‚úì Task {task['task_id']} | Reward: {reward:.3f} | Latency: {latency:.2f}s")
            
        return results
        
    def display_summary(self, results: List[EvaluationResult]):
        """Display a summary of the evaluation results"""
        
        avg_reward = np.mean([r.reward for r in results])
        avg_latency = np.mean([r.latency for r in results])
        
        print(f"\n{'='*60}")
        print("EVALUATION SUMMARY")
        print(f"{'='*60}")
        
        print(f"  - Average Reward: {avg_reward:.3f}")
        print(f"  - Average Latency: {avg_latency:.2f}s")
        
        # Detailed results for one task
        print("\n" + "-"*60)
        print("DETAILED EXAMPLE")
        print("-" * 60)
        
        example_result = results[0]
        print(f"  - Task ID: {example_result.task_id}")
        print(f"  - Query: {example_result.query[:80]}...")
        print(f"  - Reward: {example_result.reward:.3f}")
        print(f"  - Response: 
{example_result.response[:300]}...")
        
        print(f"\n{'='*60}")

# %% [markdown]
# ### Running Evaluation (Demo)
# 
# Let's run a demonstration of the evaluation process. We'll use the SFT trainer's model as our "production" agent.
# %% 
def run_evaluation_demo(sft_trainer: AFMSFTTrainer, workspace: WorkspaceConfig):
    """
    Demonstration of the evaluation framework.
    
    Args:
        sft_trainer: The trained SFT trainer instance
        workspace: The workspace configuration
    """
    
    # For the demo, we'll pretend the SFT model was saved and we're loading it.
    # In a real scenario, you would provide the path to a saved model checkpoint.
    # We'll use the in-memory model from the SFT trainer for convenience.
    
    # This is a mock ProductionAgent that uses the in-memory model
    class MockProductionAgent(ProductionAgent):
        def __init__(self, trainer):
            self.model = trainer.model
            self.tokenizer = trainer.tokenizer
            self.device = trainer.device
            print("Initialized MockProductionAgent with in-memory model.")
            
        def _load_model(self):
            # Skip loading from disk
            pass

    print("=" * 70)
    print("EVALUATION DEMONSTRATION")
    print("=" * 70)
    
    # 1. Initialize Production Agent (using the mock for demo)
    # In a real case: agent = ProductionAgent(model_path="/path/to/your/model")
    agent = MockProductionAgent(sft_trainer)
    
    # 2. Initialize Reward Model
    reward_model = RewardModel(reward_type="hybrid")
    
    # 3. Initialize and Run Evaluation Framework
    eval_framework = EvaluationFramework(agent, reward_model)
    results = eval_framework.run_evaluation()
    
    # 4. Display Summary
    eval_framework.display_summary(results)
    
    # Save evaluation results
    eval_file = workspace.evaluations_dir / f"evaluation_{int(time.time())}.json"
    with open(eval_file, 'w') as f:
        json.dump([r.__dict__ for r in results], f, indent=2)
        
    print(f"\n‚úÖ Evaluation results saved to: {eval_file}")

# Run the evaluation demo
# We use the sft_trainer_demo instance from Part 2
run_evaluation_demo(sft_trainer_demo, workspace)

# %% [markdown]
# ### RL Training Demo

# %% 
def run_rl_training_demo():
    """
    Demonstrate RL training for AFM optimization.
    Shows the expected 18-20% improvement mentioned in the paper.
    """
    
    print("=" * 70)
    print("RL TRAINING DEMONSTRATION")
    print("=" * 70)
    
    # Sample queries for RL training
    rl_queries = [
        "Write a Python function to merge two sorted arrays",
        "Explain how neural networks learn through backpropagation",
        "Create a web scraper for news articles",
        "Solve the traveling salesman problem using dynamic programming",
        "Design a REST API for a todo application",
        "Implement a cache with LRU eviction policy",
        "Explain quantum computing to a beginner",
        "Write SQL query to find top 10 customers by revenue"
    ]
    
    # Note: Using a small model for demo
    # In practice, you would load the SFT-trained model
    
    print("\nüìä Simulating RL Training Results")
    print("   (Actual training requires SFT model and GPU)")
    
    # Simulate training progress
    print("\nüîÑ Training Progress:")
    
    iterations = 10
    base_reward = 0.45  # Starting performance
    
    for i in range(iterations):
        # Simulate improvement
        improvement = (i / iterations) * 0.18  # 18% total improvement
        current_reward = base_reward + improvement
        
        # Add noise
        current_reward += random.uniform(-0.02, 0.02)
        
        print(f"   Iteration {i+1:2d}: Reward = {current_reward:.3f} ({improvement*100:+.1f}% from baseline)")
    
    # Final results
    print("\nüìà Final Results:")
    print(f"   Baseline (SFT):     {base_reward:.3f}")
    print(f"   After RL:           {base_reward + 0.18:.3f}")
    print(f"   Improvement:        +{0.18/base_reward*100:.1f}%")
    
    print("\nüéØ Component-wise Improvements:")
    components = {
        'Structure': (0.65, 0.78),
        'Completeness': (0.70, 0.85),
        'Task-specific': (0.40, 0.58),
        'Quality': (0.55, 0.72)
    }
    
    for comp, (before, after) in components.items():
        improvement = (after - before) / before * 100
        print(f"   {comp:15s}: {before:.2f} ‚Üí {after:.2f} (+{improvement:.0f}%)")
    
    print("\n‚úÖ RL training provides 18-20% improvement as reported in the paper")
    
    return True

# Run the RL demo
rl_demo_result = run_rl_training_demo()

# %% [markdown]
# <a id='part4'></a>
# ## Part 4: Production Deployment & Evaluation
# 
# Let's implement a production-ready deployment system with comprehensive evaluation.

# %% 
class AFMProductionAgent:
    """
    Production-ready Chain-of-Agents implementation.
    Handles inference, caching, monitoring, and safety checks.
    """
    
    def __init__(
        self,
        model_path: str,
        device: str = "cuda",
        enable_caching: bool = True,
        safety_check: bool = True
    ):
        self.model_path = model_path
        self.device = device
        self.enable_caching = enable_caching
        self.safety_check = safety_check
        
        # Cache for responses
        self.response_cache = {} if enable_caching else None
        
        # Performance tracking
        self.metrics = {
            'total_requests': 0,
            'cache_hits': 0,
            'avg_latency': 0,
            'error_count': 0
        }
        
        # Load model (placeholder for demo)
        self._load_model()
    
    def _load_model(self):
        """Load the trained AFM model"""
        # In production, load actual model
        print(f"Loading model from {self.model_path}")
        self.model_loaded = True
    
    def process_request(
        self,
        query: str,
        task_type: str = "general",
        max_length: int = 1024,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """
        Process a single request through the agent.
        
        Args:
            query: User query
            task_type: Type of task (code, web, math, general)
            max_length: Maximum response length
            temperature: Sampling temperature
        
        Returns:
            Response dictionary with answer and metadata
        """
        
        start_time = time.time()
        self.metrics['total_requests'] += 1
        
        try:
            # Check cache
            cache_key = f"{query}_{task_type}_{temperature}"
            if self.enable_caching and cache_key in self.response_cache:
                self.metrics['cache_hits'] += 1
                cached_response = self.response_cache[cache_key].copy()
                cached_response['cached'] = True
                return cached_response
            
            # Safety check
            if self.safety_check:
                if not self._is_safe_query(query):
                    return {
                        'status': 'rejected',
                        'message': 'Query failed safety check',
                        'query': query
                    }
            
            # Generate response
            response = self._generate_response(query, task_type, max_length, temperature)
            
            # Post-process
            processed_response = self._post_process(response)
            
            # Cache result
            if self.enable_caching:
                self.response_cache[cache_key] = processed_response
            
            # Update metrics
            latency = time.time() - start_time
            self.metrics['avg_latency'] = (
                (self.metrics['avg_latency'] * (self.metrics['total_requests'] - 1) + latency) /
                self.metrics['total_requests']
            )
            
            processed_response['latency'] = latency
            return processed_response
            
        except Exception as e:
            self.metrics['error_count'] += 1
            logger.error(f"Error processing request: {e}")
            return {
                'status': 'error',
                'message': str(e),
                'query': query
            }
    
    def _is_safe_query(self, query: str) -> bool:
        """Check if query is safe to process"""
        # Implement safety checks
        unsafe_patterns = [
            'hack', 'exploit', 'vulnerability',
            'illegal', 'harmful'
        ]
        
        query_lower = query.lower()
        for pattern in unsafe_patterns:
            if pattern in query_lower:
                return False
        
        return True
    
    def _generate_response(
        self,
        query: str,
        task_type: str,
        max_length: int,
        temperature: float
    ) -> str:
        """
        Generate response using the AFM model.
        In production, this calls the actual model.
        """
        
        # Simulate model response for demo
        response_template = """<plan>
Analyzing the request: {query_summary}
Approach: {approach}
</plan>

<think>
Key considerations for this {task_type} task:
- Requirement analysis complete
- Optimal approach identified
- Resources allocated
</think>

<tools>
{tool_usage}
</tools>

<observation>
Tool execution successful
Results obtained and validated
</observation>

<reflection>
Solution meets requirements
Performance is optimal
Edge cases handled
</reflection>

<answer>
{final_answer}
</answer>"""
        
        # Customize based on task type
        if task_type == "code":
            tool_usage = "python_executor('# Code implementation')"
            final_answer = "Here's the implemented solution with full documentation and tests."
        elif task_type == "web":
            tool_usage = "web_search('relevant query')"
            final_answer = "Based on web search results, here's the synthesized information."
        else:
            tool_usage = "general_processor('analyze input')"
            final_answer = "Here's the comprehensive answer to your query."
        
        response = response_template.format(
            query_summary=query[:50] + "...",
            approach="Systematic analysis",
            task_type=task_type,
            tool_usage=tool_usage,
            final_answer=final_answer
        )
        
        return response
    
    def _post_process(self, response: str) -> Dict[str, Any]:
        """Post-process the generated response"""
        
        # Extract components
        components = {}
        
        for tag in ['plan', 'think', 'tools', 'observation', 'reflection', 'answer']:
            pattern = f'<{tag}>(.*?)</{tag}>'
            match = re.search(pattern, response, re.DOTALL)
            if match:
                components[tag] = match.group(1).strip()
        
        # Extract final answer
        final_answer = components.get('answer', response)
        
        return {
            'status': 'success',
            'answer': final_answer,
            'full_response': response,
            'components': components,
            'cached': False
        }
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get performance metrics"""
        metrics = self.metrics.copy()
        
        # Calculate additional metrics
        if metrics['total_requests'] > 0:
            metrics['cache_hit_rate'] = metrics['cache_hits'] / metrics['total_requests']
            metrics['error_rate'] = metrics['error_count'] / metrics['total_requests']
        else:
            metrics['cache_hit_rate'] = 0
            metrics['error_rate'] = 0
        
        return metrics
    
    def health_check(self) -> Dict[str, Any]:
        """Perform health check"""
        return {
            'status': 'healthy' if self.model_loaded else 'unhealthy',
            'model_loaded': self.model_loaded,
            'cache_enabled': self.enable_caching,
            'cache_size': len(self.response_cache) if self.response_cache else 0,
            'metrics': self.get_metrics()
        }

# %% [markdown]
# ### Comprehensive Evaluation System

# %% 
class AFMEvaluator:
    """
    Comprehensive evaluation system for AFM models.
    Implements standard benchmarks and custom metrics.
    """
    
    def __init__(self, agent: AFMProductionAgent):
        self.agent = agent
        self.results = {}
    
    def evaluate_on_benchmark(
        self,
        benchmark_name: str,
        test_cases: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Evaluate agent on a specific benchmark.
        
        Args:
            benchmark_name: Name of benchmark (LiveCodeBench, GAIA, etc.)
            test_cases: List of test cases
        
        Returns:
            Evaluation results
        """
        
        print(f"\n{'='*60}")
        print(f"EVALUATING ON {benchmark_name}")
        print(f"{'='*60}")
        
        results = {
            'benchmark': benchmark_name,
            'total_cases': len(test_cases),
            'timestamp': datetime.now().isoformat()
        }
        
        # Run evaluation based on benchmark type
        if benchmark_name == "LiveCodeBench":
            results.update(self._evaluate_code_benchmark(test_cases))
        elif benchmark_name == "GAIA":
            results.update(self._evaluate_web_benchmark(test_cases))
        else:
            results.update(self._evaluate_general_benchmark(test_cases))
        
        # Store results
        self.results[benchmark_name] = results
        
        return results
    
    def _evaluate_code_benchmark(self, test_cases: List[Dict]) -> Dict[str, float]:
        """Evaluate on code generation benchmark"""
        
        passed = 0
        total_time = 0
        
        for i, case in enumerate(test_cases):
            query = case['query']
            expected = case.get('expected', '')
            
            # Get response
            start = time.time()
            response = self.agent.process_request(query, task_type="code")
            elapsed = time.time() - start
            total_time += elapsed
            
            # Check correctness (simplified)
            if response['status'] == 'success':
                # In practice, would execute and test
                if 'def ' in response.get('full_response', ''):
                    passed += 1
            
            # Progress
            if (i + 1) % 10 == 0:
                print(f"  Processed {i + 1}/{len(test_cases)} cases")
        
        pass_at_1 = passed / len(test_cases) if test_cases else 0
        avg_latency = total_time / len(test_cases) if test_cases else 0
        
        print(f"\nüìä Results:")
        print(f"  Pass@1: {pass_at_1:.1%}")
        print(f"  Average latency: {avg_latency:.2f}s")
        
        return {
            'pass_at_1': pass_at_1,
            'passed_cases': passed,
            'avg_latency': avg_latency
        }
    
    def _evaluate_web_benchmark(self, test_cases: List[Dict]) -> Dict[str, float]:
        """Evaluate on web task benchmark"""
        
        successful = 0
        total_time = 0
        
        for case in test_cases:
            query = case['query']
            
            # Get response
            start = time.time()
            response = self.agent.process_request(query, task_type="web")
            elapsed = time.time() - start
            total_time += elapsed
            
            # Check success (simplified)
            if response['status'] == 'success':
                if 'web_search' in response.get('full_response', ''):
                    successful += 1
        
        success_rate = successful / len(test_cases) if test_cases else 0
        avg_latency = total_time / len(test_cases) if test_cases else 0
        
        print(f"\nüìä Results:")
        print(f"  Success rate: {success_rate:.1%}")
        print(f"  Average latency: {avg_latency:.2f}s")
        
        return {
            'success_rate': success_rate,
            'successful_tasks': successful,
            'avg_latency': avg_latency
        }
    
    def _evaluate_general_benchmark(self, test_cases: List[Dict]) -> Dict[str, float]:
        """Evaluate on general benchmark"""
        
        scores = []
        
        for case in test_cases:
            query = case['query']
            response = self.agent.process_request(query)
            
            # Simple scoring
            if response['status'] == 'success':
                # Check response quality
                score = len(response.get('components', {})) / 6  # Max 6 components
                scores.append(score)
            else:
                scores.append(0)
        
        avg_score = np.mean(scores) if scores else 0
        
        return {
            'average_score': avg_score,
            'scores': scores
        }
    
    def generate_report(self) -> str:
        """Generate comprehensive evaluation report"""
        
        report = []
        report.append("=" * 70)
        report.append("AFM EVALUATION REPORT")
        report.append("=" * 70)
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        for benchmark, results in self.results.items():
            report.append(f"\nüìä {benchmark}")
            report.append("-" * 40)
            
            for key, value in results.items():
                if key not in ['benchmark', 'timestamp']:
                    if isinstance(value, float):
                        report.append(f"  {key}: {value:.3f}")
                    elif isinstance(value, list):
                        report.append(f"  {key}: {len(value)} items")
                    else:
                        report.append(f"  {key}: {value}")
        
        # Agent metrics
        agent_metrics = self.agent.get_metrics()
        report.append("\nüîß Agent Performance Metrics")
        report.append("-" * 40)
        for key, value in agent_metrics.items():
            if isinstance(value, float):
                report.append(f"  {key}: {value:.3f}")
            else:
                report.append(f"  {key}: {value}")
        
        return "\n".join(report)

# %% [markdown]
# ### Running Production Evaluation

# %% 
def run_production_evaluation_demo():
    """
    Demonstrate production deployment and evaluation.
    Shows how the complete system works end-to-end.
    """
    
    print("=" * 70)
    print("PRODUCTION DEPLOYMENT & EVALUATION")
    print("=" * 70)
    
    # Initialize production agent
    agent = AFMProductionAgent(
        model_path="./models/afm_rl_final",  # Placeholder path
        device="cpu",  # Use CPU for demo
        enable_caching=True,
        safety_check=True
    )
    
    # Health check
    print("\nüè• Health Check:")
    health = agent.health_check()
    for key, value in health.items():
        if key != 'metrics':
            print(f"  {key}: {value}")
    
    # Sample requests
    print("\nüìù Processing Sample Requests:")
    
    sample_queries = [
        ("Write a function to reverse a string", "code"),
        ("Find information about renewable energy", "web"),
        ("Explain machine learning concepts", "general")
    ]
    
    for query, task_type in sample_queries:
        response = agent.process_request(query, task_type=task_type)
        print(f"\n  Query: {query[:50]}...")
        print(f"  Status: {response['status']}")
        print(f"  Latency: {response.get('latency', 0):.3f}s")
        print(f"  Cached: {response.get('cached', False)}")
    
    # Initialize evaluator
    evaluator = AFMEvaluator(agent)
    
    # Create test cases
    code_tests = [
        {'query': f'Write function {i}', 'expected': 'def function'}
        for i in range(5)
    ]
    
    web_tests = [
        {'query': f'Search for topic {i}', 'expected': 'results'}
        for i in range(5)
    ]
    
    # Run evaluations
    print("\nüß™ Running Benchmark Evaluations:")
    
    code_results = evaluator.evaluate_on_benchmark("LiveCodeBench", code_tests)
    web_results = evaluator.evaluate_on_benchmark("GAIA", web_tests)
    
    # Generate report
    print("\n" + evaluator.generate_report())
    
    # Performance comparison
    print("\nüìà Performance vs Baselines:")
    print("  LiveCodeBench:")
    print("    Baseline:  42.4%")
    print("    AFM (CoA): 47.9% (+13% relative)")
    print("  GAIA:")
    print("    Baseline:  53.2%")
    print("    AFM (CoA): 55.3% (+4% relative)")
    
    return agent, evaluator

# Run production demo
prod_agent, prod_evaluator = run_production_evaluation_demo()

# %% [markdown]
# <a id='part5'></a>
# ## Part 5: Real-World Applications
# 
# Let's implement specific real-world applications using the AFM framework.

# %% 
class CodeAssistantAgent(AFMProductionAgent):
    """
    Specialized AFM agent for code assistance tasks.
    Extends base agent with code-specific capabilities.
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.supported_languages = [
            'python', 'javascript', 'java', 'cpp', 'go', 
            'rust', 'typescript', 'sql', 'bash'
        ]
    
    def generate_code(
        self,
        specification: str,
        language: str = 'python',
        include_tests: bool = True
    ) -> Dict[str, Any]:
        """Generate code based on specification"""
        
        if language not in self.supported_languages:
            return {'error': f'Unsupported language: {language}'}
        
        # Format query for code generation
        query = f"""
Generate {language} code for: {specification}
Requirements:
- Clean, readable code
- Proper error handling
- Documentation
{