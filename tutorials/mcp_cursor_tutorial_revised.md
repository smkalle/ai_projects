
# ğŸš€ Building a Productionâ€‘Ready AI Agent with MCP, Cursor.ai, GeminiÂ 2.5 & LangGraph

> **Audience:** AI/ML engineers who need a reproducible, openâ€‘source path from _zero_ to a fullâ€‘stack, researchâ€‘augmented conversational agent.  
> **Stack:** **Cursor.ai** Â· **ModelÂ ContextÂ Protocol (MCP)** Â· **GoogleÂ GeminiÂ 2.5** Â· **LangGraph** Â· **Replit**  

---

## ğŸ“‘ TableÂ ofÂ Contents
1. [Why This Stack?](#why-this-stack)
2. [Prerequisites](#prerequisites)
3. [PartÂ 0Â â€” Connect Cursor to MCP](#part-0--connect-cursor-to-mcp)
4. [PartÂ 1Â â€” LocalÂ &Â Cloud Setup](#part-1--local--cloud-setup)
5. [PartÂ 2Â â€” Architecture DeepÂ Dive](#part-2--architecture-deep-dive)
6. [PartÂ 3Â â€” Handsâ€‘On Customisation](#part-3--hands-on-customisation)
7. [PartÂ 4Â â€” Test, Iterate & Deploy](#part-4--test-iterate--deploy)
8. [Troubleshooting](#troubleshooting)
9. [Credits](#credits)

---

## Why This Stack?
| Layer | Why it matters |
|-------|----------------|
| **MCP** | Open protocol for connecting LLMs to external data/tools without bespoke adapters. |
| **Cursor IDE** | AIâ€‘native code editor with oneâ€‘click MCP integration, inline edits, chatâ€‘withâ€‘code. |
| **GeminiÂ 2.5** | Googleâ€™s latest multimodal LLM (128â€¯k+ context, strong reasoning, code understanding). |
| **LangGraph** | Declarative stateâ€‘machine library for multiâ€‘actor LLM workflows. |
| **Replit** | Zeroâ€‘friction cloud IDE with CI/CD deploy & live URLs. |

> *2023 GitHub Octoverse reported a **40â€¯%** surge in openâ€‘source AI projects, while Replit case studies show prototypes built **3Ã—** faster with AIâ€‘assisted coding.*

---

## Prerequisites
| Tool | MinÂ Version | Notes |
|------|-------------|-------|
| **Cursor** |Â `v0.48+` | <https://cursor.com> |
| **Python** |Â `3.10+` | Poetry/uv recommended |
| **GoogleÂ AI key** | GeminiÂ 2.5Â Pro | <https://ai.google.dev> |
| **Replit** account | Free tier OK | â€œRun onÂ Replitâ€ supported |
| **MCP PythonÂ SDK** |Â `>=0.3` | `pipÂ installÂ mcp` |

---

## PartÂ 0Â â€” Connect Cursor to MCP
###Â 0.1Â GUI install
1. Open **Cursorâ€¯â†’Â Settingsâ€¯â†’Â MCP**.  
2. Browse available tools & click **Install**.  
3. OAuth if promptedâ€”the tool will appear under *Available Tools*.

###Â 0.2Â ManualÂ `.cursor/mcp.json`
```jsonc
{
  "mcpServers": {
    "local-weather": {
      "command": "python",
      "args": ["weather_server.py"],
      "env": { "OPEN_METEO_URL": "https://api.open-meteo.com" }
    }
  }
}
```
Cursor autostarts eachÂ server and exposes its declared tools to the builtâ€‘in agent.

###Â 0.3Â TransportÂ Options

| Transport | RunsÂ Where | Useâ€‘case |
|-----------|-----------|----------|
| **stdio** | Local process | Fast prototyping |
| **SSE** | Local/remote HTTP | Multiâ€‘user teams |
| **Streamable HTTP** | Remote microâ€‘tools | Highâ€‘latency tolerant |

---

## PartÂ 1Â â€” LocalÂ &Â Cloud Setup
###Â 1.1Â Configure GeminiÂ 2.5 in Cursor
* `âŒ˜/CtrlÂ +,`â€¯â†’Â **Modelsâ€¯â†’Â AddÂ Custom**  
* Paste `GOOGLE_API_KEY`, verify â†’ choose **`geminiâ€‘2.5â€‘pro`** as default.

###Â 1.2Â Fork Googleâ€™s Fullâ€‘Stack Agent on Replit
```bash
replit push https://github.com/google/full-stack-agent
```
Add secrets:
```env
GOOGLE_API_KEY=...
OPEN_METEO_URL=https://api.open-meteo.com
```

###Â 1.3Â Install Deps & Example MCP Server
```bash
pip install mcp fastapi uvicorn langgraph google-generativeai python-dotenv
git submodule add https://github.com/modelcontextprotocol/python-sdk mcp_sdk
cp mcp_sdk/examples/weather_server.py .
```

---

## PartÂ 2Â â€” Architecture DeepÂ Dive
```mermaid
graph TD
  UI[Next.js Chat UI] -->|HTTP| API[FastAPI Gateway]
  API --> LG[LangGraph]
  LG -->|tool_call| CUR(CursorÂ MCPÂ Client)
  CUR -->|stdio /Â HTTP| MCP[(MCPÂ Server)]
  MCP --> W(WeatherÂ API) & S(SearchÂ API)
  LG --> GEM(GeminiÂ 2.5Â LLM)
```

* **UI**: React/NextÂ chat streams viaÂ SSE.  
* **LangGraph**: NodesÂ = tools; edgesÂ = controlâ€‘flow loops.  
* **Cursor MCP Client**: Handles schema validation, auth, retries.  

---

## PartÂ 3Â â€” Handsâ€‘On Customisation
###Â 3.1Â â€œHelloâ€‘Worldâ€ Persona Tweak
1. Open `backend/graph.py` in Cursor.  
2. Ask Chat: *â€œfind system promptâ€*.  
3. Replace the default prompt with:  

```python
"You are **Chronos**, a witty historian who cites primary sources."
```

###Â 3.2Â Add Researchâ€‘Augmented Generation (RAG)
*Improve query diversity*:
```python
def build_queries(q: str) -> list[str]:
    base = q.lower()
    return [
        f"{base} primary source",
        f"academic review of {base}",
        f"latest news on {base}"
    ]
```
*Return citations* by attaching a `sources` list and formatting footnotes in Geminiâ€™s final prompt.

###Â 3.3Â Create a Weather MCP Tool
`weather_server.py`
```python
from mcp.server.fastmcp import FastMCP
import httpx, os, geopy

mcp = FastMCP("Weather Server")

@mcp.tool(description="Get current temperature")
async def weather_now(city: str) -> str:
    lat, lon = geopy.geocoders.Nominatim(user_agent="weather").geocode(city)[1]
    r = httpx.get(os.getenv("OPEN_METEO_URL"), params={
        "latitude": lat, "longitude": lon, "hourly": "temperature_2m"})
    temp = r.json()["hourly"]["temperature_2m"][0]
    return f"{temp}â€¯Â°C right now in {city}"

if __name__ == "__main__":
    mcp.run_stdio()
```
Declare it in `.cursor/mcp.json` (see **0.2**). Cursor will autoâ€‘discover **`weather_now`**.

---

## PartÂ 4Â â€” Test, IterateÂ & Deploy
```bash
python weather_server.py &   # start MCP tool
uvicorn backend.api:app --reload
```
1. Open Replit preview.  
2. Ask: â€œWhatâ€™s the weather in Tokyo and cite two primary sources on its climate record?â€  
3. Iterate edits in Cursor â†’ hot reload triggers on Replit.  
4. Press **Replitâ€¯â†’â€¯Deploy** for a public URL.

---

## Troubleshooting
| Symptom | Fix |
|---------|-----|
| `401Â Unauthorized` from Gemini | Check billing & `GOOGLE_API_KEY`. |
| `ModuleNotFoundError: mcp` | `pip install mcp` and verify venv. |
| Cursor lists only 40 tools | Reduce active servers or disable unused tools. |
| Tools invisible in remote SSH | Prefer SSE/HTTP transports or run Cursor locally. |

---

## Credits
1. ModelÂ ContextÂ Protocol spec â€” <https://modelcontextprotocol.io>  
2. Cursor MCP docs â€” <https://docs.cursor.com/context/model-context-protocol>  
3. Google DeepMind GeminiÂ 2.5 â€” <https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/>  
4. LangGraph overview â€” <https://www.langchain.com/langgraph>  
5. GitHub OctoverseÂ 2023 â€” <https://github.blog/news-insights/research/the-state-of-open-source-and-ai/>  
6. Replit AI case study â€” <https://freshvanroot.com/blog/review-replit-ai-app-prototyping/>  
7. Anthropic MCP announcement â€” <https://www.anthropic.com/news/model-context-protocol>  
