{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4bde6e",
   "metadata": {},
   "source": [
    "# Building Asynchronous Gemini Pipelines with **GenAI Processors**\n",
    "\n",
    "**Release date:** 10Â JulÂ 2025  \n",
    "**Library version used:** `genai-processorsÂ 1.0.3`  \n",
    "\n",
    "> *This tutorial notebook is designed for absolute beginners. Each section mixes explanation with runnable code so you can learn by doing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cadb400",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "2. [Installation](#Installation)\n",
    "3. [Quickstart](#Quickstart)\n",
    "4. [Core Concepts](#Core-Concepts)\n",
    "5. [Handsâ€‘On Examples](#Hands-On-Examples)\n",
    "6. [Testing Your Processors](#Testing-Your-Processors)\n",
    "7. [Performance Tips](#Performance-Tips)\n",
    "8. [Deployment Snippet](#Deployment-Snippet)\n",
    "9. [Next Steps & Exercises](#Next-Steps-&-Exercises)\n",
    "10. [Resources](#Resources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08577886",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "- **PythonÂ â‰¥Â 3.10**\n",
    "- A **Google Cloud** project with the **Gemini API** enabled (for live examples)\n",
    "- Basic familiarity with running cells in Jupyter\n",
    "- A virtual environment is strongly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8822b04",
   "metadata": {},
   "source": [
    "## 2. Installation\n",
    "Run the cell below to install the library. The `--upgrade` flag is a good habit to make sure you get the latest patch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf75f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip wheel\n",
    "!pip install genai-processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0007e5",
   "metadata": {},
   "source": [
    "## 3. Quickstart<a id='Quickstart'></a>\n",
    "The next cell creates the simplest possible pipeline: text in â†’ Gemini model â†’ text out. The call is **fully async**, so we wrap it in an `asyncio` event loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f78c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from genai_processors.core import genai_model\n",
    "from genai_processors import content_api\n",
    "\n",
    "async def hello_gemini():\n",
    "    pipeline = genai_model.GenaiModel(model_name='gemini-2.0-flash')\n",
    "    inp = content_api.from_text('Hello, GenAI Processors!')\n",
    "    async for part in pipeline(inp):\n",
    "        if part.text:\n",
    "            print(part.text)\n",
    "\n",
    "await hello_gemini()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a1c65",
   "metadata": {},
   "source": [
    "## 4. Core Concepts<a id='Core-Concepts'></a>\n",
    "\n",
    "### 4.1 Processor & PartProcessor\n",
    "`Processor` is an **async function or class** that receives a *stream* of `ProcessorPart` objects and emits a (possibly transformed) stream.  \n",
    "Use the `@processor.PartProcessor` decorator for quick oneâ€‘off functions.\n",
    "\n",
    "### 4.2 ProcessorPart\n",
    "A `ProcessorPart` wraps **content + metadata** (e.g.\\ MIME type, role, custom attributes).  \n",
    "It can represent text, images, audio frames, or arbitrary JSON.\n",
    "\n",
    "### 4.3 Chaining & Parallelism\n",
    "- Use **`+`** to chain processors sequentially (output of left â†’ input of right).  \n",
    "- Use **`//`** to run processors in *parallel* on the same input stream.\n",
    "\n",
    "The library schedules downstream work as soon as a **single part** is ready, which drastically cuts **Timeâ€‘Toâ€‘Firstâ€‘Token** compared with fully synchronous pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5d054b",
   "metadata": {},
   "source": [
    "## 5. Handsâ€‘On Examples<a id='Hands-On-Examples'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc0031",
   "metadata": {},
   "source": [
    "### 5.1 ExampleÂ 1 â€“ Uppercase â†’ Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_processors import processor\n",
    "from genai_processors.core import genai_model, streams, content_api\n",
    "\n",
    "@processor.PartProcessor\n",
    "async def uppercase(part):\n",
    "    if part.text:\n",
    "        part.text = part.text.upper()\n",
    "    return part\n",
    "\n",
    "async def run_pipeline():\n",
    "    text_pipeline = uppercase + genai_model.GenaiModel(model_name='gemini-2.0-pro')\n",
    "    inp = content_api.from_text('Synchronous code is so 2024.')\n",
    "    async for part in text_pipeline(inp):\n",
    "        if part.text:\n",
    "            print(part.text)\n",
    "\n",
    "await run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd233041",
   "metadata": {},
   "source": [
    "### 5.2 ExampleÂ 2 â€“ Live Audio & Video Agent *(optional)*\n",
    "This example needs microphone/camera access and valid Gemini Live API credentials, so feel free to skip in a binder environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2445d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and configure credentials first\n",
    "# from genai_processors.core import audio_io, video, live_model, streams\n",
    "# cam_mic   = video.VideoIn() + audio_io.PyAudioIn()\n",
    "# speaker   = audio_io.PyAudioOut()\n",
    "# live_llm  = live_model.LiveProcessor(model_name='gemini-2.0-pro')\n",
    "# live_agent = cam_mic + live_llm + speaker\n",
    "# await live_agent(streams.endless_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd267cf6",
   "metadata": {},
   "source": [
    "### 5.3 ExampleÂ 3 â€“ Emitting Custom JSON Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_processors import processor, content_api, streams\n",
    "\n",
    "@processor.PartProcessor\n",
    "async def sentiment_json(part):\n",
    "    if part.text:\n",
    "        score = 1 if 'ðŸ™‚' in part.text else -1\n",
    "        yield content_api.from_json({'sentiment': score})\n",
    "\n",
    "async def demo_json():\n",
    "    pipe = sentiment_json\n",
    "    inp = content_api.from_text('I love async programming ðŸ™‚')\n",
    "    async for part in pipe(inp):\n",
    "        print(part.json())\n",
    "\n",
    "await demo_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a51c4",
   "metadata": {},
   "source": [
    "## 6. Testing Your Processors<a id='Testing-Your-Processors'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running `pytest`, save this as test_uppercase.py\n",
    "import pytest, asyncio\n",
    "from genai_processors import content_api, streams\n",
    "from tutorial_notebook import uppercase\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_uppercase():\n",
    "    s = streams.stream_content(['abc'])\n",
    "    out = [p async for p in uppercase(s)]\n",
    "    assert out[0].text == 'ABC'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0f8313",
   "metadata": {},
   "source": [
    "## 7. Performance Tips<a id='Performance-Tips'></a>\n",
    "- **Chunk large inputs:** emit smaller `ProcessorPart`s so downstream stages start sooner.\n",
    "- **Avoid global state:** processors should be pure; easier to parallelise.\n",
    "- **Use `LiveProcessor`** for streaming tokenâ€‘level output.\n",
    "- **Profile:** `asyncio.run(main(), debug=True)` activates slowâ€‘task logging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59c276",
   "metadata": {},
   "source": [
    "## 8. Deployment Snippet<a id='Deployment-Snippet'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<'DOCKER' > Dockerfile\n",
    "FROM python:3.12-slim\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "RUN pip install --no-cache-dir genai-processors uvicorn\n",
    "CMD [\"python\", \"-m\", \"uvicorn\", \"app:router\", \"--host=0.0.0.0\", \"--port=8080\"]\n",
    "DOCKER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb7c1c8",
   "metadata": {},
   "source": [
    "## 9. Next Steps & Exercises<a id='Next-Steps-&-Exercises'></a>\n",
    "1. **Write a LowercaseTextProcessor** mirroring the uppercase example.\n",
    "2. **Parallel audio transcription + sentiment analysis:** Use `//` to fanâ€‘out.\n",
    "3. **Error handling challenge:** Make a processor that retries Gemini calls with exponential backoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ef431",
   "metadata": {},
   "source": [
    "## 11. Challenge Solutions\n",
    "Below we implement three requested features with runnable code and explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21616fff",
   "metadata": {},
   "source": [
    "### 11.1Â LowercaseTextProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25efb690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_processors import processor\n",
    "\n",
    "@processor.PartProcessor\n",
    "async def lowercase(part):\n",
    "    \"\"\"Converts any text content to lowerâ€‘case.\"\"\"\n",
    "    if part.text:\n",
    "        part.text = part.text.lower()\n",
    "    return part\n",
    "\n",
    "from genai_processors import content_api\n",
    "\n",
    "async def _demo_lower():\n",
    "    inp = content_api.from_text('HELLO WORLD!')\n",
    "    out = [p async for p in lowercase(inp)]\n",
    "    print(out[0].text)\n",
    "\n",
    "await _demo_lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09303619",
   "metadata": {},
   "source": [
    "### 11.2Â Parallel Audio Transcription + Sentiment Analysis\n",
    "Uses the `//` operator to fan out an incoming audio stream to two processors in parallel: a stub *Transcriber* and the earlier `sentiment_json` processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_processors import processor, streams, content_api\n",
    "import random, asyncio\n",
    "\n",
    "@processor.PartProcessor\n",
    "async def transcribe_stub(part):\n",
    "    if hasattr(part, 'audio'):\n",
    "        txt = f\"<transcribed_{random.randint(0,999)}> ðŸ™‚\"  # add emoji for sentiment demo\n",
    "        yield content_api.from_text(txt)\n",
    "\n",
    "@processor.PartProcessor\n",
    "async def sentiment_json(part):\n",
    "    if part.text:\n",
    "        score = 1 if 'ðŸ™‚' in part.text else -1\n",
    "        yield content_api.from_json({'sentiment': score})\n",
    "\n",
    "# Fanâ€‘out pipeline\n",
    "fanout = transcribe_stub // sentiment_json\n",
    "\n",
    "async def _demo_fanout():\n",
    "    class DummyAudioPart:\n",
    "        def __init__(self):\n",
    "            self.audio = b'\\x00\\x01'\n",
    "    inp_stream = streams.stream_parts([DummyAudioPart()])\n",
    "    async for part in fanout(inp_stream):\n",
    "        if hasattr(part, 'text'):\n",
    "            print('Text:', part.text)\n",
    "        else:\n",
    "            print('Sentiment JSON:', part.json())\n",
    "\n",
    "await _demo_fanout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96b6ca",
   "metadata": {},
   "source": [
    "### 11.3Â Robust Gemini Calls with Exponential Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aefd4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools, random, asyncio\n",
    "from genai_processors.core import genai_model\n",
    "from genai_processors import content_api\n",
    "\n",
    "def retry_async(max_attempts=5, initial_delay=1.0, factor=2.0):\n",
    "    \"\"\"Exponential backoff decorator for async callables.\"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        async def wrapper(*args, **kwargs):\n",
    "            delay = initial_delay\n",
    "            for attempt in range(1, max_attempts + 1):\n",
    "                try:\n",
    "                    return await func(*args, **kwargs)\n",
    "                except Exception as exc:\n",
    "                    if attempt == max_attempts:\n",
    "                        raise\n",
    "                    jitter = random.uniform(0, delay * 0.1)\n",
    "                    await asyncio.sleep(delay + jitter)\n",
    "                    delay *= factor\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "class SafeGenai(genai_model.GenaiModel):\n",
    "    @retry_async(max_attempts=4, initial_delay=1.5)\n",
    "    async def __call__(self, *args, **kwargs):\n",
    "        return await super().__call__(*args, **kwargs)\n",
    "\n",
    "async def _demo_safe():\n",
    "    model = SafeGenai(model_name='gemini-2.0-pro')\n",
    "    inp = content_api.from_text('Generate a haiku about async backoff')\n",
    "    async for part in model(inp):\n",
    "        if part.text:\n",
    "            print(part.text)\n",
    "\n",
    "await _demo_safe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93267a",
   "metadata": {},
   "source": [
    "## 10. Resources<a id='Resources'></a>\n",
    "\n",
    "| Resource | Link |\n",
    "|----------|------|\n",
    "| Blog Announcement | https://developers.googleblog.com/en/genai-processors/ |\n",
    "| GitHub Repo | https://github.com/google-gemini/genai-processors |\n",
    "| Live API Docs | https://ai.google.dev/gemini-api/docs/live |\n",
    "| AsyncIO Primer | https://medium.com/%40Shrishml/all-about-python-asyncio-ca1f5a8974b0 |\n",
    "| uvloop & async performance | https://ai.plainenglish.io/the-role-of-uvloop-in-async-python-for-ai-and-machine-learning-pipelines-c7fec45a4966 |\n",
    "| OpenTools AI overview | https://opentools.ai/news/google-deepmind-launches-genai-processors-revolutionizing-ai-development |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f4b6d",
   "metadata": {},
   "source": [
    "## 5.4 ExampleÂ 4Â â€“Â LowercaseTextProcessor\n",
    "A mirror image of our earlier uppercase demo, transforming incoming text streams to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ef721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_processors import processor, content_api, streams\n",
    "\n",
    "@processor.PartProcessor\n",
    "async def lowercase(part):\n",
    "    if part.text:\n",
    "        part.text = part.text.lower()\n",
    "    return part\n",
    "\n",
    "async def demo_lower():\n",
    "    pipe = lowercase\n",
    "    inp = content_api.from_text('HELLO THERE!')\n",
    "    async for part in pipe(inp):\n",
    "        print(part.text)\n",
    "\n",
    "await demo_lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ac116",
   "metadata": {},
   "source": [
    "## 5.5 ExampleÂ 5Â â€“Â Parallel Audio Transcription **and** Sentiment Analysis\n",
    "We fanâ€‘out the same audio stream to two processors using `//`. One branch performs *speechâ€‘toâ€‘text* while the other runs a quick sentiment analysis on the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7953e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_processors import processor, content_api, streams\n",
    "from genai_processors.core import genai_model, audio_io\n",
    "\n",
    "# --- Branch A: Speechâ€‘toâ€‘Text ---\n",
    "@processor.PartProcessor\n",
    "async def dummy_transcribe(part):\n",
    "    \"\"\"Placeholder transcription; replace with real model/API.\"\"\"\n",
    "    if part.audio:\n",
    "        yield content_api.from_text('(transcription) hello world')\n",
    "\n",
    "# --- Branch B: Sentiment ---\n",
    "@processor.PartProcessor\n",
    "async def quick_sentiment(part):\n",
    "    if part.text:\n",
    "        score = 1 if 'hello' in part.text.lower() else -1\n",
    "        part.attrs['sentiment'] = score\n",
    "        return part\n",
    "\n",
    "# Fanâ€‘out using //, then merge outputs and send to Gemini for a summary\n",
    "fanout = dummy_transcribe // quick_sentiment\n",
    "summariser = genai_model.GenaiModel(model_name='gemini-2.0-flash')\n",
    "\n",
    "async def run_audio_pipeline():\n",
    "    # This uses a short WAV clip packaged with the examples; swap with live mic in real use.\n",
    "    audio_part = content_api.from_binary(open('/mnt/data/sample.wav','rb').read(), mime_type='audio/wav')\n",
    "    pipeline = fanout + summariser\n",
    "    async for p in pipeline(streams.stream_content([audio_part])):\n",
    "        if p.text:\n",
    "            print(p.text)\n",
    "\n",
    "# await run_audio_pipeline()  # Uncomment after adding a sample.wav and API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c61d64",
   "metadata": {},
   "source": [
    "## 5.6 ExampleÂ 6Â â€“Â Retrying Gemini Calls with Exponential Backoff\n",
    "Network hiccups? Rate limits? Implement a resilient processor that automatically retries failed Gemini invocations with exponential backoff + jitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2b2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, random, logging\n",
    "from genai_processors.core import genai_model\n",
    "from genai_processors import processor\n",
    "\n",
    "logger = logging.getLogger('retry_demo')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "@processor.Processor\n",
    "class ResilientGemini(genai_model.GenaiModel):\n",
    "    def __init__(self, model_name: str, max_retries: int = 5):\n",
    "        super().__init__(model_name=model_name)\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    async def __call__(self, inp_stream):\n",
    "        delay = 1.0  # seconds\n",
    "        for attempt in range(1, self.max_retries + 1):\n",
    "            try:\n",
    "                async for part in super().__call__(inp_stream):\n",
    "                    yield part\n",
    "                return  # Success â†’ exit\n",
    "            except Exception as exc:\n",
    "                logger.warning('Attempt %d failed: %s', attempt, exc)\n",
    "                if attempt == self.max_retries:\n",
    "                    raise\n",
    "                jitter = random.uniform(0, 0.5)\n",
    "                await asyncio.sleep(delay + jitter)\n",
    "                delay = min(delay * 2, 30)  # cap at 30â€¯s\n",
    "\n",
    "# Demo stub (requires API key):\n",
    "async def demo_resilient():\n",
    "    from genai_processors import content_api\n",
    "    inp = content_api.from_text('Hello with retry!')\n",
    "    model = ResilientGemini('gemini-2.0-pro')\n",
    "    async for p in model(streams.stream_content([inp])):\n",
    "        if p.text:\n",
    "            print(p.text)\n",
    "\n",
    "# await demo_resilient()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
