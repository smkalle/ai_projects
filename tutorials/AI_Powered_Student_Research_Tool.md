# ğŸš€ Building Your AIâ€‘Powered Student Research Tool with Gemini & LangGraph

This tutorial guides you through setting up and understanding a **deepâ€‘research** tool that leverages the **Gemini API** and **LangGraph** to create AI agents capable of performing comprehensive web researchâ€”complete with citations.  
A **React** frontend handles user interaction, while a **LangGraphâ€‘powered Python** backend does the heavy lifting.

Youâ€™ll be working with an **openâ€‘source quickâ€‘start project** that demonstrates:

* Dynamic searchâ€‘query generation  
* Iterative refinement of results through reflection  
* Automatic identification of knowledge gaps to provide thorough answers  

---

## ğŸ› ï¸ Prerequisites

| Requirement | Purpose |
|-------------|---------|
| **Node.jsÂ (â‰¥Â v18)** & **npm/yarn/pnpm** | Frontâ€‘end development |
| **PythonÂ (â‰¥Â 3.8)** | Backâ€‘end development |
| **Git** | Clone the repository |
| **`GEMINI_API_KEY`** | Access the Gemini API (get one from **[GoogleÂ AIÂ Studio](https://aistudio.google.com/getting-started)**) |
| **(Optional) SearchÂ APIÂ keys** | Tavily or GoogleÂ Search support |
|Â &nbsp;&nbsp;â€¢Â `TAVILY_API_KEY` | **[Tavily](https://tavily.com/)** search |
|Â &nbsp;&nbsp;â€¢Â `GOOGLE_SEARCH_API_KEY` + `GOOGLE_CSE_ID` | GoogleÂ ProgrammableÂ Search |

---

## ğŸ“ StepÂ 1Â â€“ ProjectÂ Setup

1. **Clone the repository**

   ```bash
   git clone https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart.git
   cd gemini-fullstack-langgraph-quickstart
   ```

2. **Configure the backend environment**

   ```bash
   cd backend
   cp .env.example .env
   ```

   Edit **`.env`** and add your keys:

   ```dotenv
   GEMINI_API_KEY="YOUR_ACTUAL_API_KEY"

   # Tavily (optional)
   TAVILY_API_KEY="YOUR_TAVILY_API_KEY"

   # LangSmith (optional but recommended)
   LANGCHAIN_TRACING_V2="true"
   LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
   LANGCHAIN_API_KEY="YOUR_LANGSMITH_API_KEY"
   LANGCHAIN_PROJECT="gemini-fullstack-langgraph-quickstart"
   ```

   > **Tip:** Confirm which search API is used inÂ `backend/app/agent.py` and set only the variables you need.

3. **Install backend dependencies**

   ```bash
   pip install .
   ```

4. **Install frontend dependencies**

   ```bash
   cd ../frontend
   npm install
   ```

---

## ğŸƒâ€â™‚ï¸ StepÂ 2Â â€“ RunningÂ theÂ DevelopmentÂ Environment

From the project root you can launch both services at once:

```bash
make dev
```

**Or run them in separate terminals:**

* **Backend (LangGraph)**  
  ```bash
  cd backend
  langgraph dev        # defaultÂ â†’ http://localhost:8000
  ```

* **Frontend (React +Â Vite)**  
  ```bash
  cd frontend
  npm run dev          # defaultÂ â†’ http://localhost:5173
  ```

---

## ğŸ›ï¸ StepÂ 3Â â€“ UnderstandingÂ theÂ Architecture

### FrontendÂ (React)

* Presents a form for research queries  
* Shows the AIâ€‘generated answer **with citations**

### BackendÂ (PythonÂ +Â LangGraph)

* Uses **Gemini** models for language reasoning  
* Builds a **stateful, multiâ€‘agent graph** with **LangGraph**

### How the Agents Collaborate

1. **QueryÂ GenerationÂ Agent** â†’ crafts search queries  
2. **WebÂ SearchingÂ Agent** â†’ calls Tavily/Google Search  
3. **ResultÂ AnalysisÂ Agent** â†’ extracts facts & detects gaps  
4. **QueryÂ RefinementÂ Agent** â†’ improves queries when needed  
5. **AnswerÂ CompilationÂ Agent** â†’ synthesises a cited answer  
6. **SupervisorÂ Agent** â†’ routes control between the above agents

---

## âš™ï¸ StepÂ 4Â â€“ DeepÂ DiveÂ intoÂ theÂ BackendÂ (Conceptual)

```python
# backend/app/workflow.py  (illustrative)
from langgraph.graph import StateGraph, TypedDict
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import List

class ResearchState(TypedDict):
    user_query: str
    search_queries: List[str]
    # â€¦ other fields

workflow = StateGraph(ResearchState)

# â€” Nodes -----------------------------------------------------------
workflow.add_node("query_generator", generate_queries_node)
workflow.add_node("web_searcher", search_web_node)
workflow.add_node("query_refiner", refine_queries_node)
workflow.add_node("answer_compiler", compile_answer_node)

# â€” Edges -----------------------------------------------------------
workflow.set_entry_point("query_generator")

workflow.add_edge("query_generator", "web_searcher")

# Decide dynamically whether to refine or compile
workflow.add_conditional_edges(
    "web_searcher",
    should_refine_or_compile,   # returns "refine" / "compile" / "__end__"
    {
        "refine": "query_refiner",
        "compile": "answer_compiler",
        "__end__": "__end__"
    }
)

app = workflow.compile()
```

### GeminiÂ Integration

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest")

prompt = ChatPromptTemplate.from_template(
    "Generate three diverse search queries for the topic: '{topic}'"
)

query_generation_chain = prompt | llm
response = query_generation_chain.invoke({"topic": user_query})
```

---

## ğŸ”— StepÂ 5Â â€“ FrontendÂ IntegrationÂ (Conceptual)

The backend is served with **LangServe**, exposing `/research_agent/invoke`.

```tsx
// frontend/src/ResearchComponent.tsx (excerpt)
const apiUrl = "http://localhost:8000/research_agent/invoke";

const response = await fetch(apiUrl, {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({ input: { user_query: query } })
});
const data = await response.json();
setResult(data.output);
```

---

## ğŸ§ª StepÂ 6Â â€“ Using &Â Testing theÂ Tool

1. Ensure both servers are running (`make dev`).  
2. Open **http://localhost:5173** in your browser.  
3. Enter a query like **â€œlatest advancements in quantum computingâ€**.  
4. Wait for processing and review the answer & citations.

---

## ğŸ“¦ StepÂ 7Â â€“ DeploymentÂ (OptionalÂ viaÂ Docker)

```bash
# Build the image
docker build -t gemini-fullstack-langgraph -f Dockerfile .

# Launch with DockerÂ Compose (requires populated backend/.env)
docker-compose up --build
```

The app will be available on the port specified in **docker-compose.yml** (e.g.,Â `http://localhost:8080`).

---

## ğŸ”§ StepÂ 8Â â€“ CustomizationÂ Ideas

* **Modify agents** â€“ fineâ€‘tune prompts or logic  
* **Add tools** â€“ PDF readers, ArXiv API, database connectors  
* **Bring your data** â€“ academic DBs, internal knowledge bases  
* **Polish the UI** â€“ richer result views, feedback widgets  

---

## âš ï¸ ImportantÂ Considerations

* **Gemini API quotas** â€“ monitor limits in GoogleÂ CloudÂ Console.  
* **Search API costs** â€“ Tavily / Google Programmable Search have their own pricing.  
* **Monitoring** â€“ enable **LangSmith** via environment variables for tracing.

---

## ğŸ‰ Conclusion

You now have a working scaffold for an AIâ€‘powered deepâ€‘research assistant.  
Build on itâ€”add new agents, integrate fresh data sources, and craft specialised tools for students and researchers.

---

## ğŸ“š KeyÂ Resources

* **GitHub repo** â€“ `gemini-fullstack-langgraph-quickstart`  
* **LangGraph docs** â€“ https://python.langgraph.org  
* **GoogleÂ AIÂ Studio** â€“ https://aistudio.google.com  
* **LangSmith** â€“ https://smith.langchain.com  
* **GoogleÂ CloudÂ ConsoleÂ â€“Â Quotas** â€“ https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/quotas
