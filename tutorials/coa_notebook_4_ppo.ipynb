{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Agents Part 4: PPO - The 18-20% Performance Breakthrough\n",
    "\n",
    "**Time**: 60 minutes | **Level**: Advanced | **Prerequisite**: Parts 1-3\n",
    "\n",
    "## The Final Frontier üöÄ\n",
    "\n",
    "Your AFM from SFT achieves 45% on GAIA. Good, but not great.\n",
    "\n",
    "**With PPO (Proximal Policy Optimization)**: 55.3% on GAIA! üéØ\n",
    "\n",
    "That's an **18-20% relative improvement**. This is the secret sauce that makes CoA beat all other multi-agent systems.\n",
    "\n",
    "Let's build it from first principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Why Does PPO Help AFMs?\n",
    "\n",
    "Let's understand the problem SFT leaves unsolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_sft_limitations():\n",
    "    \"\"\"Show why SFT alone isn't enough for AFM\"\"\"\n",
    "    \n",
    "    print(\"ü§î SFT LIMITATIONS FOR AFM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    problems = [\n",
    "        {\n",
    "            \"issue\": \"Imitation Learning Ceiling\",\n",
    "            \"description\": \"AFM can only be as good as training trajectories\",\n",
    "            \"example\": \"If all trajectories score 70%, AFM caps at 70%\"\n",
    "        },\n",
    "        {\n",
    "            \"issue\": \"No Exploration\",\n",
    "            \"description\": \"SFT just copies, never discovers new strategies\",\n",
    "            \"example\": \"Never learns better agent combinations\"\n",
    "        },\n",
    "        {\n",
    "            \"issue\": \"Reward Misalignment\",\n",
    "            \"description\": \"Optimizes for copying, not for actual task success\",\n",
    "            \"example\": \"Perfect imitation ‚â† perfect performance\"\n",
    "        },\n",
    "        {\n",
    "            \"issue\": \"Distribution Mismatch\",\n",
    "            \"description\": \"Training data ‚â† real world deployment\",\n",
    "            \"example\": \"Works on seen tasks, fails on new ones\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, problem in enumerate(problems, 1):\n",
    "        print(f\"\\n{i}. {problem['issue']}\")\n",
    "        print(f\"   Problem: {problem['description']}\")\n",
    "        print(f\"   Example: {problem['example']}\")\n",
    "    \n",
    "    print(\"\\nüí° THE SOLUTION: Reinforcement Learning\")\n",
    "    print(\"   Let the AFM explore and improve beyond training data!\")\n",
    "    print(\"   Reward actual task performance, not just imitation\")\n",
    "\n",
    "demonstrate_sft_limitations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: PPO in 5 Minutes\n",
    "\n",
    "Quick crash course on PPO before we apply it to AFMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class SimplePPO:\n",
    "    \"\"\"PPO explained in the simplest terms\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epsilon = 0.2  # Clipping parameter\n",
    "        self.policy_old = None\n",
    "        self.policy_new = None\n",
    "    \n",
    "    def explain_ppo(self):\n",
    "        \"\"\"Explain PPO conceptually\"\"\"\n",
    "        \n",
    "        print(\"üéØ PPO IN 5 MINUTES\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        print(\"\\n1Ô∏è‚É£ THE GOAL:\")\n",
    "        print(\"   Improve policy (AFM) using rewards from environment\")\n",
    "        print(\"   Don't change too much at once (stability)\")\n",
    "        \n",
    "        print(\"\\n2Ô∏è‚É£ THE PROCESS:\")\n",
    "        print(\"   üìä Collect experience: (state, action, reward)\")\n",
    "        print(\"   üéØ Estimate advantages: how much better is this action?\")\n",
    "        print(\"   üìà Update policy: move toward better actions\")\n",
    "        print(\"   ‚úÇÔ∏è  Clip updates: don't change too drastically\")\n",
    "        \n",
    "        print(\"\\n3Ô∏è‚É£ THE MATH (simplified):\")\n",
    "        print(\"   ratio = new_policy(action) / old_policy(action)\")\n",
    "        print(\"   clipped_ratio = clip(ratio, 1-Œµ, 1+Œµ)\")\n",
    "        print(\"   loss = min(ratio * advantage, clipped_ratio * advantage)\")\n",
    "        \n",
    "        print(\"\\n4Ô∏è‚É£ FOR AFM:\")\n",
    "        print(\"   State: Task description\")\n",
    "        print(\"   Action: Generated agent response\")\n",
    "        print(\"   Reward: How well did it solve the task?\")\n",
    "        \n",
    "        print(\"\\n‚ú® RESULT: AFM learns to generate better agent chains!\")\n",
    "    \n",
    "    def simulate_update(self, advantages):\n",
    "        \"\"\"Simulate a PPO update step\"\"\"\n",
    "        \n",
    "        print(\"\\nüîÑ PPO UPDATE SIMULATION\")\n",
    "        print(\"-\"*30)\n",
    "        \n",
    "        # Simulate policy probabilities\n",
    "        old_probs = np.random.rand(5)\n",
    "        new_probs = old_probs + np.random.normal(0, 0.1, 5)\n",
    "        new_probs = np.maximum(new_probs, 0.001)  # Avoid zeros\n",
    "        \n",
    "        ratios = new_probs / old_probs\n",
    "        clipped_ratios = np.clip(ratios, 1 - self.epsilon, 1 + self.epsilon)\n",
    "        \n",
    "        print(\"Example PPO update:\")\n",
    "        for i in range(5):\n",
    "            print(f\"  Action {i}: ratio={ratios[i]:.3f}, \"\n",
    "                  f\"clipped={clipped_ratios[i]:.3f}, \"\n",
    "                  f\"advantage={advantages[i]:.2f}\")\n",
    "        \n",
    "        return clipped_ratios\n",
    "\n",
    "# Demonstrate PPO\n",
    "ppo = SimplePPO()\n",
    "ppo.explain_ppo()\n",
    "\n",
    "# Simulate update\n",
    "sample_advantages = np.array([0.5, -0.2, 0.8, -0.1, 0.3])\n",
    "ppo.simulate_update(sample_advantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Design Reward Function for AFM\n",
    "\n",
    "The reward function determines what the AFM learns to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFMRewardFunction:\n",
    "    \"\"\"Reward function for Agent Foundation Model training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = {\n",
    "            'task_completion': 0.4,\n",
    "            'agent_coherence': 0.2,\n",
    "            'output_quality': 0.2,\n",
    "            'efficiency': 0.1,\n",
    "            'user_preference': 0.1\n",
    "        }\n",
    "    \n",
    "    def evaluate_task_completion(self, task, output):\n",
    "        \"\"\"Did the AFM actually solve the task?\"\"\"\n",
    "        # Simulate task completion check\n",
    "        if \"error\" in output.lower() or \"failed\" in output.lower():\n",
    "            return 0.0\n",
    "        elif \"complete\" in output.lower() or \"success\" in output.lower():\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.5\n",
    "    \n",
    "    def evaluate_agent_coherence(self, output):\n",
    "        \"\"\"Do the simulated agents work well together?\"\"\"\n",
    "        # Check for agent markers\n",
    "        agent_markers = ['[Planner]', '[Coder]', '[Reviewer]']\n",
    "        present_agents = sum(1 for marker in agent_markers if marker in output)\n",
    "        \n",
    "        # Reward having all agents participate\n",
    "        coherence_score = present_agents / len(agent_markers)\n",
    "        \n",
    "        # Bonus for logical flow\n",
    "        if '[Planner]' in output and '[Coder]' in output:\n",
    "            plan_pos = output.find('[Planner]')\n",
    "            code_pos = output.find('[Coder]')\n",
    "            if plan_pos < code_pos:  # Logical order\n",
    "                coherence_score += 0.2\n",
    "        \n",
    "        return min(coherence_score, 1.0)\n",
    "    \n",
    "    def evaluate_output_quality(self, output):\n",
    "        \"\"\"Is the output high quality?\"\"\"\n",
    "        quality_indicators = [\n",
    "            'detailed', 'comprehensive', 'thorough',\n",
    "            'optimized', 'robust', 'scalable'\n",
    "        ]\n",
    "        \n",
    "        negative_indicators = [\n",
    "            'unclear', 'incomplete', 'buggy',\n",
    "            'broken', 'inefficient'\n",
    "        ]\n",
    "        \n",
    "        quality_score = 0.5  # Baseline\n",
    "        \n",
    "        for indicator in quality_indicators:\n",
    "            if indicator in output.lower():\n",
    "                quality_score += 0.1\n",
    "        \n",
    "        for indicator in negative_indicators:\n",
    "            if indicator in output.lower():\n",
    "                quality_score -= 0.1\n",
    "        \n",
    "        return max(0, min(quality_score, 1.0))\n",
    "    \n",
    "    def evaluate_efficiency(self, output):\n",
    "        \"\"\"Is the solution efficient?\"\"\"\n",
    "        # Longer isn't always better, but too short is bad\n",
    "        length = len(output.split())\n",
    "        \n",
    "        if length < 20:\n",
    "            return 0.2  # Too brief\n",
    "        elif 20 <= length <= 100:\n",
    "            return 1.0  # Just right\n",
    "        elif 100 < length <= 200:\n",
    "            return 0.8  # A bit verbose\n",
    "        else:\n",
    "            return 0.5  # Too verbose\n",
    "    \n",
    "    def evaluate_user_preference(self, output):\n",
    "        \"\"\"Would users prefer this response?\"\"\"\n",
    "        # Simulate user preference (in reality, this could be human feedback)\n",
    "        positive_words = ['easy', 'clear', 'helpful', 'practical']\n",
    "        score = 0.5\n",
    "        \n",
    "        for word in positive_words:\n",
    "            if word in output.lower():\n",
    "                score += 0.1\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def calculate_reward(self, task, output):\n",
    "        \"\"\"Calculate total reward for AFM output\"\"\"\n",
    "        \n",
    "        components = {\n",
    "            'task_completion': self.evaluate_task_completion(task, output),\n",
    "            'agent_coherence': self.evaluate_agent_coherence(output),\n",
    "            'output_quality': self.evaluate_output_quality(output),\n",
    "            'efficiency': self.evaluate_efficiency(output),\n",
    "            'user_preference': self.evaluate_user_preference(output)\n",
    "        }\n",
    "        \n",
    "        # Weighted sum\n",
    "        total_reward = sum(self.weights[k] * components[k] for k in components)\n",
    "        \n",
    "        return total_reward, components\n",
    "\n",
    "# Test the reward function\n",
    "reward_fn = AFMRewardFunction()\n",
    "\n",
    "# Example AFM outputs\n",
    "good_output = \"\"\"[Planner]: Breaking down the task into clear steps...\n",
    "[Coder]: Implementing a robust and scalable solution...\n",
    "[Reviewer]: Code review complete. The solution is comprehensive and ready.\"\"\"\n",
    "\n",
    "bad_output = \"\"\"Error: I don't understand this task. The solution is unclear and broken.\"\"\"\n",
    "\n",
    "# Calculate rewards\n",
    "print(\"üèÜ REWARD FUNCTION EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, (name, output) in enumerate([(\"Good Output\", good_output), (\"Bad Output\", bad_output)], 1):\n",
    "    reward, components = reward_fn.calculate_reward(\"Build API\", output)\n",
    "    \n",
    "    print(f\"\\n{i}. {name}:\")\n",
    "    print(f\"   Total Reward: {reward:.3f}\")\n",
    "    \n",
    "    for component, score in components.items():\n",
    "        bar = '‚ñà' * int(score * 10)\n",
    "        print(f\"   {component:15}: {bar:10} {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: PPO Training Loop for AFM\n",
    "\n",
    "Put it all together: generate, evaluate, update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFMPPO:\n",
    "    \"\"\"PPO training for Agent Foundation Model\"\"\"\n",
    "    \n",
    "    def __init__(self, afm_model, reward_function):\n",
    "        self.afm = afm_model\n",
    "        self.reward_fn = reward_function\n",
    "        \n",
    "        # PPO hyperparameters\n",
    "        self.clip_epsilon = 0.2\n",
    "        self.learning_rate = 3e-4\n",
    "        self.ppo_epochs = 4\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        \n",
    "        # Training tracking\n",
    "        self.episode_rewards = []\n",
    "        self.performance_history = []\n",
    "    \n",
    "    def generate_response(self, task):\n",
    "        \"\"\"Generate AFM response (simulated)\"\"\"\n",
    "        # In reality, this would be afm.generate(task)\n",
    "        responses = [\n",
    "            f\"[Planner]: Analyzing '{task}' and creating detailed plan...\\n[Coder]: Implementing solution with best practices...\\n[Reviewer]: Review complete, solution approved.\",\n",
    "            f\"[Planner]: Breaking down '{task}' into steps...\\n[Coder]: Writing basic implementation...\\n[Reviewer]: Looks okay.\",\n",
    "            f\"Error: Could not process '{task}' properly. Solution incomplete.\",\n",
    "            f\"[Planner]: Planning for '{task}'...\\n[Coder]: def solution(): return 'done'\\n[Reviewer]: Comprehensive solution ready.\"\n",
    "        ]\n",
    "        \n",
    "        return random.choice(responses)\n",
    "    \n",
    "    def collect_trajectories(self, tasks, num_samples=100):\n",
    "        \"\"\"Collect experience trajectories\"\"\"\n",
    "        trajectories = []\n",
    "        \n",
    "        print(\"üìä Collecting PPO trajectories...\")\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            task = random.choice(tasks)\n",
    "            \n",
    "            # Generate response\n",
    "            response = self.generate_response(task)\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward, components = self.reward_fn.calculate_reward(task, response)\n",
    "            \n",
    "            # Store trajectory\n",
    "            trajectories.append({\n",
    "                'task': task,\n",
    "                'response': response,\n",
    "                'reward': reward,\n",
    "                'components': components\n",
    "            })\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                print(f\"  Collected {i+1}/{num_samples} trajectories\")\n",
    "        \n",
    "        avg_reward = np.mean([t['reward'] for t in trajectories])\n",
    "        print(f\"‚úÖ Average reward: {avg_reward:.3f}\")\n",
    "        \n",
    "        return trajectories\n",
    "    \n",
    "    def calculate_advantages(self, trajectories):\n",
    "        \"\"\"Calculate advantages for PPO update\"\"\"\n",
    "        rewards = [t['reward'] for t in trajectories]\n",
    "        \n",
    "        # Simple advantage calculation (rewards - baseline)\n",
    "        baseline = np.mean(rewards)\n",
    "        advantages = [r - baseline for r in rewards]\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = np.array(advantages)\n",
    "        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
    "        \n",
    "        return advantages\n",
    "    \n",
    "    def ppo_update(self, trajectories, advantages):\n",
    "        \"\"\"Perform PPO update (simplified)\"\"\"\n",
    "        \n",
    "        print(\"üîÑ PPO Update Step\")\n",
    "        \n",
    "        # In reality, this would be gradient descent on the PPO loss\n",
    "        # For demo, we'll simulate the update\n",
    "        \n",
    "        positive_advantages = sum(1 for a in advantages if a > 0)\n",
    "        negative_advantages = len(advantages) - positive_advantages\n",
    "        \n",
    "        print(f\"   Positive advantages: {positive_advantages}/{len(advantages)}\")\n",
    "        print(f\"   Policy update strength: {np.mean(np.abs(advantages)):.3f}\")\n",
    "        \n",
    "        # Simulate model improvement\n",
    "        improvement = np.random.uniform(0.01, 0.05)\n",
    "        print(f\"   Simulated improvement: +{improvement:.3f}\")\n",
    "        \n",
    "        return improvement\n",
    "    \n",
    "    def train_epoch(self, tasks):\n",
    "        \"\"\"One epoch of PPO training\"\"\"\n",
    "        \n",
    "        # Collect trajectories\n",
    "        trajectories = self.collect_trajectories(tasks, num_samples=50)\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = self.calculate_advantages(trajectories)\n",
    "        \n",
    "        # PPO updates\n",
    "        total_improvement = 0\n",
    "        for epoch in range(self.ppo_epochs):\n",
    "            improvement = self.ppo_update(trajectories, advantages)\n",
    "            total_improvement += improvement\n",
    "        \n",
    "        # Track performance\n",
    "        avg_reward = np.mean([t['reward'] for t in trajectories])\n",
    "        self.episode_rewards.append(avg_reward)\n",
    "        \n",
    "        return avg_reward, total_improvement\n",
    "\n",
    "# Initialize PPO training\n",
    "print(\"üöÄ INITIALIZING PPO TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Mock AFM model\n",
    "class MockAFM:\n",
    "    def __init__(self):\n",
    "        self.performance = 0.45  # Starting at 45%\n",
    "\n",
    "afm_model = MockAFM()\n",
    "ppo_trainer = AFMPPO(afm_model, reward_fn)\n",
    "\n",
    "print(f\"AFM initialized with {afm_model.performance:.1%} baseline performance\")\n",
    "print(f\"PPO config: lr={ppo_trainer.learning_rate}, clip_Œµ={ppo_trainer.clip_epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run PPO Training - Watch the Magic!\n",
    "\n",
    "Train the AFM and see performance improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training tasks\n",
    "training_tasks = [\n",
    "    \"Build REST API for user management\",\n",
    "    \"Implement authentication system\",\n",
    "    \"Create database schema\",\n",
    "    \"Optimize query performance\",\n",
    "    \"Add error handling\",\n",
    "    \"Write unit tests\",\n",
    "    \"Deploy to production\",\n",
    "    \"Monitor system health\"\n",
    "]\n",
    "\n",
    "def run_ppo_training(trainer, tasks, epochs=5):\n",
    "    \"\"\"Run complete PPO training\"\"\"\n",
    "    \n",
    "    print(\"üéØ STARTING PPO TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Training epochs: {epochs}\")\n",
    "    print(f\"Tasks: {len(tasks)}\")\n",
    "    print(f\"Target: 55.3% GAIA performance\\n\")\n",
    "    \n",
    "    performance_history = []\n",
    "    baseline_performance = 0.45  # 45% baseline\n",
    "    current_performance = baseline_performance\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nüìç EPOCH {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Train one epoch\n",
    "        avg_reward, improvement = trainer.train_epoch(tasks)\n",
    "        \n",
    "        # Update performance (simplified)\n",
    "        current_performance += improvement * 0.1  # Scale improvement\n",
    "        current_performance = min(current_performance, 0.60)  # Cap at 60%\n",
    "        \n",
    "        performance_history.append(current_performance)\n",
    "        \n",
    "        # Show progress\n",
    "        improvement_pct = (current_performance - baseline_performance) / baseline_performance * 100\n",
    "        \n",
    "        print(f\"\\nüìä Epoch {epoch + 1} Results:\")\n",
    "        print(f\"   Average reward: {avg_reward:.3f}\")\n",
    "        print(f\"   Performance: {current_performance:.1%}\")\n",
    "        print(f\"   Improvement: +{improvement_pct:.1f}% from baseline\")\n",
    "        \n",
    "        # Progress bar\n",
    "        progress = int((epoch + 1) / epochs * 20)\n",
    "        bar = '‚ñà' * progress + '‚ñë' * (20 - progress)\n",
    "        print(f\"   Progress: {bar} {((epoch + 1) / epochs * 100):.0f}%\")\n",
    "    \n",
    "    # Final results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ PPO TRAINING COMPLETE!\")\n",
    "    print(f\"\\nüìà Performance Journey:\")\n",
    "    print(f\"   Baseline (SFT): {baseline_performance:.1%}\")\n",
    "    print(f\"   Final (PPO): {current_performance:.1%}\")\n",
    "    \n",
    "    relative_improvement = (current_performance - baseline_performance) / baseline_performance * 100\n",
    "    print(f\"   Relative gain: +{relative_improvement:.1f}%\")\n",
    "    \n",
    "    # Compare to CoA paper results\n",
    "    print(f\"\\nüéØ CoA Paper Comparison:\")\n",
    "    print(f\"   Paper result: 55.3% GAIA\")\n",
    "    print(f\"   Our result: {current_performance:.1%}\")\n",
    "    \n",
    "    if current_performance >= 0.55:\n",
    "        print(\"   Status: ‚úÖ TARGET ACHIEVED!\")\n",
    "    elif current_performance >= 0.52:\n",
    "        print(\"   Status: üü° Close to target\")\n",
    "    else:\n",
    "        print(\"   Status: üî¥ Needs more training\")\n",
    "    \n",
    "    return performance_history\n",
    "\n",
    "# Run the training!\n",
    "results = run_ppo_training(ppo_trainer, training_tasks, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize PPO Learning Curve\n",
    "\n",
    "See how performance improves over training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_learning_curve(performance_history):\n",
    "    \"\"\"ASCII art learning curve\"\"\"\n",
    "    \n",
    "    print(\"\\nüìà PPO LEARNING CURVE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Normalize to 0-20 scale for visualization\n",
    "    min_perf = min(performance_history)\n",
    "    max_perf = max(performance_history)\n",
    "    \n",
    "    print(f\"Performance Range: {min_perf:.1%} ‚Üí {max_perf:.1%}\\n\")\n",
    "    \n",
    "    # Create ASCII graph\n",
    "    for epoch, perf in enumerate(performance_history):\n",
    "        # Scale to 0-30 characters\n",
    "        bar_length = int((perf - 0.4) / (0.6 - 0.4) * 30)\n",
    "        bar = '‚ñà' * max(0, bar_length)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: {bar:30} {perf:.1%}\")\n",
    "    \n",
    "    # Add target line\n",
    "    target_length = int((0.553 - 0.4) / (0.6 - 0.4) * 30)\n",
    "    target_bar = '‚îÄ' * target_length + 'üéØ'\n",
    "    print(f\"Target:  {target_bar:30} 55.3%\")\n",
    "    \n",
    "    # Show key insights\n",
    "    improvement = performance_history[-1] - performance_history[0]\n",
    "    relative_improvement = improvement / performance_history[0] * 100\n",
    "    \n",
    "    print(f\"\\nüìä Key Insights:\")\n",
    "    print(f\"   Total improvement: +{improvement:.1%}\")\n",
    "    print(f\"   Relative improvement: +{relative_improvement:.1f}%\")\n",
    "    print(f\"   Learning rate: {improvement/len(performance_history):.1%} per epoch\")\n",
    "\n",
    "visualize_learning_curve(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare All Approaches\n",
    "\n",
    "Traditional ‚Üí SFT ‚Üí PPO performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_approaches():\n",
    "    \"\"\"Compare Traditional vs SFT vs PPO\"\"\"\n",
    "    \n",
    "    approaches = [\n",
    "        {\n",
    "            'name': 'Traditional Multi-Agent',\n",
    "            'performance': 0.42,\n",
    "            'cost': 3.0,  # 3x API calls\n",
    "            'latency': 3.0,  # 3x latency\n",
    "            'complexity': 'High',\n",
    "            'pros': ['Specialized agents', 'Interpretable'],\n",
    "            'cons': ['Slow', 'Expensive', 'Complex orchestration']\n",
    "        },\n",
    "        {\n",
    "            'name': 'AFM with SFT',\n",
    "            'performance': 0.45,\n",
    "            'cost': 1.0,  # 1x API call\n",
    "            'latency': 1.0,  # 1x latency\n",
    "            'complexity': 'Medium',\n",
    "            'pros': ['Fast', 'Cheap', 'Single model'],\n",
    "            'cons': ['Limited by training data', 'No exploration']\n",
    "        },\n",
    "        {\n",
    "            'name': 'AFM with PPO',\n",
    "            'performance': 0.553,\n",
    "            'cost': 1.0,  # 1x API call\n",
    "            'latency': 1.0,  # 1x latency\n",
    "            'complexity': 'Medium',\n",
    "            'pros': ['Best performance', 'Fast', 'Cheap', 'Self-improving'],\n",
    "            'cons': ['Complex training', 'Reward engineering']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"üèÅ COMPLETE COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(\"\\nüìä Performance (GAIA Benchmark):\")\n",
    "    for approach in approaches:\n",
    "        bar_length = int(approach['performance'] * 50)\n",
    "        bar = '‚ñà' * bar_length\n",
    "        print(f\"  {approach['name']:20} {bar:25} {approach['performance']:.1%}\")\n",
    "    \n",
    "    # Cost comparison\n",
    "    print(\"\\nüí∞ Cost (relative to PPO):\")\n",
    "    for approach in approaches:\n",
    "        bar_length = int(approach['cost'] * 10)\n",
    "        bar = '‚ñà' * bar_length\n",
    "        print(f\"  {approach['name']:20} {bar:30} {approach['cost']:.1f}x\")\n",
    "    \n",
    "    # Speed comparison\n",
    "    print(\"\\n‚ö° Speed (relative to PPO):\")\n",
    "    for approach in approaches:\n",
    "        # Invert for speed (lower latency = higher speed)\n",
    "        speed = 1 / approach['latency']\n",
    "        bar_length = int(speed * 10)\n",
    "        bar = '‚ñà' * bar_length\n",
    "        print(f\"  {approach['name']:20} {bar:10} {speed:.1f}x\")\n",
    "    \n",
    "    # Detailed breakdown\n",
    "    print(\"\\nüìã Detailed Comparison:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for approach in approaches:\n",
    "        print(f\"\\n{approach['name']}:\")\n",
    "        print(f\"  Performance: {approach['performance']:.1%}\")\n",
    "        print(f\"  Cost: {approach['cost']:.1f}x\")\n",
    "        print(f\"  Complexity: {approach['complexity']}\")\n",
    "        print(f\"  Pros: {', '.join(approach['pros'])}\")\n",
    "        print(f\"  Cons: {', '.join(approach['cons'])}\")\n",
    "    \n",
    "    # Final verdict\n",
    "    print(\"\\nüèÜ WINNER: AFM with PPO\")\n",
    "    print(\"   üéØ Best performance: 55.3% vs 42% traditional\")\n",
    "    print(\"   ‚ö° 3x faster than traditional\")\n",
    "    print(\"   üí∞ 3x cheaper than traditional\")\n",
    "    print(\"   üöÄ Continues to improve with more training\")\n",
    "\n",
    "compare_all_approaches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Custom PPO Implementation üõ†Ô∏è\n",
    "\n",
    "Build your own PPO algorithm for AFM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourPPO:\n",
    "    \"\"\"Your custom PPO implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, clip_epsilon=0.2):\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        # TODO: Add your parameters\n",
    "    \n",
    "    def compute_advantages(self, rewards, values=None):\n",
    "        \"\"\"Compute advantage estimates\"\"\"\n",
    "        # TODO: Implement GAE (Generalized Advantage Estimation)\n",
    "        # or simpler advantage calculation\n",
    "        advantages = np.array(rewards)  # Placeholder\n",
    "        return advantages\n",
    "    \n",
    "    def ppo_loss(self, old_probs, new_probs, advantages):\n",
    "        \"\"\"Compute PPO clipped loss\"\"\"\n",
    "        # TODO: Implement the actual PPO loss\n",
    "        # L = min(r_t * A_t, clip(r_t, 1-Œµ, 1+Œµ) * A_t)\n",
    "        loss = 0.0  # Placeholder\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, trajectories):\n",
    "        \"\"\"One PPO training step\"\"\"\n",
    "        # TODO: Implement complete training step\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "your_ppo = YourPPO()\n",
    "test_rewards = [0.5, 0.7, 0.3, 0.8, 0.6]\n",
    "advantages = your_ppo.compute_advantages(test_rewards)\n",
    "print(f\"Your advantages: {advantages}\")\n",
    "print(\"Complete the implementation above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Advanced Reward Engineering üéØ\n",
    "\n",
    "Design a reward function that gets 60%+ performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRewardFunction:\n",
    "    \"\"\"Your advanced reward function for AFM\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # TODO: Design your reward components\n",
    "        pass\n",
    "    \n",
    "    def calculate_reward(self, task, afm_output, ground_truth=None):\n",
    "        \"\"\"Calculate advanced reward\"\"\"\n",
    "        # TODO: Implement sophisticated reward calculation\n",
    "        # Ideas:\n",
    "        # - Semantic similarity to ground truth\n",
    "        # - Code execution success (for coding tasks)\n",
    "        # - User preference modeling\n",
    "        # - Multi-objective optimization\n",
    "        # - Uncertainty-aware rewards\n",
    "        \n",
    "        reward = 0.5  # Placeholder\n",
    "        return reward\n",
    "\n",
    "# Benchmark your reward function\n",
    "advanced_reward = AdvancedRewardFunction()\n",
    "\n",
    "test_cases = [\n",
    "    (\"Build API\", \"[Planner]: Plan ready [Coder]: Code complete [Reviewer]: Approved\"),\n",
    "    (\"Debug error\", \"Error: Cannot process this task\"),\n",
    "    (\"Optimize query\", \"[Planner]: Analysis done [Coder]: Query optimized [Reviewer]: 10x faster\")\n",
    "]\n",
    "\n",
    "print(\"Testing your advanced reward function:\")\n",
    "for task, output in test_cases:\n",
    "    reward = advanced_reward.calculate_reward(task, output)\n",
    "    print(f\"  Task: {task[:15]}... ‚Üí Reward: {reward:.3f}\")\n",
    "\n",
    "print(\"\\nGoal: Design rewards that guide AFM to 60%+ performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Hyperparameter Optimization üî¨\n",
    "\n",
    "Find the best PPO hyperparameters for AFM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(search_space, num_trials=10):\n",
    "    \"\"\"Search for optimal PPO hyperparameters\"\"\"\n",
    "    \n",
    "    # TODO: Implement hyperparameter optimization\n",
    "    # Techniques to try:\n",
    "    # 1. Grid search\n",
    "    # 2. Random search\n",
    "    # 3. Bayesian optimization\n",
    "    # 4. Population-based training\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"üî¨ HYPERPARAMETER SEARCH\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        # TODO: Sample hyperparameters\n",
    "        config = {\n",
    "            'learning_rate': random.uniform(1e-5, 1e-3),\n",
    "            'clip_epsilon': random.uniform(0.1, 0.3),\n",
    "            'batch_size': random.choice([32, 64, 128]),\n",
    "            'ppo_epochs': random.choice([2, 4, 8])\n",
    "        }\n",
    "        \n",
    "        # TODO: Train with these hyperparameters\n",
    "        performance = random.uniform(0.45, 0.60)  # Simulate\n",
    "        \n",
    "        results.append((config, performance))\n",
    "        print(f\"Trial {trial+1}: {performance:.1%} performance\")\n",
    "    \n",
    "    # Find best configuration\n",
    "    best_config, best_performance = max(results, key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\nüèÜ Best configuration:\")\n",
    "    for key, value in best_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Performance: {best_performance:.1%}\")\n",
    "    \n",
    "    return best_config, best_performance\n",
    "\n",
    "# Define search space\n",
    "search_space = {\n",
    "    'learning_rate': [1e-5, 1e-3],\n",
    "    'clip_epsilon': [0.1, 0.3],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'ppo_epochs': [2, 4, 8]\n",
    "}\n",
    "\n",
    "best_config, best_perf = hyperparameter_search(search_space, num_trials=5)\n",
    "print(f\"\\nChallenge: Can you get above 57%? Current best: {best_perf:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways üéì\n",
    "\n",
    "1. **PPO Breakthrough**: +18-20% performance over SFT alone\n",
    "2. **Reward Design**: Critical for guiding AFM learning\n",
    "3. **Exploration**: PPO discovers strategies beyond training data\n",
    "4. **Stability**: Clipping prevents catastrophic policy updates\n",
    "5. **SOTA Results**: 55.3% GAIA beats all multi-agent systems!\n",
    "\n",
    "## The Complete CoA Pipeline üîÑ\n",
    "\n",
    "We've now built the complete Chain-of-Agents system:\n",
    "\n",
    "1. **Part 1**: Multi-agent trajectories ‚Üí Training data\n",
    "2. **Part 2**: Progressive filtering ‚Üí High-quality data\n",
    "3. **Part 3**: SFT ‚Üí AFM that mimics agents\n",
    "4. **Part 4**: PPO ‚Üí AFM that beats agents!\n",
    "\n",
    "## Why This Is Revolutionary üåü\n",
    "\n",
    "**Traditional AI**: Hand-coded agents, complex orchestration\n",
    "**Chain-of-Agents**: Learn agent behaviors, optimize automatically\n",
    "\n",
    "- **Performance**: State-of-the-art results\n",
    "- **Efficiency**: 3x faster, 3x cheaper\n",
    "- **Scalability**: One model handles all agents\n",
    "- **Adaptability**: Continues improving with more data\n",
    "\n",
    "## What's Next? üöÄ\n",
    "\n",
    "Build your own CoA system:\n",
    "1. Collect domain-specific agent trajectories\n",
    "2. Design custom reward functions\n",
    "3. Scale to production workloads\n",
    "4. Beat the 55.3% GAIA benchmark!\n",
    "\n",
    "## Final Challenge üèÜ\n",
    "\n",
    "Implement the complete CoA pipeline and achieve:\n",
    "- **55%+ GAIA performance**\n",
    "- **< 2 second inference time**\n",
    "- **< $0.01 per query cost**\n",
    "\n",
    "## Homework üìù\n",
    "\n",
    "1. Implement real PPO with PyTorch\n",
    "2. Design task-specific reward functions\n",
    "3. Train on 10,000+ trajectories\n",
    "4. Benchmark against GPT-4 + traditional agents\n",
    "5. Deploy as production API\n",
    "6. Read the complete CoA paper from OPPO\n",
    "\n",
    "**Congratulations!** You've built Chain-of-Agents from scratch! üéâ\n",
    "\n",
    "You now understand why CoA represents the future of multi-agent AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}