# GenAI Processors: A Practical Handsâ€‘On Tutorial
> Build modular, asynchronous AI pipelines with Google DeepMindâ€™s **GenAI Processors** (released **10â€¯Julâ€¯2025**).

---

## Why GenAIâ€¯Processors?
GenAIâ€¯Processors wraps *PythonÂ `asyncio`* around a unified **Processor â†” ProcessorPart** abstraction.  
It lets you:

| Goal | How GenAIâ€¯Processors Helps |
|------|---------------------------|
| Reduce latency / TTFT | Concurrent execution of pipeline stages |
| Handle multimodal data | A single stream of `ProcessorPart` objects |
| Reâ€‘use & test components | Chain (`+`) or branch (`//`) processors |
| Go from idea to live agent fast | Readyâ€‘made `GenaiModel` & `LiveProcessor` for Gemini API |

---

## 1â€¯Â·â€¯Prerequisites
* PythonÂ â‰¥â€¯3.10
* A Google Cloud project with **Gemini API** enabled (for live examples)
* A *virtualenv* (recommended):

```bash
python -m venv .venv && source .venv/bin/activate
```

---

## 2â€¯Â·â€¯Installation

```bash
pip install --upgrade pip
pip install genai-processors   # v1.0.3 at the time of writing
```

---

## 3â€¯Â·â€¯Quickstart: â€œHelloÂ Geminiâ€

```python
from genai_processors.core import genai_model
from genai_processors import content_api, streams

pipeline = genai_model.GenaiModel(model_name="gemini-2.0-flash")

inp = content_api.from_text("Hello, Gemini Processors!")
out_stream = await pipeline(inp)

async for part in out_stream:
    print(part.text)
```

Save as **quickstart.py** and run:

```bash
python -m asyncio quickstart.py
```

---

## 4â€¯Â·â€¯Core Concepts

### 4.1Â Processor / PartProcessor
```python
from genai_processors import processor

@processor.PartProcessor
async def uppercase(part):
    if part.text:
        part.text = part.text.upper()
    return part
```
* A **Processor** accepts / returns **async streams** of `ProcessorPart`.
* Chain processors with **`+`** (serial) or **`//`** (parallel).

### 4.2Â ProcessorPart
```python
from genai_processors import content_api
img_part  = content_api.from_image("cat.png", mime_type="image/png")
text_part = content_api.from_text("meow")
```
Metadata (MIME type, role, custom attrs) travels with every part.

### 4.3Â Streams & Concurrency
```python
from genai_processors import streams
parts = streams.stream_content(["Hello", "world"])
```
The library schedules downstream processors *concurrently* whenever their
dependencies are ready, minimising **Timeâ€‘Toâ€‘Firstâ€‘Token**.

---

## 5â€¯Â·â€¯Building Pipelines Stepâ€‘byâ€‘Step

### 5.1Â ExampleÂ 1 â€“ Uppercase â†’ Gemini

```python
text_pipeline = uppercase + genai_model.GenaiModel("gemini-2.0-pro")

inp = content_api.from_text("Synchronous code is so 2024.")
async for part in text_pipeline(inp):
    print(part.text)          # => Synchronous code is so 2024. (Gemini reply)
```

### 5.2Â ExampleÂ 2 â€“ Live AudioÂ +Â Video Agent

```python
from genai_processors.core import audio_io, video, live_model

cam_mic   = video.VideoIn() + audio_io.PyAudioIn()
speaker   = audio_io.PyAudioOut()
live_llm  = live_model.LiveProcessor(model_name="gemini-2.0-pro")

live_agent = cam_mic + live_llm + speaker

async for part in live_agent(streams.endless_stream()):
    # e.g. print live transcription tokens
    if part.text:
        print(part.text)
```

### 5.3Â ExampleÂ 3 â€“ Custom JSON ProcessorPart

```python
@processor.PartProcessor
async def sentiment_json(part):
    if part.text:
        score = 1 if "ğŸ™‚" in part.text else -1
        yield content_api.from_json({"sentiment": score})
```

---

## 6â€¯Â·â€¯Testing & Debugging

```python
import pytest, asyncio, types

@pytest.mark.asyncio
async def test_uppercase():
    from genai_processors import content_api, streams
    from tutorial.upper import uppercase
    s = streams.stream_content(["abc"])
    out = [p async for p in uppercase(s)]
    assert out[0].text == "ABC"
```

* Use **`pytest-asyncio`** for coroutine tests.
* Use the **`streams.preview()`** helper to peek at parts without consuming them.

---

## 7â€¯Â·â€¯Performance Tips

| Tip | Explanation |
|-----|-------------|
| Keep processors *pure* | Avoid global state; easier to parallelise |
| Chunk large inputs | Emit smaller `ProcessorPart`s to start downstream work sooner |
| Prefer `LiveProcessor` | Streams tokens as theyâ€™re produced |
| Profile with `asyncio.run(debug=True, â€¦)` | Enables builtâ€‘in slowâ€‘task detector |

---

## 8â€¯Â·â€¯Deployment (Docker + Cloud Run)

```dockerfile
FROM python:3.12-slim
WORKDIR /app
COPY . /app
RUN pip install --no-cache-dir genai-processors
CMD ["python", "-m", "uvicorn", "app:router", "--host=0.0.0.0", "--port=8080"]
```

> Deploy with  
> `gcloud run deploy genai-pipeline --source=. --region=us-central1`

---

## 9â€¯Â·â€¯Contributing

1. Fork <https://github.com/google-gemini/genai-processors>.
2. Add your processor under **`contrib/`**.
3. Run `make test && make lint`.
4. Open a PR â€“ follow the **CLA** and style guide in `CONTRIBUTING.md`.

---

## 10â€¯Â·â€¯Further Reading
* Official blogâ€‰â€“â€‰*â€œAnnouncing GenAIÂ Processorsâ€*  
*Colab notebooks* (Content API, Processor Intro, LiveÂ API)  
*Examples directory* (Realâ€‘Time Live, Research Agent, Live Commentary)

---

Â©Â 2025 The GenAIâ€¯Processors authors Â· Apacheâ€‘2.0  
Pull requests welcome!
